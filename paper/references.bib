
@article{arnbjerg-nielsen2013,
    title = {Impacts of Climate Change on Rainfall Extremes and Urban Drainage
             Systems: A Review},
    shorttitle = {Impacts of Climate Change on Rainfall Extremes and Urban
                  Drainage Systems},
    author = {{Arnbjerg-Nielsen}, K. and Willems, P. and Olsson, J. and Beecham,
              S. and Pathirana, A. and B{\"u}low Gregersen, I. and Madsen, H. and
              Nguyen, V.-T.-V.},
    year = {2013},
    month = jul,
    journal = {Water Science and Technology},
    volume = {68},
    number = {1},
    pages = {16--28},
    issn = {0273-1223, 1996-9732},
    doi = {10/f44bgx},
    abstract = {A review is made of current methods for assessing future changes
                in urban rainfall extremes and their effects on urban drainage
                systems, due to anthropogenic-induced climate change. The review
                concludes that in spite of significant advances there are still
                many limitations in our understanding of how to describe
                precipitation patterns in a changing climate in order to design
                and operate urban drainage infrastructure. Climate change may
                well be the driver that ensures that changes in urban drainage
                paradigms are identified and suitable solutions implemented.
                Design and optimization of urban drainage infrastructure
                considering climate change impacts and co-optimizing these with
                other objectives will become ever more important to keep our
                cities habitable into the future.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Arnbjerg-Nielsen et al_2013_Impacts of climate
            change on rainfall extremes and urban drainage systems.pdf},
}

@article{arthur2018,
    title = {Social Sensing of Floods in the {{UK}}},
    author = {Arthur, Rudy and Boulton, Chris A. and Shotton, Humphrey and
              Williams, Hywel T. P.},
    editor = {Schumann, Guy J-P.},
    year = {2018},
    month = jan,
    journal = {PLOS ONE},
    volume = {13},
    number = {1},
    pages = {e0189327},
    issn = {1932-6203},
    doi = {10.1371/journal.pone.0189327},
    abstract = {Social sensing'' is a form of crowd-sourcing that involves
                systematic analysis of digital communications to detect
                real-world events. Here we consider the use of social sensing for
                observing natural hazards. In particular, we present a case study
                that uses data from a popular social media platform (Twitter) to
                detect and locate flood events in the UK. In order to improve
                data quality we apply a number of filters (timezone, simple text
                filters and a naive Bayes `relevance' filter) to the data. We
                then use place names in the user profile and message text to
                infer the location of the tweets. These two steps remove most of
                the irrelevant tweets and yield orders of magnitude more located
                tweets than we have by relying on geotagged data. We demonstrate
                that high resolution social sensing of floods is feasible and we
                can produce high-quality historical and real-time maps of floods
                using Twitter.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Arthur et al_2018_Social sensing of floods in
            the UK.pdf},
}

@article{ashktorab2014,
    title = {Tweedr: Mining {{Twitter}} to {{Inform}}},
    author = {Ashktorab, Zahra and Brown, Christopher and Nandi, Manojit and
              Culotta, Aron},
    year = {2014},
    pages = {5},
    abstract = {In this paper, we introduce Tweedr, a Twitter-mining tool that
                extracts actionable information for disaster relief workers
                during natural disasters. The Tweedr pipeline consists of three
                main parts: classification, clustering and extraction. In the
                classification phase, we use a variety of classification methods
                (sLDA, SVM, and logistic regression) to identify tweets reporting
                damage or casualties. In the clustering phase, we use filters to
                merge tweets that are similar to one another; and finally, in the
                extraction phase, we extract tokens and phrases that report
                specific information about different classes of infrastructure
                damage, damage types, and casualties. We empirically validate our
                approach with tweets collected from 12 different crises in the
                United States since 2006.},
    langid = {english},
    keywords = {⛔ No DOI found},
    file = {/home/cjber/drive/pdf/Ashktorab et al_2014_Tweedr.pdf},
}

@article{atefeh2015,
    title = {A {{Survey}} of {{Techniques}} for {{Event Detection}} in {{Twitter
             }}},
    author = {Atefeh, Farzindar and Khreich, Wael},
    year = {2015},
    journal = {Computational Intelligence},
    volume = {31},
    number = {1},
    pages = {132--164},
    issn = {1467-8640},
    doi = {10/f62chq},
    abstract = {Twitter is among the fastest-growing microblogging and online
                social networking services. Messages posted on Twitter (tweets)
                have been reporting everything from daily life stories to the
                latest local and global news and events. Monitoring and analyzing
                this rich and continuous user-generated content can yield
                unprecedentedly valuable information, enabling users and
                organizations to acquire actionable knowledge. This article
                provides a survey of techniques for event detection from Twitter
                streams. These techniques aim at finding real-world occurrences
                that unfold over space and time. In contrast to conventional
                media, event detection from Twitter streams poses new challenges.
                Twitter streams contain large amounts of meaningless messages and
                polluted content, which negatively affect the detection
                performance. In addition, traditional text mining techniques are
                not suitable, because of the short length of tweets, the large
                number of spelling and grammatical errors, and the frequent use
                of informal and mixed language. Event detection techniques
                presented in literature address these issues by adapting
                techniques from various fields to the uniqueness of Twitter. This
                article classifies these techniques according to the event type,
                detection task, and detection method and discusses commonly used
                features. Finally, it highlights the need for public benchmarks
                to evaluate the performance of different detection approaches and
                various features.},
    langid = {english},
    keywords = {event detection,event identification,microblogs,monitoring
                social media,Twitter data stream},
    annotation = {\_eprint:
                  https://onlinelibrary.wiley.com/doi/pdf/10.1111/coin.12017},
    file = {/home/cjber/drive/pdf/Atefeh_Khreich_2015_A Survey of Techniques for
            Event Detection in
            Twitter.pdf;/home/cjber/drive/zotero/storage/SVVZTP57/coin.html},
}

@article{barbieri2020,
    title = {{{TweetEval}}: Unified {{Benchmark}} and {{Comparative Evaluation}}
             for {{Tweet Classification}}},
    shorttitle = {{{TweetEval}}},
    author = {Barbieri, Francesco and {Camacho-Collados}, Jose and Neves,
              Leonardo and {Espinosa-Anke}, Luis},
    year = {2020},
    month = oct,
    journal = {arXiv:2010.12421 [cs]},
    eprint = {2010.12421},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {The experimental landscape in natural language processing for
                social media is too fragmented. Each year, new shared tasks and
                datasets are proposed, ranging from classics like sentiment
                analysis to irony detection or emoji prediction. Therefore, it is
                unclear what the current state of the art is, as there is no
                standardized evaluation protocol, neither a strong set of
                baselines trained on such domainspecific data. In this paper, we
                propose a new evaluation framework (TWEETEVAL) consisting of
                seven heterogeneous Twitter-specific classification tasks. We
                also provide a strong set of baselines as starting point, and
                compare different language modeling pre-training strategies. Our
                initial experiments show the effectiveness of starting off with
                existing pretrained generic language models, and continue
                training them on Twitter corpora.},
    archiveprefix = {arXiv},
    langid = {english},
    keywords = {⛔ No DOI found,Computer Science - Computation and Language,
                Computer Science - Social and Information Networks},
    file = {/home/cjber/drive/pdf/Barbieri et al_2020_TweetEval.pdf},
}

@article{beck2020,
    title = {Representation {{Problems}} in {{Linguistic Annotations}}:
             Ambiguity, {{Variation}}, {{Uncertainty}}, {{Error}} and {{Bias}}},
    author = {Beck, Christin and Booth, Hannah and {El-Assady}, Mennatallah and
              Butt, Miriam},
    year = {2020},
    pages = {14},
    abstract = {The development of linguistic corpora is fraught with various
                problems of annotation and representation. These constitute a
                very real challenge for the development and use of annotated
                corpora, but as yet not much literature exists on how to address
                the underlying problems. In this paper, we identify and discuss
                five sources of representation problems, which are independent
                though interrelated: ambiguity, variation, uncertainty, error and
                bias. We outline and characterize these sources, discussing how
                their improper treatment can have stark consequences for research
                outcomes. Finally, we discuss how an adequate treatment can
                inform corpus-related linguistic research, both computational and
                theoretical, improving the reliability of research results and
                NLP models, as well as informing the more general reproducibility
                issue.},
    langid = {english},
    keywords = {⛔ No DOI found},
    file = {/home/cjber/drive/zotero/storage/9B52D8FP/Beck et al. -
            Representation Problems in Linguistic Annotations.pdf},
}

@article{brants2003,
    title = {A {{System}} for {{New Event Detection}}},
    author = {Brants, Thorsten and Chen, Francine and Farahat, Ayman},
    year = {2003},
    pages = {8},
    doi = {10/fcmb5q},
    abstract = {We present a new method and system for performing the New Event
                Detection task, i.e., in one or multiple streams of news stories,
                all stories on a previously unseen (new) event are marked. The
                method is based on an incremental TF-IDF model. Our extensions
                include: generation of source-specific models, similarity score
                normalization based on document-specific averages, similarity
                score normalization based on source-pair specific averages, term
                reweighting based on inverse event frequencies, and segmentation
                of the documents. We also report on extensions that did not
                improve results. The system performs very well on TDT3 and TDT4
                test data and scored second in the TDT-2002 evaluation.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Brants et al_2003_A System for New Event
            Detection.pdf},
}

@article{brengarth2016,
    title = {{{WEB}} 2.0: How Social Media Applications Leverage Nonprofit
             Responses during a Wildfire Crisis},
    shorttitle = {{{WEB}} 2.0},
    author = {Brengarth, Lauren Bacon and Mujkic, Edin},
    year = {2016},
    month = jan,
    journal = {Computers in Human Behavior},
    volume = {54},
    pages = {589--596},
    issn = {07475632},
    doi = {10/gmkgqg},
    abstract = {This study examines how Web 2.0 applications were used during a
                catastrophic wildfire in the Western United States that claimed
                two human lives, more than 18,000 acres of land and nearly 350
                homes. The study sheds light on how Web 2.0 applications were
                applied as a tool to transmit information while the disaster was
                unfolding. This research highlights unique nonprofit cases that
                inform the role and reliability of Web 2.0 applications during a
                crisis, and the roles that nonprofit organizations and the
                general public play while facing a dire emergency. In the cases
                presented, Web 2.0 applications served as a bridge between first
                responders, the population in immediate wildfire danger, and the
                citizens who were trying to help, resulting in saved lives,
                property, and natural resources. By combining existing literature
                and collected qualitative data, the researchers argue that Web
                2.0 applications represent flexible communication tools for
                transmission of timely information during a crisis situation.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Brengarth_Mujkic_2016_WEB 2.pdf},
}

@article{brouwer2017,
    title = {Probabilistic Flood Extent Estimates from Social Media Flood
             Observations},
    author = {Brouwer, Tom and Eilander, Dirk and {van Loenen}, Arnejan and
              Booij, Martijn J. and Wijnberg, Kathelijne M. and Verkade, Jan S.
              and Wagemaker, Jurjen},
    year = {2017},
    month = may,
    journal = {Natural Hazards and Earth System Sciences},
    volume = {17},
    number = {5},
    pages = {735--747},
    issn = {1684-9981},
    doi = {10/gcdh2v},
    abstract = {The increasing number and severity of floods, driven by
                phenomena such as urbanization, deforestation, subsidence and
                climate change, create a growing need for accurate and timely
                flood maps. In this paper we present and evaluate a method to
                create deterministic and probabilistic flood maps from Twitter
                messages that mention locations of flooding. A deterministic
                flood map created for the December 2015 flood in the city of York
                (UK) showed good performance (F (2) = 0.69; a statistic ranging
                from 0 to 1, with 1 expressing a perfect fit with validation data
                ). The probabilistic flood maps we created showed that, in the
                York case study, the uncertainty in flood extent was mainly
                induced by errors in the precise locations of flood observations
                as derived from Twitter data. Errors in the terrain elevation
                data or in the parameters of the applied algorithm contributed
                less to flood extent uncertainty. Although these maps tended to
                overestimate the actual probability of flooding, they gave a
                reasonable representation of flood extent uncertainty in the
                area. This study illustrates that inherently uncertain data from
                social media can be used to derive information about flooding.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Brouwer et al_2017_Probabilistic flood extent
            estimates from social media flood observations.pdf},
}

@article{caragea2011,
    title = {Classifying {{Text Messages}} for the {{Haiti Earthquake}}},
    author = {Caragea, Cornelia and McNeese, Nathan and Jaiswal, Anuj and
              Traylor, Greg and Kim, Hyun-Woo and Mitra, Prasenjit and Wu,
              Dinghao and Tapia, Andrea H and Giles, Lee and Jansen, Bernard J
              and Yen, John},
    year = {2011},
    pages = {10},
    abstract = {In case of emergencies (e.g., earthquakes, flooding), rapid
                responses are needed in order to address victims' requests for
                help. Social media used around crises involves self-organizing
                behavior that can produce accurate results, often in advance of
                official communications. This allows affected population to send
                tweets or text messages, and hence, make them heard. The ability
                to classify tweets and text messages automatically, together with
                the ability to deliver the relevant information to the
                appropriate personnel are essential for enabling the personnel to
                timely and efficiently work to address the most urgent needs, and
                to understand the emergency situation better. In this study, we
                developed a reusable information technology infrastructure,
                called Enhanced Messaging for the Emergency Response Sector (
                EMERSE). The components of EMERSE are: (i) an iPhone application;
                (ii) a Twitter crawler component; (iii) machine translation; and
                (iv) automatic message classification. While each component is
                important in itself and deserves a detailed analysis, in this
                paper we focused on the automatic classification component, which
                classifies and aggregates tweets and text messages about the
                Haiti disaster relief so that they can be easily accessed by
                non-governmental organizations, relief workers, people in Haiti,
                and their friends and families.},
    langid = {english},
    keywords = {⛔ No DOI found},
    file = {/home/cjber/drive/pdf/Caragea et al_2011_Classifying Text Messages
            for the Haiti Earthquake.pdf},
}

@article{caragea2016,
    title = {Identifying {{Informative Messages}} in {{Disaster Events}} Using {
             {Convolutional Neural Networks}}},
    author = {Caragea, Cornelia and Silvescu, Adrian and Tapia, Andrea H},
    year = {2016},
    pages = {8},
    abstract = {Social media is a vital source of information during any major
                event, especially natural disasters. Data produced through social
                networking sites is seen as ubiquitous, rapid and accessible, and
                it is believed to empower average citizens to become more
                situationally aware during disasters and coordinate to help
                themselves. However, with the exponential increase in the volume
                of social media data, so comes the increase in data that are
                irrelevant to a disaster, thus, diminishing peoples' ability to
                find the information that they need in order to organize relief
                efforts, find help, and potentially save lives. In this paper, we
                present an approach to identifying informative messages in social
                media streams during disaster events. Our approach is based on
                Convolutional Neural Networks and shows significant improvement
                in performance over models that use the ``bag of words'' and
                n-grams as features on several datasets of messages from flooding
                events.},
    langid = {english},
    keywords = {⛔ No DOI found},
    file = {/home/cjber/drive/pdf/Caragea et al_2016_Identifying Informative
            Messages in Disaster Events using Convolutional Neural.pdf},
}

@article{carley2016,
    title = {Crowd Sourcing Disaster Management: The Complex Nature of {{Twitter
             }} Usage in {{Padang Indonesia}}},
    shorttitle = {Crowd Sourcing Disaster Management},
    author = {Carley, Kathleen M. and Malik, Momin and Landwehr, Peter M. and
              Pfeffer, J{\"u}rgen and Kowalchuck, Michael},
    year = {2016},
    month = dec,
    journal = {Safety Science},
    volume = {90},
    pages = {48--61},
    issn = {09257535},
    doi = {10/gc7q4j},
    langid = {english},
    file = {/home/cjber/drive/pdf/Carley et al_2016_Crowd sourcing disaster
            management.pdf},
}

@book{castillo2016,
    title = {Big Crisis Data: Social Media in Disasters and Time-Critical
             Situations},
    author = {Castillo, Carlos},
    year = {2016},
    publisher = {{Cambridge University Press}},
    isbn = {1-316-69457-7},
    file = {/home/cjber/drive/pdf/Castillo_2016_Big crisis data.pdf},
}

@article{chen2014,
    title = {Mining {{Social Media Data}} for {{Understanding Students}}' {{
             Learning Experiences}}},
    author = {Chen, Xin and Vorvoreanu, Mihaela and Madhavan, Krishna},
    year = {2014},
    month = jul,
    journal = {IEEE Transactions on Learning Technologies},
    volume = {7},
    number = {3},
    pages = {246--259},
    issn = {1939-1382},
    doi = {10/f6swvp},
    abstract = {Students' informal conversations on social media (e.g., Twitter,
                Facebook) shed light into their educational experiences-opinions,
                feelings, and concerns about the learning process. Data from such
                uninstrumented environments can provide valuable knowledge to
                inform student learning. Analyzing such data, however, can be
                challenging. The complexity of students' experiences reflected
                from social media content requires human interpretation. However,
                the growing scale of data demands automatic data analysis
                techniques. In this paper, we developed a workflow to integrate
                both qualitative analysis and large-scale data mining techniques.
                We focused on engineering students' Twitter posts to understand
                issues and problems in their educational experiences. We first
                conducted a qualitative analysis on samples taken from about 25,
                000 tweets related to engineering students' college life. We
                found engineering students encounter problems such as heavy study
                load, lack of social engagement, and sleep deprivation. Based on
                these results, we implemented a multi-label classification
                algorithm to classify tweets reflecting students' problems. We
                then used the algorithm to train a detector of student problems
                from about 35,000 tweets streamed at the geo-location of Purdue
                University. This work, for the first time, presents a methodology
                and results that show how informal social media data can provide
                insights into students' experiences.},
    keywords = {Classification algorithms,computers and education,Cultural
                differences,Data mining,Education,Educational institutions,
                Engineering students,Media,social networking,Twitter,web text
                analysis},
    file = {/home/cjber/drive/pdf/Chen et al_2014_Mining Social Media Data for
            Understanding Students’ Learning
            Experiences.pdf;/home/cjber/drive/zotero/storage/UWJUU6T4/6697807.html
            },
}

@article{debruijn2018,
    ids = {debruijn2018a},
    title = {{{TAGGS}}: Grouping {{Tweets}} to {{Improve Global Geoparsing}} for
             {{Disaster Response}}},
    shorttitle = {{{TAGGS}}},
    author = {{de Bruijn}, Jens A. and {de Moel}, Hans and Jongman, Brenden and
              Wagemaker, Jurjen and Aerts, Jeroen C. J. H.},
    year = {2018},
    month = jun,
    journal = {Journal of Geovisualization and Spatial Analysis},
    volume = {2},
    number = {1},
    pages = {2},
    issn = {2509-8810, 2509-8829},
    doi = {10/ggwjt3},
    abstract = {Timely and accurate information about ongoing events are crucial
                for relief organizations seeking to effectively respond to
                disasters. Recently, social media platforms, especially Twitter,
                have gained traction as a novel source of information on disaster
                events. Unfortunately, geographical information is rarely
                attached to tweets, which hinders the use of Twitter for
                geographical applications. As a solution, geoparsing algorithms
                extract and can locate geographical locations referenced in a
                tweet's text. This paper describes TAGGS, a new algorithm that
                enhances location disambiguation by employing both metadata and
                the contextual spatial information of groups of tweets
                referencing the same location regarding a specific disaster type.
                Validation demonstrated that TAGGS approximately attains a recall
                of 0.82 and precision of 0.91. Without lowering precision, this
                roughly doubles the number of correctly found administrative
                subdivisions and cities, towns, and villages as compared to
                individual geoparsing. We applied TAGGS to 55.1 million
                flood-related tweets in 12 languages, collected over 3 years. We
                found 19.2 million tweets mentioning one or more flood locations,
                which can be towns (11.2 million), administrative subdivisions (
                5.1 million), or countries (4.6 million). In the future, TAGGS
                could form the basis for a global event detection system.},
    langid = {english},
    file = {/home/cjber/drive/pdf/de Bruijn et
            al_2018_TAGGS.pdf;/home/cjber/drive/pdf/de Bruijn et
            al_2018_TAGGS2.pdf},
}

@article{debruijn2020,
    title = {Improving the Classification of Flood Tweets with Contextual
             Hydrological Information in a Multimodal Neural Network},
    author = {{de Bruijn}, Jens A. and {de Moel}, Hans and Weerts, Albrecht H.
              and {de Ruiter}, Marleen C. and Basar, Erkan and Eilander, Dirk and
              Aerts, Jeroen C.J.H.},
    year = {2020},
    month = jul,
    journal = {Computers \& Geosciences},
    volume = {140},
    pages = {104485},
    issn = {00983004},
    doi = {10/gk8gzg},
    abstract = {While text classification can classify tweets, assessing whether
                a tweet is related to an ongoing flood event or not, based on its
                text, remains difficult. Inclusion of contextual hydrological
                information could improve the perfor\- mance of such algorithms.
                Here, a multilingual multimodal neural network is designed that
                can effectively use both textual and hydrological information.
                The classification data was obtained from Twitter using
                flood-related keywords in English, French, Spanish and
                Indonesian. Subsequently, hydrological information was extracted
                from a global precipitation dataset based on the tweet's
                timestamp and locations mentioned in its text. Three experiments
                were performed analyzing precision, recall and F1-scores while
                comparing a neural network that uses hydrological information
                against a neural network that does not. Results showed that
                F1-scores improved significantly across all experiments. Most
                notably, when optimizing for precision the neural network with hy
                \- drological information could achieve a precision of 0.91 while
                the neural network without hydrological infor\- mation failed to
                effectively optimize. Moreover, this study shows that including
                hydrological information can assist in the translation of the
                classification algorithm to unseen languages.},
    langid = {english},
    file = {/home/cjber/drive/pdf/de Bruijn et al_2020_Improving the
            classification of flood tweets with contextual hydrological.pdf},
}

@article{devlin2019,
    ids = {devlin2019a},
    title = {{{BERT}}: Pre-Training of {{Deep Bidirectional Transformers}} for {
             {Language Understanding}}},
    shorttitle = {{{BERT}}},
    author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova,
              Kristina},
    year = {2019},
    month = may,
    journal = {arXiv:1810.04805 [cs]},
    eprint = {1810.04805},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {We introduce a new language representation model called BERT,
                which stands for Bidirectional Encoder Representations from
                Transformers. Unlike recent language representation models (
                Peters et al., 2018a; Radford et al., 2018), BERT is designed to
                pretrain deep bidirectional representations from unlabeled text
                by jointly conditioning on both left and right context in all
                layers. As a result, the pre-trained BERT model can be finetuned
                with just one additional output layer to create state-of-the-art
                models for a wide range of tasks, such as question answering and
                language inference, without substantial taskspecific architecture
                modifications.},
    archiveprefix = {arXiv},
    langid = {english},
    keywords = {⛔ No DOI found,Computer Science - Computation and Language},
    file = {/home/cjber/drive/pdf/Devlin et
            al_2019_BERT.pdf;/home/cjber/drive/pdf/Devlin et
            al_2019_BERT2.pdf;/home/cjber/drive/zotero/storage/XFVJWZDY/1810.html
            },
}

@inproceedings{dou2012,
    title = {{{LeadLine}}: Interactive Visual Analysis of Text Data through
             Event Identification and Exploration},
    shorttitle = {{{LeadLine}}},
    booktitle = {2012 {{IEEE Conference}} on {{Visual Analytics Science}} and {{
                 Technology}} ({{VAST}})},
    author = {Dou, Wenwen and Wang, Xiaoyu and Skau, Drew and Ribarsky, William
              and Zhou, Michelle X.},
    year = {2012},
    month = oct,
    pages = {93--102},
    doi = {10/ggd95t},
    abstract = {Text data such as online news and microblogs bear valuable
                insights regarding important events and responses to such events.
                Events are inherently temporal, evolving over time. Existing
                visual text analysis systems have provided temporal views of
                changes based on topical themes extracted from text data. But few
                have associated topical themes with events that cause the
                changes. In this paper, we propose an interactive visual
                analytics system, LeadLine, to automatically identify meaningful
                events in news and social media data and support exploration of
                the events. To characterize events, LeadLine integrates topic
                modeling, event detection, and named entity recognition
                techniques to automatically extract information regarding the
                investigative 4 Ws: who, what, when, and where for each event. To
                further support analysis of the text corpora through events,
                LeadLine allows users to interactively examine meaningful events
                using the 4 Ws to develop an understanding of how and why.
                Through representing large-scale text corpora in the form of
                meaningful events, LeadLine provides a concise summary of the
                corpora. LeadLine also supports the construction of simple
                narratives through the exploration of events. To demonstrate the
                efficacy of LeadLine in identifying events and supporting
                exploration, two case studies were conducted using news and
                social media data.},
    keywords = {Crawlers,Data mining,Event detection,Lead,Time series analysis,
                Twitter,Visualization},
    file = {/home/cjber/drive/pdf/Dou et
            al_2012_LeadLine.pdf;/home/cjber/drive/zotero/storage/TB84RDXX/6400485.html
            },
}

@misc{europeanseverestormslaboratoryessl,
    title = {European {{Severe Weather Database}}},
    author = {{European Severe Storms Laboratory (ESSL)}},
    howpublished = {https://www.eswd.eu/},
    file = {/home/cjber/drive/zotero/storage/7TVQCQSA/www.eswd.eu.html},
}

@software{falconwa2019,
    author = {Falcon, William and {The PyTorch Lightning team}},
    doi = {10.5281/zenodo.3828935},
    license = {Apache-2.0},
    month = {3},
    title = {{PyTorch Lightning}},
    url = {https://github.com/PyTorchLightning/pytorch-lightning},
    version = {1.4},
    year = {2019},
}

@article{forzieri2017,
    title = {Increasing Risk over Time of Weather-Related Hazards to the {{
             European}} Population: A Data-Driven Prognostic Study},
    shorttitle = {Increasing Risk over Time of Weather-Related Hazards to the {{
                  European}} Population},
    author = {Forzieri, Giovanni and Cescatti, Alessandro and e Silva, Filipe
              Batista and Feyen, Luc},
    year = {2017},
    month = aug,
    journal = {The Lancet Planetary Health},
    volume = {1},
    number = {5},
    pages = {e200-e208},
    publisher = {{Elsevier}},
    issn = {2542-5196},
    doi = {10/gfz74r},
    abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}h3{$>$}Background{$<$}/h3{$><$}
                p{$>$}The observed increase in the effects on human beings of
                weather-related disasters has been largely attributed to the rise
                in population exposed, with a possible influence of global
                warming. Yet, future risks of weather-related hazards on human
                lives in view of climate and demographic changes have not been
                comprehensively investigated.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{
                $><$}p{$>$}We assessed the risk of weather-related hazards to the
                European population in terms of annual numbers of deaths in 30
                year intervals relative to the reference period (1981\textendash
                2010) up to the year 2100 (2011\textendash 40, 2041\textendash 70
                , and 2071\textendash 100) by combining disaster records with
                high-resolution hazard and demographic projections in a
                prognostic modelling framework. We focused on the hazards with
                the greatest impacts\textemdash heatwaves and cold waves,
                wildfires, droughts, river and coastal floods, and windstorms
                \textemdash and evaluated their spatial and temporal variations
                in intensity and frequency under a business-as-usual scenario of
                greenhouse gas emissions. We modelled long-term demographic
                dynamics through a territorial modelling platform to represent
                the evolution of human exposure under a corresponding
                middle-of-the-road socioeconomic scenario. We appraised human
                vulnerability to weather extremes on the basis of more than 2300
                records collected from disaster databases during the reference
                period and assumed it to be static under a scenario of no
                adaptation.{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$><$}p{$>$}We
                found that weather-related disasters could affect about
                two-thirds of the European population annually by the year 2100 (
                351 million people exposed per year [uncertainty range 126
                million to 523 million] during the period 2071\textendash 100)
                compared with 5\% during the reference period (1981\textendash
                2010; 25 million people exposed per year). About 50 times the
                number of fatalities occurring annually during the reference
                period (3000 deaths) could occur by the year 2100 (152 000 deaths
                [80 500\textendash 239 800]). Future effects show a prominent
                latitudinal gradient, increasing towards southern Europe, where
                the premature mortality rate due to weather extremes (about 700
                annual fatalities per million inhabitants [482\textendash 957]
                during the period 2071\textendash 100 \emph{vs} 11 during the
                reference period) could become the greatest environmental risk
                factor. The projected changes are dominated by global warming (
                accounting for more than 90\% of the rise in risk to human beings
                ), mainly through a rise in the frequency of heatwaves (about
                2700 heat-related fatalities per year during the reference period
                \emph{vs} 151 500 [80 100\textendash 239 000] during the period
                2071\textendash 100).{$<$}/p{$><$}h3{$>$}Interpretation{$<$}/h3{
                $><$}p{$>$}Global warming could result in rapidly rising costs of
                weather-related hazards to human beings in Europe unless adequate
                adaptation measures are taken. Our results could aid in
                prioritisation of regional investments to address the unequal
                burden of effects on human beings of weather-related hazards and
                differences in adaptation capacities.{$<$}/p{$><$}h3{$>$}Funding{
                $<$}/h3{$><$}p{$>$}European Commission.{$<$}/p{$>$}},
    langid = {english},
    pmid = {29851641},
    file = {/home/cjber/drive/pdf/Forzieri et al_2017_Increasing risk over time
            of weather-related hazards to the European
            population.pdf;/home/cjber/drive/zotero/storage/D73EKQYF/fulltext.html
            },
}

@article{gao2017,
    title = {Constructing Gazetteers from Volunteered {{Big Geo}}-{{Data}} Based
             on {{Hadoop}}},
    author = {Gao, Song and Li, Linna and Li, Wenwen and Janowicz, Krzysztof and
              Zhang, Yue},
    year = {2017},
    month = jan,
    journal = {Computers, Environment and Urban Systems},
    volume = {61},
    pages = {172--186},
    issn = {01989715},
    doi = {10/f9jhdk},
    abstract = {Traditional gazetteers are built and maintained by authoritative
                mapping agencies. In the age of Big Data, it is possible to
                construct gazetteers in a data-driven approach by mining rich
                volunteered geographic information (VGI) from the Web. In this
                research, we build a scalable distributed platform and a
                highperformance geoprocessing workflow based on the Hadoop
                ecosystem to harvest crowd-sourced gazetteer entries. Using
                experiments based on geotagged datasets in Flickr, we find that
                the MapReduce-based workflow running on the spatially enabled
                Hadoop cluster can reduce the processing time compared with
                traditional desktop-based operations by an order of magnitude. We
                demonstrate how to use such a novel spatial-computing
                infrastructure to facilitate gazetteer research. In addition, we
                introduce a provenance-based trust model for quality assurance.
                This work offers new insights on enriching future gazetteers with
                the use of Hadoop clusters, and makes contributions in connecting
                GIS to the cloud computing environment for the next frontier of
                Big Geo-Data analytics.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Gao et al_2017_Constructing gazetteers from
            volunteered Big Geo-Data based on Hadoop.pdf},
}

@article{ghafarian2020,
    title = {Identifying Crisis-Related Informative Tweets Using Learning on
             Distributions},
    author = {Ghafarian, Seyed Hossein and Yazdi, Hadi Sadoghi},
    year = {2020},
    month = mar,
    journal = {Information Processing \& Management},
    volume = {57},
    number = {2},
    pages = {102145},
    issn = {0306-4573},
    doi = {10/gj8br7},
    abstract = {Social networks like Twitter are good means for people to
                express themselves and ask for help in times of crisis. However,
                to provide help, authorities need to identify informative posts
                on the network from the vast amount of non-informative ones to
                better know what is actually happening. Traditional methods for
                identifying informative posts put emphasis on the presence or
                absence of certain words which has limitations for classifying
                these posts. In contrast, in this paper, we propose to consider
                the (overall) distribution of words in the post. To do this,
                based on the distributional hypothesis in linguistics, we assume
                that each tweet is a distribution from which we have drawn a
                sample of words. Building on recent developments in learning
                methods, namely learning on distributions, we propose an approach
                which identifies informative tweets by using distributional
                assumption. Extensive experiments have been performed on Twitter
                data from more than 20 crisis incidents of nearly all types of
                incidents. These experiments show the superiority of the proposed
                approach in a number of real crisis incidents. This implies that
                better modelling of the content of a tweet based on recent
                advances in estimating distributions and using domain-specific
                knowledge for various types of crisis incidents such as floods or
                earthquakes, may help to achieve higher accuracy in the task.},
    langid = {english},
    keywords = {Crisis incidents tweets,Crisis management,Learning on
                distributions},
    file = {/home/cjber/drive/pdf/Ghafarian_Yazdi_2020_Identifying
            crisis-related informative tweets using learning on
            distributions.pdf;/home/cjber/drive/zotero/storage/25LTNWYD/S030645731930322X.html
            },
}

@article{grace2020a,
    title = {Toponym {{Usage}} in {{Social Media}} in {{Emergencies}}},
    author = {Grace, Rob},
    year = {2020},
    month = oct,
    journal = {International Journal of Disaster Risk Reduction},
    pages = {101923},
    issn = {22124209},
    doi = {10/ghjp44},
    abstract = {For emergency responders, the utility of social media hinges on
                available and accurate geographic information. Responders must
                locate events reported on social media to assess a situation,
                coordinate resources, and provide assistance. Unfortunately,
                however, social media often lacks geographic metadata and, if
                available, can inaccurately reflect the locations of reported
                events. Toponyms, place names in social media content, provide
                another source of geographic information yet toponym usage in
                emergencies remains poorly understood.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Grace_2020_Toponym Usage in Social Media in
            Emergencies.pdf},
}

@article{hasan2018,
    title = {A Survey on Real-Time Event Detection from the {{Twitter}} Data
             Stream},
    author = {Hasan, Mahmud and Orgun, Mehmet A and Schwitter, Rolf},
    year = {2018},
    month = aug,
    journal = {Journal of Information Science},
    volume = {44},
    number = {4},
    pages = {443--463},
    issn = {0165-5515, 1741-6485},
    doi = {10/ggnxd6},
    abstract = {The proliferation of social networking services has resulted in
                a rapid growth of their user base, spanning across the world. The
                collective information generated from these online platforms is
                overwhelming, in terms of both the amount of content produced
                every moment and the diversity of topics discussed. The real-time
                nature of the information produced by users has prompted
                researchers to analyse this content, in order to gain timely
                insight into the current state of affairs. Specifically, the
                microblogging service Twitter has been a recent focus of
                researchers to gather information on events occurring in real
                time. This article presents a survey of a wide variety of event
                detection methods applied to streaming Twitter data, classifying
                them according to shared common traits, and then discusses
                different aspects of the subtasks and challenges involved in
                event detection. We believe this survey will act as a guide and
                starting point for aspiring researchers to gain a structured view
                on state-of-the-art real-time event detection and spur further
                research in this direction.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Hasan et al_2018_A survey on real-time event
            detection from the Twitter data stream.pdf},
}

@article{hudson-smith2009,
    title = {{{NeoGeography}} and {{Web}} 2.0: Concepts, Tools and Applications},
    shorttitle = {{{NeoGeography}} and {{Web}} 2.0},
    author = {{Hudson-Smith}, Andrew and Crooks, Andrew and Gibin, Maurizio and
              Milton, Richard and Batty, Michael},
    year = {2009},
    month = jun,
    journal = {Journal of Location Based Services},
    volume = {3},
    number = {2},
    pages = {118--145},
    issn = {1748-9725, 1748-9733},
    doi = {10/fqgsfh},
    langid = {english},
    file = {/home/cjber/drive/pdf/Hudson-Smith et al_2009_NeoGeography and Web
            2.pdf},
}

@inproceedings{hughes2014,
    title = {Online Public Communications by Police \& Fire Services during the
             2012 {{Hurricane Sandy}}},
    booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}}
                 in {{Computing Systems}}},
    author = {Hughes, Amanda L. and St. Denis, Lise A. A. and Palen, Leysia and
              Anderson, Kenneth M.},
    year = {2014},
    month = apr,
    pages = {1505--1514},
    publisher = {{ACM}},
    address = {{Toronto Ontario Canada}},
    doi = {10/gmf7qx},
    abstract = {Social media and other online communication tools are a subject
                of great interest in mass emergency response. Members of the
                public are turning to these solutions to seek and offer emergency
                information. Emergency responders are working to determine what
                social media policies should be in terms of their ``public
                information'' functions. We report on the online communications
                from all the coastal fire and police departments within a 100
                mile radius of Hurricane Sandy's US landfall. Across four types
                of online communication media, we collected data from 840 fire
                and police departments. Findings indicate that few departments
                used these online channels in their Sandy response efforts, and
                that communications differed between fire and police departments
                and across media type. However, among the highly engaged
                departments, there is evidence that they bend and adapt policies
                about what constitutes appropriate public communication in the
                face of emergency demands; therefore, we propose that flexibility
                is important in considering future emergency online communication
                policy. We conclude with design recommendations for making online
                communication media more ``listenable'' for both emergency
                managers and members of the public.},
    isbn = {978-1-4503-2473-1},
    langid = {english},
    file = {/home/cjber/drive/pdf/Hughes et al_2014_Online public communications
            by police & fire services during the 2012.pdf},
}

@article{imran2013,
    title = {Extracting {{Information Nuggets}} from {{Disaster}}- {{Related
             Messages}} in {{Social Media}}},
    author = {Imran, Muhammad and Elbassuoni, Shady and Castillo, Carlos and
              Diaz, Fernando and Meier, Patrick},
    year = {2013},
    pages = {10},
    abstract = {Microblogging sites such as Twitter can play a vital role in
                spreading information during ``natural'' or man-made disasters.
                But the volume and velocity of tweets posted during crises today
                tend to be extremely high, making it hard for disaster-affected
                communities and professional emergency responders to process the
                information in a timely manner. Furthermore, posts tend to vary
                highly in terms of their subjects and usefulness; from messages
                that are entirely off-topic or personal in nature, to messages
                containing critical information that augments situational
                awareness. Finding actionable information can accelerate disaster
                response and alleviate both property and human losses. In this
                paper, we describe automatic methods for extracting information
                from microblog posts. Specifically, we focus on extracting
                valuable ``information nuggets'', brief, self-contained
                information items relevant to disaster response. Our methods
                leverage machine learning methods for classifying posts and
                information extraction. Our results, validated over one large
                disaster-related dataset, reveal that a careful design can yield
                an effective system, paving the way for more sophisticated data
                analysis and visualization systems.},
    langid = {english},
    keywords = {⛔ No DOI found},
    file = {/home/cjber/drive/pdf/Imran et al_2013_Extracting Information
            Nuggets from Disaster- Related Messages in Social Media.pdf},
}

@article{imran2015,
    title = {Processing {{Social Media Messages}} in {{Mass Emergency}}: A {{
             Survey}}},
    shorttitle = {Processing {{Social Media Messages}} in {{Mass Emergency}}},
    author = {Imran, Muhammad and Castillo, Carlos and Diaz, Fernando and Vieweg
              , Sarah},
    year = {2015},
    month = jul,
    journal = {ACM Computing Surveys},
    volume = {47},
    number = {4},
    pages = {1--38},
    issn = {0360-0300, 1557-7341},
    doi = {10/gdxwgx},
    abstract = {Social media platforms provide active communication channels
                during mass convergence and emergency events such as disasters
                caused by natural hazards. As a result, first responders,
                decision makers, and the public can use this information to gain
                insight into the situation as it unfolds. In particular, many
                social media messages communicated during emergencies convey
                timely, actionable information. Processing social media messages
                to obtain such information, however, involves solving multiple
                challenges including: parsing brief and informal messages,
                handling information overload, and prioritizing different types
                of information found in messages. These challenges can be mapped
                to classical information processing operations such as filtering,
                classifying, ranking, aggregating, extracting, and summarizing.
                We survey the state of the art regarding computational methods to
                process social media messages and highlight both their
                contributions and shortcomings. In addition, we examine their
                particularities, and methodically examine a series of key
                subproblems ranging from the detection of events to the creation
                of actionable and useful summaries. Research thus far has, to a
                large extent, produced methods to extract situational awareness
                information from social media. In this survey, we cover these
                various approaches, and highlight their benefits and
                shortcomings. We conclude with research challenges that go beyond
                situational awareness, and begin to look at supporting decision
                making and coordinating emergency-response actions.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Imran et al_2015_Processing Social Media
            Messages in Mass Emergency.pdf},
}

@article{imran2020,
    title = {Using {{AI}} and {{Social Media Multimodal Content}} for {{Disaster
             Response}} and {{Management}}: Opportunities, {{Challenges}}, and {{
             Future Directions}}},
    shorttitle = {Using {{AI}} and {{Social Media Multimodal Content}} for {{
                  Disaster Response}} and {{Management}}},
    author = {Imran, Muhammad and Ofli, Ferda and Caragea, Doina and Torralba,
              Antonio},
    year = {2020},
    month = sep,
    journal = {Information Processing \& Management},
    volume = {57},
    number = {5},
    pages = {102261},
    issn = {03064573},
    doi = {10/gmf7s4},
    abstract = {People increasingly use Social Media (SM) platforms such as
                Twitter and Facebook during disasters and emergencies to post
                situational updates including reports of injured or dead people,
                infrastructure damage, requests of urgent needs, and the like.
                Information on SM comes in many forms, such as textual messages,
                images, and videos. Several studies have shown the utility of SM
                information for disaster response and management, which
                encouraged humanitarian organizations to start incorporating SM
                data sources into their workflows. However, several challenges
                prevent these organizations from using SM data for response
                efforts. These challenges include near-real-time information
                processing, information overload, information extraction,
                summarization, and verification of both textual and visual
                content. We highlight various applications and opportunities of
                SM multimodal data, latest advancements, current challenges, and
                future directions for the crisis informatics and other related
                research fields.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Imran et al_2020_Using AI and Social Media
            Multimodal Content for Disaster Response
            and.pdf;/home/cjber/drive/pdf/Imran et al_2020_Using AI and Social
            Media Multimodal Content for Disaster Response and2.pdf},
}

@inproceedings{iyengar2011,
    title = {Content-{{Based Prediction}} of {{Temporal Boundaries}} for {{
             Events}} in {{Twitter}}},
    booktitle = {2011 {{IEEE Third Int}}'l {{Conference}} on {{Privacy}}, {{
                 Security}}, {{Risk}} and {{Trust}} and 2011 {{IEEE Third Int}}'l
                 {{Conference}} on {{Social Computing}}},
    author = {Iyengar, Akshaya and Finin, Tim and Joshi, Anupam},
    year = {2011},
    month = oct,
    pages = {186--191},
    publisher = {{IEEE}},
    address = {{Boston, MA, USA}},
    doi = {10/fzz5ns},
    abstract = {Social media services like Twitter, Flickr and YouTube publish
                high volumes of user generated content as a major event occurs,
                making them a potential data source for event analysis. The large
                volume and noisy content of social media makes automatic
                preprocessing essential. Intuitively, the eventrelated data falls
                into three major phases: the buildup to the event, the event
                itself, and the post-event effects and repercussions. We describe
                an approach to automatically determine when an anticipated event
                started and ended by analyzing the content of tweets using an SVM
                classifier and hidden Markov model. We evaluate our performance
                by predicting event boundaries on Twitter data for a set of
                events in the domains of sports, weather and social activities.},
    isbn = {978-1-4577-1931-8 978-0-7695-4578-3},
    langid = {english},
    file = {/home/cjber/drive/pdf/Iyengar et al_2011_Content-Based Prediction of
            Temporal Boundaries for Events in Twitter.pdf},
}

@article{joshi2018,
    title = {Twitter {{Sentiment Analysis System}}},
    author = {Joshi, Shaunak and Deshpande, Deepali},
    year = {2018},
    month = jun,
    journal = {International Journal of Computer Applications},
    volume = {180},
    number = {47},
    pages = {35--39},
    issn = {09758887},
    doi = {10/gdvjxg},
    abstract = {Social media is increasingly used by humans to express their
                feelings and opinions in the form of short text messages.
                Detecting sentiments in text has a wide range of applications
                including identifying anxiety or depression of individuals and
                measuring well-being or mood of a community. Sentiments can be
                expressed in many ways that can be seen such as facial expression
                and gestures, speech and by written text. Sentiment Analysis in
                text documents is essentially a content \textendash{} based
                classification problem involving concepts from the domains of
                Natural Language Processing as well as Machine Learning. In this
                paper, sentiment recognition based on textual data and the
                techniques used in sentiment analysis are discussed.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Joshi_Deshpande_2018_Twitter Sentiment
            Analysis System.pdf},
}

@article{kim2018,
    title = {Social Network Analysis: Characteristics of Online Social Networks
             after a Disaster},
    shorttitle = {Social Network Analysis},
    author = {Kim, Jooho and Hastak, Makarand},
    year = {2018},
    month = feb,
    journal = {International Journal of Information Management},
    volume = {38},
    number = {1},
    pages = {86--96},
    issn = {02684012},
    doi = {10/gcqdv5},
    abstract = {Social media, such as Twitter and Facebook, plays a critical
                role in disaster management by propagating emergency information
                to a disaster-affected community. It ranks as the fourth most
                popular source for accessing emergency information. Many studies
                have explored social media data to understand the networks and
                extract critical information to develop a pre- and post-disaster
                mitigation plan.},
    langid = {english},
    keywords = {Disaster communication,Disaster response,Emergency information,
                Social media,Social network analysis (SNA)},
    file = {/home/cjber/drive/pdf/Kim_Hastak_2018_Social network
            analysis.pdf;/home/cjber/drive/pdf/Kim_Hastak_2018_Social network
            analysis2.pdf;/home/cjber/drive/zotero/storage/5GQPVPME/S026840121730525X.html
            },
}

@article{kron2019,
    title = {Changes in Risk of Extreme Weather Events in {{Europe}}},
    author = {Kron, Wolfgang and L{\"o}w, Petra and Kundzewicz, Zbigniew W.},
    year = {2019},
    month = oct,
    journal = {Environmental Science \& Policy},
    volume = {100},
    pages = {74--83},
    issn = {14629011},
    doi = {10/gmhcpm},
    abstract = {Over the last decades, the damage caused by weather events has
                increased dramatically and ubiquitously. In Europe, weather
                catastrophes constitute a growing burden on national economies
                and insurance companies, not least because of the costs of
                precautionary measures. For a long time, the insurance sector has
                flagged that weather disasters are on the rise, both in terms of
                the number of occurrences and material damage caused. The main
                reasons for this are: increase in the number and area of
                settlements in exposed areas, the accumulation of ever more
                valuable and vulnerable assets in these areas, as well as the
                climate and environmental changes that have already taken place.
                This paper examines observed changes in risk of various
                categories of weather disasters in Europe, backed by statistical
                analyses of relevant, updated information originating from a
                valuable and quite unique source, Munich Re's NatCatSERVICE
                database, that is of considerable interest and value to the
                scientific community and beyond (e.g. in the reinsurance and
                insurance industries). The paper also calls for partnership in
                the reduction of risk of weather extremes and discusses the role
                of the insurance industry.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Kron et al_2019_Changes in risk of extreme
            weather events in Europe.pdf},
}

@article{kryvasheyeu2016,
    title = {Rapid Assessment of Disaster Damage Using Social Media Activity},
    author = {Kryvasheyeu, Yury and Chen, Haohui and Obradovich, Nick and Moro,
              Esteban and Van Hentenryck, Pascal and Fowler, James and Cebrian,
              Manuel},
    year = {2016},
    month = mar,
    journal = {Science Advances},
    volume = {2},
    number = {3},
    pages = {e1500779},
    issn = {2375-2548},
    doi = {10/gc5tfp},
    abstract = {Could social media data aid in disaster response and damage
                assessment? Countries face both an increasing frequency and an
                increasing intensity of natural disasters resulting from climate
                change. During such events, citizens turn to social media
                platforms for disaster-related communication and information.
                Social media improves situational awareness, facilitates
                dissemination of emergency information, enables early warning
                systems, and helps coordinate relief efforts. In addition, the
                spatiotemporal distribution of disaster-related messages helps
                with the real-time monitoring and assessment of the disaster
                itself. We present a multiscale analysis of Twitter activity
                before, during, and after Hurricane Sandy. We examine the online
                response of 50 metropolitan areas of the United States and find a
                strong relationship between proximity to Sandy's path and
                hurricane-related social media activity. We show that real and
                perceived threats, together with physical disaster effects, are
                directly observable through the intensity and composition of
                Twitter's message stream. We demonstrate that per-capita Twitter
                activity strongly correlates with the per-capita economic damage
                inflicted by the hurricane. We verify our findings for a wide
                range of disasters and suggest that massive online social
                networks can be used for rapid assessment of damage caused by a
                large-scale disaster.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Kryvasheyeu et al_2016_Rapid assessment of
            disaster damage using social media activity.pdf},
}

@article{laylavi2016,
    title = {A {{Multi}}-{{Element Approach}} to {{Location Inference}} of {{
             Twitter}}: A {{Case}} for {{Emergency Response}}},
    shorttitle = {A {{Multi}}-{{Element Approach}} to {{Location Inference}} of
                  {{Twitter}}},
    author = {Laylavi, Farhad and Rajabifard, Abbas and Kalantari, Mohsen},
    year = {2016},
    month = may,
    journal = {ISPRS International Journal of Geo-Information},
    volume = {5},
    number = {5},
    pages = {56},
    publisher = {{Multidisciplinary Digital Publishing Institute}},
    doi = {10/f8v96g},
    abstract = {Since its inception, Twitter has played a major role in
                real-world events\textemdash especially in the aftermath of
                disasters and catastrophic incidents, and has been increasingly
                becoming the first point of contact for users wishing to provide
                or seek information about such situations. The use of Twitter in
                emergency response and disaster management opens up avenues of
                research concerning different aspects of Twitter data quality,
                usefulness and credibility. A real challenge that has attracted
                substantial attention in the Twitter research community exists in
                the location inference of twitter data. Considering that less
                than 2\% of tweets are geotagged, finding location inference
                methods that can go beyond the geotagging capability is
                undoubtedly the priority research area. This is especially true
                in terms of emergency response, where spatial aspects of
                information play an important role. This paper introduces a
                multi-elemental location inference method that puts the
                geotagging aside and tries to predict the location of tweets by
                exploiting the other inherently attached data elements. In this
                regard, textual content, users' profile location and place
                labelling, as the main location-related elements, are taken into
                account. Location-name classes in three granularity levels are
                defined and employed to look up the location references from the
                location-associated elements. The inferred location of the finest
                granular level is assigned to a tweet, based on a novel location
                assignment rule. The location assigned by the location inference
                process is considered to be the inferred location of a tweet, and
                is compared with the geotagged coordinates as the ground truth of
                the study. The results show that this method is able to
                successfully infer the location of 87\% of the tweets at the
                average distance error of 12.2 km and the median distance error
                of 4.5 km, which is a significant improvement compared with that
                of the current methods that can predict the location with much
                larger distance errors or at a city-level resolution at best.},
    copyright = {http://creativecommons.org/licenses/by/3.0/},
    langid = {english},
    keywords = {emergency response,location inference,social media,twitter},
    file = {/home/cjber/drive/pdf/Laylavi et al_2016_A Multi-Element Approach to
            Location Inference of
            Twitter.pdf;/home/cjber/drive/zotero/storage/CIYJPY4Y/56.html},
}

@article{li2018b,
    title = {Disaster Response Aided by Tweet Classification with a Domain
             Adaptation Approach},
    author = {Li, Hongmin and Caragea, Doina and Caragea, Cornelia and Herndon,
              Nic},
    year = {2018},
    journal = {Journal of Contingencies and Crisis Management},
    volume = {26},
    number = {1},
    pages = {16--27},
    issn = {1468-5973},
    doi = {10/gc35fc},
    abstract = {Social media platforms such as Twitter provide valuable
                information for aiding disaster response during emergency events.
                Machine learning could be used to identify such information.
                However, supervised learning algorithms rely on labelled data,
                which is not readily available for an emerging target disaster.
                While labelled data might be available for a prior source
                disaster, supervised classifiers learned only from the source
                disaster may not perform well on the target disaster, as each
                event has unique characteristics (e.g., type, location, and
                culture) and may cause different social media responses. To
                address this limitation, we propose to use a domain adaptation
                approach, which learns classifiers from unlabelled target data,
                in addition to source labelled data. Our approach uses the Na\"
                ive Bayes classifier, together with an iterative Self-Training
                strategy. Experimental results on the task of identifying tweets
                relevant to a disaster of interest show that the domain
                adaptation classifiers are better as compared to the supervised
                classifiers learned only from labelled source data.},
    langid = {english},
    keywords = {classification,disaster response,domain adaptation,Twitter},
    annotation = {\_eprint:
                  https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-5973.12194
                  },
    file = {/home/cjber/drive/pdf/Li et al_2018_Disaster response aided by tweet
            classification with a domain
            adaptation.pdf;/home/cjber/drive/zotero/storage/IWSN8FME/1468-5973.html
            },
}

@article{lin2016,
    title = {Crisis Communication, Learning and Responding: Best Practices in
             Social Media},
    shorttitle = {Crisis Communication, Learning and Responding},
    author = {Lin, Xialing and Spence, Patric R. and Sellnow, Timothy L. and
              Lachlan, Kenneth A.},
    year = {2016},
    month = dec,
    journal = {Computers in Human Behavior},
    volume = {65},
    pages = {601--605},
    issn = {0747-5632},
    doi = {10/ggff7z},
    abstract = {As noted by Seeger (2006) the notion of best practices is often
                use to improve professional practice; to create research and
                functional recommendations to use in a specific situation. This
                essay describes best practices in crisis communication
                specifically through the use of social media. It provides
                suggestions and approaches for improving the effectiveness of
                crisis communication and learning with and between organizations,
                governments and citizens. Seven best practices for effective
                crisis communication using social media are outlined.},
    langid = {english},
    keywords = {Best practices,Communicaiton,Crisis communication,Risk,Social
                media},
    file = {/home/cjber/drive/pdf/Lin et al_2016_Crisis communication, learning
            and
            responding.pdf;/home/cjber/drive/zotero/storage/UZBP5PHV/S0747563216304137.html
            },
}

@article{lorini2019,
    title = {Integrating {{Social Media}} into a {{Pan}}-{{European Flood
             Awareness System}}: A {{Multilingual Approach}}},
    shorttitle = {Integrating {{Social Media}} into a {{Pan}}-{{European Flood
                  Awareness System}}},
    author = {Lorini, V. and Castillo, C. and Dottori, F. and Kalas, M. and
              Nappo, D. and Salamon, P.},
    year = {2019},
    month = apr,
    journal = {arXiv:1904.10876 [cs]},
    eprint = {1904.10876},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {This paper describes a prototype system that integrates social
                media analysis into the European Flood Awareness System (EFAS).
                This integration allows the collection of social media data to be
                automatically triggered by flood risk warnings determined by a
                hydro-meteorological model. Then, we adopt a multi-lingual
                approach to find flood-related messages by employing two
                state-of-the-art methodologies: language-agnostic word embeddings
                and language-aligned word embeddings. Both approaches can be used
                to bootstrap a classifier of social media messages for a new
                language with little or no labeled data. Finally, we describe a
                method for selecting relevant and representative messages and
                displaying them back in the interface of EFAS.},
    archiveprefix = {arXiv},
    langid = {english},
    keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,
                Computer Science - Computation and Language,Computer Science -
                Information Retrieval},
    file = {/home/cjber/drive/pdf/Lorini et al_2019_Integrating Social Media
            into a Pan-European Flood Awareness System.pdf},
}

@article{lu2012,
    title = {Predictability of Population Displacement after the 2010 {{Haiti}}
             Earthquake},
    author = {Lu, X. and Bengtsson, L. and Holme, P.},
    year = {2012},
    month = jul,
    journal = {Proceedings of the National Academy of Sciences},
    volume = {109},
    number = {29},
    pages = {11576--11581},
    issn = {0027-8424, 1091-6490},
    doi = {10/f2z47x},
    langid = {english},
    file = {/home/cjber/drive/pdf/Lu et al_2012_Predictability of population
            displacement after the 2010 Haiti earthquake.pdf},
}

@article{mani2010,
    ids = {mani2010a},
    title = {{{SpatialML}}: Annotation Scheme, Resources, and Evaluation},
    shorttitle = {{{SpatialML}}},
    author = {Mani, Inderjeet and Doran, Christy and Harris, Dave and Hitzeman,
              Janet and Quimby, Rob and Richer, Justin and Wellner, Ben and
              Mardis, Scott and Clancy, Seamus},
    year = {2010},
    month = sep,
    journal = {Language Resources and Evaluation},
    volume = {44},
    number = {3},
    pages = {263--280},
    issn = {1574-020X, 1574-0218},
    doi = {10/cnmp8m},
    langid = {english},
    file = {/home/cjber/drive/pdf/Mani et
            al_2010_SpatialML.pdf;/home/cjber/drive/zotero/storage/Q3VUBRJA/Mani
            et al_2010_SpatialML2.pdf},
}

@article{martinez-rojas2018,
    title = {Twitter as a Tool for the Management and Analysis of Emergency
             Situations: A Systematic Literature Review},
    shorttitle = {Twitter as a Tool for the Management and Analysis of Emergency
                  Situations},
    author = {{Mart{\'i}nez-Rojas}, Mar{\'i}a and {Pardo-Ferreira}, Mar{\'i}a
              del Carmen and {Rubio-Romero}, Juan Carlos},
    year = {2018},
    month = dec,
    journal = {International Journal of Information Management},
    volume = {43},
    pages = {196--208},
    issn = {0268-4012},
    doi = {10/gfmbxd},
    abstract = {The importance of timely, accurate and effective use of
                available information is essential to the proper management of
                emergency situations. In recent years, emerging technologies have
                provided new approaches towards the distribution and acquisition
                of crowdsourced information to facilitate situational awareness
                and management during emergencies. In this regard, internet and
                social networks have shown potential to be an effective tool in
                disseminating and obtaining up-to-date information. Among the
                most popular social networks, research has pointed to Twitter as
                a source of information that offers valuable real-time data for
                decision-making. The objective of this paper is to conduct a
                systematic literature review that provides an overview of the
                current state of research concerning the use of Twitter to
                emergencies management, as well as presents the challenges and
                future research directions.},
    langid = {english},
    keywords = {Data,Emergencies,Management,Review,Social network,Twitter},
    file = {/home/cjber/drive/pdf/Martínez-Rojas et al_2018_Twitter as a tool
            for the management and analysis of emergency
            situations.pdf;/home/cjber/drive/zotero/storage/TU826T2M/S0268401218303499.html
            },
}

@inproceedings{mathioudakis2010,
    title = {{{TwitterMonitor}}: Trend Detection over the Twitter Stream},
    shorttitle = {{{TwitterMonitor}}},
    booktitle = {Proceedings of the 2010 {{ACM SIGMOD International Conference}}
                 on {{Management}} of Data},
    author = {Mathioudakis, Michael and Koudas, Nick},
    year = {2010},
    month = jun,
    pages = {1155--1158},
    publisher = {{ACM}},
    address = {{Indianapolis Indiana USA}},
    doi = {10/dr8qjr},
    abstract = {We present TwitterMonitor, a system that performs trend
                detection over the Twitter stream. The system identifies emerging
                topics (i.e. `trends') on Twitter in real time and provides
                meaningful analytics that synthesize an accurate description of
                each topic. Users interact with the system by ordering the
                identified trends using different criteria and submitting their
                own description for each trend.},
    isbn = {978-1-4503-0032-2},
    langid = {english},
    file = {/home/cjber/drive/pdf/Mathioudakis_Koudas_2010_TwitterMonitor.pdf},
}

@book{mazzoleni2017,
    title = {Improving {{Flood Prediction Assimilating Uncertain Crowdsourced
             Data}} into {{Hydrologic}} and {{Hydraulic Models}}},
    author = {Mazzoleni, Maurizio},
    year = {2017},
    abstract = {Cover -- Half Title -- Title Page -- Copyright Page --
                Dedication -- Acknoledgments -- Summary -- Samenvatting --
                Sommario -- Table of Contents -- 1: Introduction -- 1.1
                Background -- 1.1.1 Flood forecasting and early warning systems
                -- 1.1.2 Hydrological and hydrodynamic modelling -- 1.1.3
                Uncertainty in hydrological and hydrodynamic modelling -- 1.1.4
                Data assimilation -- 1.1.5 Citizen Science -- 1.2 Motivation --
                1.3 Terminology -- 1.4 Research objectives -- 1.5 Outline of the
                thesis -- 2: Case studies and models -- 2.1 Introduction -- 2.2
                Case 1 - Brue Catchment (UK) -- 2.2.1 Catchment description --
                2.2.2 Model description -- 2.3 Case 2 - Bacchiglione Catchment (
                Italy) -- 2.3.1 Catchment description -- 2.3.2 Model description
                -- 2.4 Case 3 - Trinity and Sabine Rivers (USA) -- 2.4.1 Rivers
                description -- 2.4.2 Model description -- 2.5 Case 4 - Synthetic
                river reach -- 3: Data assimilation methods -- 3.1 Introduction
                -- 3.2 Direct insertion -- 3.3 Nudging scheme -- 3.4 Kalman
                Filter -- 3.5 Ensemble Kalman Filter -- 3.6 Asynchronous Ensemble
                Kalman Filter -- 4: Assimilation of synchronous data in
                hydrological models -- 4.1 Introduction -- 4.2 Methodology --
                4.2.1 Assimilation of intermittent observations -- 4.2.2
                Observation and model error -- 4.2.3 Generation of synthetic
                observations -- 4.3 Experimental setup -- 4.3.1 Experiment 4.1:
                Streamflow data from static physical (StPh) sensors -- 4.3.2
                Experiment 4.2: Streamflow data from static social (StSc) sensors
                -- 4.3.3 Experiment 4.3: Intermittent streamflow data from static
                social (StSc) sensors -- 4.3.4 Experiment 4.4: Heterogeneous
                network of static physical (StPh) and static social (StSc)
                sensors -- 4.4 Results and discussion -- 4.4.1 Experiment 4.1 --
                4.4.2 Experiment 4.2 -- 4.4.3 Experiment 4.3 -- 4.4.4 Experiment
                4.4 -- 4.5 Conclusions 5: Assimilation of asynchronous data in
                hydrological models -- 5.1 Introduction -- 5.2 Methodology --
                5.2.1 Assimilation of asynchronous observations -- 5.2.2
                Observation and model error -- 5.2.3 Generation of synthetic
                observations -- 5.3 Experimental setup -- 5.3.1 Experiment 5.1:
                Observations from a single static social (StSc) sensor -- 5.3.2
                Experiments 5.2: Observations from distributed static physical (
                StPh) and static social (StSc) sensors -- 5.4 Results and
                discussion -- 5.4.1 Experiment 5.1 -- 5.4.2 Experiment 5.2 -- 5.5
                Conclusions -- 6: Assimilation of synchronous data in hydraulic
                models -- 6.1 Introduction -- 6.2 Methodology -- 6.2.1 Data
                assimilation methods -- 6.2.2 Observation and model error --
                6.2.3 Streamflow observations -- 6.3 Experimental setup -- 6.3.1
                Experiment 6.1: Effect of different DA methods -- 6.3.2
                Experiment 6.2: Effect of sensors location on KF performances --
                6.4 Results and discussions -- 6.4.1 Experiment 6.1 -- 6.4.2
                Experiment 6.2 -- 6.5 Conclusions -- 7: Assimilation of
                synchronous data in a cascade of models -- 7.1 Introduction --
                7.2 Methodology -- 7.2.1 Data assimilation method -- 7.2.2
                Observation and model error -- 7.2.3 Generation of synthetic
                observations -- 7.3 Experimental setup -- 7.3.1 Experiment 7.1:
                Assimilation of data from static physical (StPh) sensors -- 7.3.2
                Experiment 7.2: Assimilation of data from static social (StSc)
                sensors -- 7.3.3 Experiment 7.3: Assimilation of data from
                dynamic social (DySc) sensors -- 7.3.4 Experiment 7.4: Realistic
                scenarios of engagements -- 7.4 Results and discussion -- 7.4.1
                Experiment 7.1 -- 7.4.2 Experiment 7.2 -- 7.4.3 Experiment 7.3 --
                7.4.4 Experiment 7.4 -- 7.5 Conclusions -- 8: Conclusions and
                recommendations -- 8.1 Overview -- 8.2 Research outcomes -- 8.3
                Limitations and recommendations -- References -- List of acronyms
                -- List of Table -- List of Figures},
    isbn = {978-1-351-65256-8 978-1-138-03590-4},
    langid = {english},
    annotation = {OCLC: 1021121623},
    file = {/home/cjber/drive/zotero/storage/RHT8CV4W/Mazzoleni - 2017 -
            Improving Flood Prediction Assimilating Uncertain .pdf},
}

@misc{metoffice,
    title = {Met {{Office WOW}}},
    author = {{Met Office}},
    abstract = {The UK Met Office Weather Observation Website (WOW). WOW allows
                anyone to submit their own weather data, anywhere in the world.},
    howpublished = {https://wow.metoffice.gov.uk/},
    file = {/home/cjber/drive/zotero/storage/2RSJF9FZ/wow.metoffice.gov.uk.html},
}

@article{middleton2014,
    title = {Real-{{Time Crisis Mapping}} of {{Natural Disasters Using Social
             Media}}},
    author = {Middleton, Stuart E. and Middleton, Lee and Modafferi, Stefano},
    year = {2014},
    month = mar,
    journal = {IEEE Intelligent Systems},
    volume = {29},
    number = {2},
    pages = {9--17},
    issn = {1541-1672},
    doi = {10/gfv7c6},
    abstract = {We present a social media crisis mapping platform for natural
                disasters. We take locations from gazetteer, street map and
                volunteered geographic information (VGI) sources for areas at
                risk of disaster and match them to geo-parsed real-time tweet
                data streams. We use statistical analysis to generate real-time
                crisis maps. Geo-parsing results are benchmarked against existing
                published work and evaluated across multi-lingual datasets. We
                report two case studies comparing 5-day tweet crisis maps to
                official post-event impact assessment from the US National
                Geospatial Agency (NGA) compiled from verified satellite and
                aerial imagery sources.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Middleton et al_2014_Real-Time Crisis Mapping
            of Natural Disasters Using Social Media.pdf},
}

@article{mikolov2013,
    title = {Efficient Estimation of Word Representations in Vector Space},
    author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    year = {2013},
    journal = {arXiv preprint arXiv:1301.3781},
    eprint = {1301.3781},
    eprinttype = {arxiv},
    archiveprefix = {arXiv},
    keywords = {\#nosource,⛔ No DOI found},
}

@article{morstatter2013,
    title = {Is the {{Sample Good Enough}}? Comparing {{Data}} from {{Twitter}}
             's {{Streaming API}} with {{Twitter}}'s {{Firehose}}},
    author = {Morstatter, Fred and Pfeffer, Juergen and Liu, Huan and Carley,
              Kathleen M},
    year = {2013},
    pages = {9},
    langid = {english},
    keywords = {⛔ No DOI found},
    file = {/home/cjber/drive/pdf/Morstatter et al_2013_Is the Sample Good
            Enough.pdf},
}

@article{muller2015,
    title = {Crowdsourcing for Climate and Atmospheric Sciences: Current Status
             and Future Potential},
    shorttitle = {Crowdsourcing for Climate and Atmospheric Sciences},
    author = {Muller, C. L. and Chapman, L. and Johnston, S. and Kidd, C. and
              Illingworth, S. and Foody, G. and Overeem, A. and Leigh, R. R.},
    year = {2015},
    journal = {International Journal of Climatology},
    volume = {35},
    number = {11},
    pages = {3185--3203},
    issn = {1097-0088},
    doi = {10/f7qrps},
    abstract = {Crowdsourcing is traditionally defined as obtaining data or
                information by enlisting the services of a (potentially large)
                number of people. However, due to recent innovations, this
                definition can now be expanded to include `and/or from a range of
                public sensors, typically connected via the Internet.' A large
                and increasing amount of data is now being obtained from a huge
                variety of non-traditional sources \textendash{} from smart phone
                sensors to amateur weather stations to canvassing members of the
                public. Some disciplines (e.g. astrophysics, ecology) are already
                utilizing crowdsourcing techniques (e.g. citizen science
                initiatives, web 2.0 technology, low-cost sensors), and while its
                value within the climate and atmospheric science disciplines is
                still relatively unexplored, it is beginning to show promise.
                However, important questions remain; this paper introduces and
                explores the wide-range of current and prospective methods to
                crowdsource atmospheric data, investigates the quality of such
                data and examines its potential applications in the context of
                weather, climate and society. It is clear that crowdsourcing is
                already a valuable tool for engaging the public, and if
                appropriate validation and quality control procedures are adopted
                and implemented, it has much potential to provide a valuable
                source of high temporal and spatial resolution, real-time data,
                especially in regions where few observations currently exist,
                thereby adding value to science, technology and society.},
    langid = {english},
    keywords = {Amateur,Applications,Big data,Citizen science,Internet of things
                ,Sensors},
    annotation = {\_eprint:
                  https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/joc.4210},
    file = {/home/cjber/drive/pdf/Muller et al_2015_Crowdsourcing for climate
            and atmospheric
            sciences.pdf;/home/cjber/drive/zotero/storage/VUUZPAPQ/joc.html},
}

@article{nakayama2018,
    title = {Doccano: Text Annotation for Humans},
    author = {Nakayama, Hiroki and Kubo, Takahiro and Kamura, Junya and
              Taniguchi, Yasufumi and Liang, Xu},
    year = {2018},
    keywords = {⛔ No DOI found},
}

@article{nguyen2017,
    title = {Robust {{Classification}} of {{Crisis}}-{{Related Data}} on {{
             Social Networks Using Convolutional Neural Networks}}},
    author = {Nguyen, Dat and Mannai, Kamela Ali Al and Joty, Shafiq and Sajjad,
              Hassan and Imran, Muhammad and Mitra, Prasenjit},
    year = {2017},
    month = may,
    journal = {Proceedings of the International AAAI Conference on Web and
               Social Media},
    volume = {11},
    number = {1},
    pages = {632--635},
    issn = {2334-0770},
    abstract = {The role of social media, in particular microblogging platforms
                such as Twitter, as a conduit for actionable and tactical
                information during disasters is increasingly acknowledged.
                However, time-critical analysis of big crisis data on social
                media streams brings challenges to machine learning techniques,
                especially the ones that use supervised learning. The scarcity of
                labeled data, particularly in the early hours of a crisis, delays
                the learning process. Existing classification methods require a
                significant amount of labeled data specific to a particular event
                for training plus a lot of feature engineering to achieve best
                results. In this work, we introduce neural network based
                classification methods for identifying useful tweets during a
                crisis situation. At the onset of a disaster when no labeled data
                is available, our proposed method makes the best use of the
                out-of-event data and achieves good results.},
    copyright = {Copyright (c) 2021 Proceedings of the International AAAI
                 Conference on Web and Social Media},
    langid = {english},
    keywords = {⛔ No DOI found},
    file = {/home/cjber/drive/pdf/Nguyen et al_2017_Robust Classification of
            Crisis-Related Data on Social Networks Using.pdf},
}

@article{northcutt2021,
    title = {Pervasive {{Label Errors}} in {{Test Sets Destabilize Machine
             Learning Benchmarks}}},
    author = {Northcutt, Curtis G. and Athalye, Anish and Mueller, Jonas},
    year = {2021},
    month = apr,
    journal = {arXiv:2103.14749 [cs, stat]},
    eprint = {2103.14749},
    eprinttype = {arxiv},
    primaryclass = {cs, stat},
    abstract = {We identify label errors in the test sets of 10 of the most
                commonly-used computer vision, natural language, and audio
                datasets, and subsequently study the potential for these label
                errors to affect benchmark results. Errors in test sets are
                numerous and widespread: we estimate an average of 3.4\% errors
                across the 10 datasets,1 where for example 2916 label errors
                comprise 6\% of the ImageNet validation set. Putative label
                errors are identified using confident learning algorithms and
                then human-validated via crowdsourcing (54\% of the
                algorithmically-flagged candidates are indeed erroneously labeled
                ). Traditionally, machine learning practitioners choose which
                model to deploy based on test accuracy \textemdash{} our findings
                advise caution here, proposing that judging models over correctly
                labeled test sets may be more useful, especially for noisy
                real-world datasets. Surprisingly, we find that lower capacity
                models may be practically more useful than higher capacity models
                in real-world datasets with high proportions of erroneously
                labeled data. For example, on ImageNet with corrected labels:
                ResNet-18 outperforms ResNet50 if the prevalence of originally
                mislabeled test examples increases by just 6\%. On CIFAR-10 with
                corrected labels: VGG-11 outperforms VGG-19 if the prevalence of
                originally mislabeled test examples increases by just 5\%.},
    archiveprefix = {arXiv},
    langid = {english},
    keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,
                Computer Science - Machine Learning,Statistics - Machine Learning
                },
    file = {/home/cjber/drive/zotero/storage/DFF4JUHY/Northcutt et al. - 2021 -
            Pervasive Label Errors in Test Sets Destabilize Ma.pdf},
}

@inproceedings{olteanu2015,
    title = {What to {{Expect When}} the {{Unexpected Happens}}: Social {{Media
             Communications Across Crises}}},
    shorttitle = {What to {{Expect When}} the {{Unexpected Happens}}},
    booktitle = {Proceedings of the 18th {{ACM Conference}} on {{Computer
                 Supported Cooperative Work}} \& {{Social Computing}}},
    author = {Olteanu, Alexandra and Vieweg, Sarah and Castillo, Carlos},
    year = {2015},
    month = feb,
    pages = {994--1009},
    publisher = {{ACM}},
    address = {{Vancouver BC Canada}},
    doi = {10/gmmfbh},
    abstract = {The use of social media to communicate timely information during
                crisis situations has become a common practice in recent years.
                In particular, the one-to-many nature of Twitter has created an
                opportunity for stakeholders to disseminate crisis-relevant
                messages, and to access vast amounts of information they may not
                otherwise have. Our goal is to understand what affected
                populations, response agencies and other stakeholders can expect
                \textemdash and not expect\textemdash from these data in various
                types of disaster situations. Anecdotal evidence suggests that
                different types of crises elicit different reactions from Twitter
                users, but we have yet to see whether this is in fact the case.
                In this paper, we investigate several crises\textemdash including
                natural hazards and human-induced disasters\textemdash in a
                systematic manner and with a consistent methodology. This leads
                to insights about the prevalence of different information types
                and sources across a variety of crisis situations.},
    isbn = {978-1-4503-2922-4},
    langid = {english},
    file = {/home/cjber/drive/zotero/storage/UPF3Y28W/Olteanu et al. - 2015 -
            What to Expect When the Unexpected Happens Social.pdf},
}

@incollection{paszke2019,
    title = {{{PyTorch}}: An Imperative Style, High-Performance Deep Learning
             Library},
    booktitle = {Advances in Neural Information Processing Systems 32},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam
              and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin
              , Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison,
              Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and
              Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and
              Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}
              -Buc}, F. and Fox, E. and Garnett, R.},
    year = {2019},
    pages = {8024--8035},
    publisher = {{Curran Associates, Inc.}},
}

@article{pekar2020,
    title = {Early Detection of Heterogeneous Disaster Events Using Social Media
             },
    author = {Pekar, Viktor and Binner, Jane and Najafi, Hossein and Hale, Chris
              and Schmidt, Vincent},
    year = {2020},
    month = jan,
    journal = {Journal of the Association for Information Science and Technology
               },
    volume = {71},
    number = {1},
    pages = {43--54},
    issn = {2330-1635, 2330-1643},
    doi = {10/ggwkh5},
    langid = {english},
    file = {/home/cjber/drive/zotero/storage/P9ZS46ST/Pekar et al. - 2020 -
            Early detection of heterogeneous disaster events u.pdf},
}

@inproceedings{pennington2014,
    ids = {pennington2014a},
    title = {{{GloVe}}: Global {{Vectors}} for {{Word Representation}}},
    shorttitle = {{{GloVe}}},
    booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}}
                 in {{Natural Language Processing}} ({{EMNLP}})},
    author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
    year = {2014},
    month = oct,
    pages = {1532--1543},
    publisher = {{Association for Computational Linguistics}},
    address = {{Doha, Qatar}},
    doi = {10/gfshwg},
    file = {/home/cjber/drive/pdf/Pennington et
            al_2014_GloVe.pdf;/home/cjber/drive/pdf/Pennington et
            al_2014_GloVe2.pdf},
}

@article{perng2013,
    title = {Peripheral {{Response}}: Microblogging {{During}} the 22/7/2011 {{
             Norway Attacks}}},
    shorttitle = {Peripheral {{Response}}},
    author = {Perng, Sung-Yueh and B{\"u}scher, Monika and Wood, Lisa and
              Halvorsrud, Ragnhild and Stiso, Michael and Ramirez, Leonardo and {
              Al-Akkad}, Amro},
    year = {2013},
    month = jan,
    journal = {International Journal of Information Systems for Crisis Response
               and Management},
    volume = {5},
    number = {1},
    pages = {41--57},
    issn = {1937-9390, 1937-9420},
    doi = {10/gmgncq},
    abstract = {This paper presents a case study of a very recent man-made
                crisis in Norway on 22 July, 2011, during which a single person
                first detonated a bomb in downtown Oslo and then killed 69 young
                people on the island of Ut\o ya. It proposes a novel way of
                conceptualizing the public contribution to mobilization of
                resources using microblogging, particularly tweeting. By
                examining aspects of public and professional response to this
                crisis, the notion of peripheral response is developed in
                relation to emergent forms of agile and dialogic emergency
                response. Through examining the distributed efforts of responding
                to the crisis, the paper also revisits situation awareness and
                reflects upon the dynamic and constantly changing environment
                that social media and crises inhabit together.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Perng et al_2013_Peripheral Response.pdf},
}

@article{pota2020,
    title = {An {{Effective BERT}}-{{Based Pipeline}} for {{Twitter Sentiment
             Analysis}}: A {{Case Study}} in {{Italian}}},
    shorttitle = {An {{Effective BERT}}-{{Based Pipeline}} for {{Twitter
                  Sentiment Analysis}}},
    author = {Pota, Marco and Ventura, Mirko and Catelli, Rosario and Esposito,
              Massimo},
    year = {2020},
    month = dec,
    journal = {Sensors},
    volume = {21},
    number = {1},
    pages = {133},
    issn = {1424-8220},
    doi = {10/gmhdqv},
    abstract = {Over the last decade industrial and academic communities have
                increased their focus on sentiment analysis techniques,
                especially applied to tweets. State-of-the-art results have been
                recently achieved using language models trained from scratch on
                corpora made up exclusively of tweets, in order to better handle
                the Twitter jargon. This work aims to introduce a different
                approach for Twitter sentiment analysis based on two steps.
                Firstly, the tweet jargon, including emojis and emoticons, is
                transformed into plain text, exploiting procedures that are
                language-independent or easily applicable to different languages.
                Secondly, the resulting tweets are classified using the language
                model BERT, but pre-trained on plain text, instead of tweets, for
                two reasons: (1) pre-trained models on plain text are easily
                available in many languages, avoiding resource- and
                time-consuming model training directly on tweets from scratch; (2
                ) available plain text corpora are larger than tweetonly ones,
                therefore allowing better performance. A case study describing
                the application of the approach to Italian is presented, with a
                comparison with other Italian existing solutions. The results
                obtained show the effectiveness of the approach and indicate that
                , thanks to its general basis from a methodological perspective,
                it can also be promising for other languages.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Pota et al_2020_An Effective BERT-Based
            Pipeline for Twitter Sentiment Analysis.pdf},
}

@article{reilly2021,
    title = {Organizational {{Hashtags During Times}} of {{Crisis}}: Analyzing
             the {{Broadcasting}} and {{Gatekeeping Dynamics}} of \#{{
             PorteOuverte During}} the {{November}} 2015 {{Paris Terror Attacks}}
             },
    shorttitle = {Organizational {{Hashtags During Times}} of {{Crisis}}},
    author = {Reilly, Paul and Vicari, Stefania},
    year = {2021},
    month = jan,
    journal = {Social Media + Society},
    volume = {7},
    number = {1},
    pages = {205630512199578},
    issn = {2056-3051, 2056-3051},
    doi = {10/gmdkxv},
    abstract = {Twitter hashtags allow citizens to share vital information and
                make sense of acute crisis events such as terrorist attacks. They
                also enable those watching from afar to express their sympathy
                and solidarity with the victims. Perhaps the most well known of
                these has been \#PorteOuverte (translated into English as ``Open
                Door''), first used during the November 2015 terrorist attacks in
                Paris before re-emerging during subsequent atrocities in Brussels
                (March 2016) and Nice (July 2016). The hashtag was originally
                created by journalist Sylvain Lapoix in order to connect those in
                Paris looking for somewhere to stay with those able to offer them
                refuge, before reaching an international audience courtesy of its
                amplification by public figures and citizens based overseas. This
                article adds to this emergent literature by analyzing the
                networked gatekeeping dynamics of \#PorteOuverte during the Paris
                terror attacks. It does so by reviewing the literature on Twitter
                hashtags and acute crisis events, exploring how Twitter was used
                during the Paris terror attacks, and presenting the results of a
                Social Network Analysis (SNA) of 399,256 \#PorteOuverte tweets
                posted as the attacks unfolded on 13 November 2015. Results
                indicate that professional journalists were key broadcasters
                during four identified peaks within \#PorteOuverte, helping to
                promote the informational hashtag and connect those directly
                affected. However, citizens and bloggers played an increasingly
                important gatekeeping function in the aftermath of events such as
                the Bataclan siege in Paris.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Reilly_Vicari_2021_Organizational Hashtags
            During Times of Crisis.pdf},
}

@article{rossi2018,
    title = {Early Detection and Information Extraction for Weather-Induced
             Floods Using Social Media Streams},
    author = {Rossi, C. and Acerbo, F.S. and Ylinen, K. and Juga, I. and Nurmi,
              P. and Bosca, A. and Tarasconi, F. and Cristoforetti, M. and
              Alikadic, A.},
    year = {2018},
    month = sep,
    journal = {International Journal of Disaster Risk Reduction},
    volume = {30},
    pages = {145--157},
    issn = {22124209},
    doi = {10/gd3k8z},
    abstract = {Today we are using an unprecedented wealth of social media
                platforms to generate and share information regarding a wide
                class of events, which include extreme meteorological conditions
                and natural hazards such as floods. This paper proposes an
                automated set of services that start from the availability of
                weather forecasts, including both an event detection technique
                and a selective information retrieval from on-line social media.
                The envisioned services aim to provide qualitative feedback for
                meteorological models, detect the occurrence of an emergency
                event and extract informative content that can be used to
                complement the situational awareness. We implement such services
                and evaluate them during a recent weather induced flood. Our
                approach could be highly beneficial for monitoring agencies and
                meteorological offices, who act in the early warning phase, and
                also for authorities and first responders, who manage the
                emergency response phase.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Rossi et al_2018_Early detection and
            information extraction for weather-induced floods using.pdf},
}

@article{sakaki2010,
    title = {Earthquake Shakes {{Twitter}} Users: Real-Time Event Detection by
             Social Sensors},
    author = {Sakaki, Takeshi and Okazaki, Makoto and Matsuo, Yutaka},
    year = {2010},
    pages = {10},
    doi = {10/b6zm4b},
    abstract = {Twitter, a popular microblogging service, has received much
                attention recently. An important characteristic of Twitter is its
                real-time nature. For example, when an earthquake occurs, people
                make many Twitter posts (tweets) related to the earthquake, which
                enables detection of earthquake occurrence promptly, simply by
                observing the tweets. As described in this paper, we investigate
                the real-time interaction of events such as earthquakes in
                Twitter and propose an algorithm to monitor tweets and to detect
                a target event. To detect a target event, we devise a classifier
                of tweets based on features such as the keywords in a tweet, the
                number of words, and their context. Subsequently, we produce a
                probabilistic spatiotemporal model for the target event that can
                find the center and the trajectory of the event location. We
                consider each Twitter user as a sensor and apply Kalman filtering
                and particle filtering, which are widely used for location
                estimation in ubiquitous/pervasive computing. The particle filter
                works better than other comparable methods for estimating the
                centers of earthquakes and the trajectories of typhoons. As an
                application, we construct an earthquake reporting system in
                Japan. Because of the numerous earthquakes and the large number
                of Twitter users throughout the country, we can detect an
                earthquake with high probability (96\% of earthquakes of Japan
                Meteorological Agency (JMA) seismic intensity scale 3 or more are
                detected) merely by monitoring tweets. Our system detects
                earthquakes promptly and sends e-mails to registered users.
                Notification is delivered much faster than the announcements that
                are broadcast by the JMA.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Sakaki et al_2010_Earthquake shakes Twitter
            users.pdf},
}

@inproceedings{saravanou2015,
    title = {Twitter {{Floods}} When It {{Rains}}: A {{Case Study}} of the {{UK
             Floods}} in Early 2014},
    shorttitle = {Twitter {{Floods}} When It {{Rains}}},
    booktitle = {Proceedings of the 24th {{International Conference}} on {{World
                 Wide Web}}},
    author = {Saravanou, Antonia and Valkanas, George and Gunopulos, Dimitrios
              and Andrienko, Gennady},
    year = {2015},
    month = may,
    pages = {1233--1238},
    publisher = {{ACM}},
    address = {{Florence Italy}},
    doi = {10/ghcxcv},
    abstract = {Twitter is one of the most prominent social media platforms
                nowadays. A primary reason that has brought the medium at the
                spotlight of academic attention is its real-time nature, with
                people constantly uploading information regarding their
                surroundings. This trait, coupled with the service's data access
                policy for researchers and developers, has allowed the community
                to explore Twitter's potential as a news reporting tool. Finding
                out promptly about newsworthy events can prove extremely useful
                in crisis management situations. In this paper, we explore the
                use of Twitter as a mechanism used in disaster relief, and
                consequently in public safety. In particular, we perform a case
                study on the floods that occurred in the United Kingdom during
                January 2014, and how these were reflected on Twitter, according
                to tweets (i.e., posts) submitted by the users. We present a
                systematic algorithmic analysis of tweets collected with respect
                to our use case scenario, supplemented by visual analytic tools.
                Our objective is to identify meaningful and effective ways to
                take advantage of the wealth of Twitter data in crisis management
                , and we report on the findings of our analysis.},
    isbn = {978-1-4503-3473-0},
    langid = {english},
    file = {/home/cjber/drive/pdf/Saravanou et al_2015_Twitter Floods when it
            Rains.pdf},
}

@article{schneider2010,
    title = {Read All about It: The Role of the Media in Improving Construction
             Safety and Health},
    shorttitle = {Read All about It},
    author = {Schneider, Scott and Check, Pietra},
    year = {2010},
    month = jun,
    journal = {Journal of Safety Research},
    series = {Special {{Topic}}: Construction {{Safety}}},
    volume = {41},
    number = {3},
    pages = {283--287},
    issn = {0022-4375},
    doi = {10/bh8jrq},
    langid = {english},
    file = {/home/cjber/drive/pdf/Schneider_Check_2010_Read all about
            it.pdf;/home/cjber/drive/zotero/storage/CDF6334Z/S0022437510000460.html
            },
}

@article{simon2015,
    title = {Socializing in Emergencies\textemdash{{A}} Review of the Use of
             Social Media in Emergency Situations},
    author = {Simon, Tomer and Goldberg, Avishay and Adini, Bruria},
    year = {2015},
    month = oct,
    journal = {International Journal of Information Management},
    volume = {35},
    number = {5},
    pages = {609--619},
    issn = {0268-4012},
    doi = {10/gmkgpb},
    abstract = {Social media tools are integrated in most parts of our daily
                lives, as citizens, netizens, researchers or emergency
                responders. Lessons learnt from disasters and emergencies that
                occurred globally in the last few years have shown that social
                media tools may serve as an integral and significant component of
                crisis response. Communication is one of the fundamental tools of
                emergency management. It becomes crucial when there are dozens of
                agencies and organizations responding to a disaster. Regardless
                of the type of emergency, whether a terrorist attack, a hurricane
                or an earthquake, communication lines may be overloaded and
                cellular networks overwhelmed as too many people attempt to use
                them to access information. Social scientists have presented that
                post-disaster active public participation was largely altruistic,
                including activities such as search and rescue, first aid
                treatment, victim evacuation, and on-line help. Social media
                provides opportunities for engaging citizens in the emergency
                management by both disseminating information to the public and
                accessing information from them. During emergency events,
                individuals are exposed to large quantities of information
                without being aware of their validity or risk of misinformation,
                but users are usually swift to correct them, thus making the
                social media ``self-regulating''.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Simon et al_2015_Socializing in emergencies—A
            review of the use of social media in
            emergency.pdf;/home/cjber/drive/zotero/storage/EPA2RTHI/S0268401215000638.html
            },
}

@article{songchon2021,
    title = {Quality Assessment of Crowdsourced Social Media Data for Urban
             Flood Management},
    author = {Songchon, Chanin and Wright, Grant and Beevers, Lindsay},
    year = {2021},
    month = nov,
    journal = {Computers, Environment and Urban Systems},
    volume = {90},
    pages = {101690},
    issn = {01989715},
    doi = {10/gmnpsd},
    abstract = {Urban flooding can cause widespread devastation in terms of loss
                of life and damage to property. As such, monitoring urban flood
                evolution is crucial in identifying the most affected areas,
                where emergency response resources should be directed. Flood
                monitoring through airborne or satellite remote sensing is often
                limited due to weather conditions and urban topography. In
                contrast, crowdsourced data is not affected by weather or topog\-
                raphy, and they hence offer great potential for urban flood
                monitoring through real-time information shared by individuals.
                Despite the benefits, there is no guarantee of quality associated
                with crowdsourced data, which hampers its usability. In this
                paper, we present and evaluate two different approaches (binary
                logistic regression and fuzzy logic) to assess the quality of
                crowdsourced social media data retrieved from the public Twitter
                archive. Input variables were constructed based on Twitter
                metadata and spatiotemporal analysis. Both models were trained
                and tested using actual flood-related information Tweeted during
                three consecutive years of flooding in Phetchaburi City, Thailand
                (2016 to 2018), and produced good results. The fuzzy logic
                approach is shown to perform better, however its implementation
                involves significantly more subjectivity. The ability to assess
                data quality enables the uncertainty associated with crowdsourced
                social media data to be estimated, which allows this type of data
                to supplement conventional observations, and hence improve flood
                management activities.},
    langid = {english},
    file = {/home/cjber/drive/zotero/storage/K4LVNINH/Songchon et al. - 2021 -
            Quality assessment of crowdsourced social media da.pdf},
}

@inproceedings{spielhofer2016,
    title = {Data Mining {{Twitter}} during the {{UK}} Floods: Investigating the
             Potential Use of Social Media in Emergency Management},
    shorttitle = {Data Mining {{Twitter}} during the {{UK}} Floods},
    booktitle = {2016 3rd {{International Conference}} on {{Information}} and {{
                 Communication Technologies}} for {{Disaster Management}} ({{ICT}
                 }-{{DM}})},
    author = {Spielhofer, Thomas and Greenlaw, Reynold and Markham, Deborah and
              Hahne, Anna},
    year = {2016},
    month = dec,
    pages = {1--6},
    publisher = {{IEEE}},
    address = {{Vienna, Austria}},
    doi = {10/ggwjsv},
    isbn = {978-1-5090-5234-9},
    keywords = {data,data mining,Decision support systems,emergency services,
                flooding,Twitter},
    file = {/home/cjber/drive/pdf/Spielhofer et al_2016_Data mining Twitter
            during the UK floods.pdf;/home/cjber/drive/pdf/Spielhofer et
            al_2016_Data mining Twitter during the UK
            floods2.pdf;/home/cjber/drive/pdf/Spielhofer et al_2016_Data mining
            Twitter during the UK
            floods3.pdf;/home/cjber/drive/zotero/storage/R8BEDMVV/7857213.html},
}

@misc{statista2021,
    title = {Twitter: Most Users by Country},
    shorttitle = {Twitter},
    author = {{Statista}},
    year = {2021},
    journal = {Statista},
    abstract = {The United States, Japan, and India were the three countries
                with the most Twitter users as of January 2021.},
    howpublished = {
                    https://www.statista.com/statistics/242606/number-of-active-twitter-users-in-selected-countries/
                    },
    langid = {english},
    file = {
            /home/cjber/drive/zotero/storage/8R9JAJYG/number-of-active-twitter-users-in-selected-countries.html
            },
}

@article{sundararajan2017,
    title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
    author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
    year = {2017},
    month = jun,
    journal = {arXiv:1703.01365 [cs]},
    eprint = {1703.01365},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {We study the problem of attributing the prediction of a deep
                network to its input features, a problem previously studied by
                several other works. We identify two fundamental axioms
                \textemdash Sensitivity and Implementation Invariance that
                attribution methods ought to satisfy. We show that they are not
                satisfied by most known attribution methods, which we consider to
                be a fundamental weakness of those methods. We use the axioms to
                guide the design of a new attribution method called Integrated
                Gradients. Our method requires no modification to the original
                network and is extremely simple to implement; it just needs a few
                calls to the standard gradient operator. We apply this method to
                a couple of image models, a couple of text models and a chemistry
                model, demonstrating its ability to debug networks, to extract
                rules from a network, and to enable users to engage with models
                better.},
    archiveprefix = {arXiv},
    langid = {english},
    keywords = {⛔ No DOI found,Computer Science - Machine Learning},
    file = {/home/cjber/drive/pdf/Sundararajan et al_2017_Axiomatic Attribution
            for Deep Networks.pdf},
}

@inproceedings{tjongkimsang2003,
    title = {Introduction to the {{CoNLL}}-2003 {{Shared Task}}: Language-{{
             Independent Named Entity Recognition}}},
    shorttitle = {Introduction to the {{CoNLL}}-2003 {{Shared Task}}},
    booktitle = {Proceedings of the {{Seventh Conference}} on {{Natural Language
                 Learning}} at {{HLT}}-{{NAACL}} 2003},
    author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
    year = {2003},
    pages = {142--147},
    doi = {10/d8qpkd},
    file = {/home/cjber/drive/pdf/Tjong Kim Sang_De Meulder_2003_Introduction to
            the CoNLL-2003 Shared Task.pdf},
}

@inproceedings{twaroch2008,
    title = {Acquisition of a Vernacular Gazetteer from Web Sources},
    booktitle = {Proceedings of the First International Workshop on {{Location}}
                 and the Web - {{LOCWEB}} '08},
    author = {Twaroch, Florian A. and Jones, Christopher B. and Abdelmoty, Alia
              I.},
    year = {2008},
    pages = {61--64},
    publisher = {{ACM Press}},
    address = {{Beijing, China}},
    doi = {10.1145/1367798.1367808},
    abstract = {Vernacular place names are names that are commonly in use to
                refer to geographical places. For purposes of effective
                information retrieval, the spatial extent associated with these
                names should be able to reflect people's perception of the place,
                even though this may differ sometimes from the administrative
                definition of the same place name. Due to their informal nature,
                vernacular place names are hard to capture, but methods to
                acquire and define vernacular place names are of great benefit to
                search engines and all kind of information services that deal
                with geographic data. This paper discusses the acquisition of
                vernacular use of place names from web sources and their
                representation as surface models derived by kernel density
                estimators.},
    isbn = {978-1-60558-160-6},
    langid = {english},
    keywords = {Vernacular},
    file = {/home/cjber/drive/pdf/Twaroch et al_2008_Acquisition of a vernacular
            gazetteer from web sources.pdf},
}

@article{umihara2013,
    title = {Emergent {{Use}} of {{Twitter}} in the 2011 {{Tohoku Earthquake}}},
    author = {Umihara, Junko and Nishikitani, Mariko},
    year = {2013},
    month = oct,
    journal = {Prehospital and Disaster Medicine},
    volume = {28},
    number = {5},
    pages = {434--440},
    issn = {1049-023X, 1945-1938},
    doi = {10/f5jv86},
    abstract = {Introduction: Social networks play an important role in disaster
                situations as they have become a new form of social convergence
                that provides collective information. The effect of social media
                on people who experienced disaster should be assessed.
                Hypothesis: In this study, Twitter communication during the Great
                East Japan Earthquake of March 11, 2011 was assessed. The
                hypothesis of this study was that usage of Twitter had
                psychological effects on victims of the disaster. Methods: A
                cross-sectional questionnaire survey was carried out in
                cooperation with a major Japanese newspaper three months after
                the disaster, and 1,144 volunteer participants responded. They
                were asked about their health, area of residence, property damage
                they had experienced, information sources they used at the time
                of the disaster, and their usage of Twitter. Further, the Twitter
                users were divided into two groups\textemdash with and without
                disaster experience. Their psychological effects relating to
                feelings of relief, stress or anxiety that they experienced in
                using Twitter were compared between two groups, and Twitter's
                psychological risk of disaster experience was estimated as an
                odds ratio. Results: Twitter users in this study tended to reside
                in disaster-affected areas and thought Twitter was a credible
                information source during the time of the disaster. The
                psychological effect of Twitter differed based on participants'
                disaster experience and gender. Females with disaster experience
                reported more feelings of relief and stress as a result of using
                Twitter compared to females who did not experience the disaster.
                On the other hand, males with disaster experience only reported
                more stress experiences as a result of using Twitter compared to
                those without disaster experience. Conclusion: Twitter users with
                disaster experience had a higher usage of Twitter than those
                without disaster experience. Social media might have had a
                material psychological influence on people who experienced
                disaster, and the effect differed by gender. Regardless of gender
                , negative feelings were transmitted easily among people who
                experienced the disaster. It was anticipated that the application
                of Twitter in a disaster situation will be expanded further by
                taking these findings into consideration.},
    langid = {english},
    file = {/home/cjber/drive/zotero/storage/MEBZG4UP/Umihara and Nishikitani -
            2013 - Emergent Use of Twitter in the 2011 Tohoku Earthqu.pdf},
}

@article{vaswani2017,
    title = {Attention {{Is All You Need}}},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
              Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and
              Polosukhin, Illia},
    year = {2017},
    month = dec,
    journal = {arXiv:1706.03762 [cs]},
    eprint = {1706.03762},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {The dominant sequence transduction models are based on complex
                recurrent or convolutional neural networks in an encoder-decoder
                configuration. The best performing models also connect the
                encoder and decoder through an attention mechanism. We propose a
                new simple network architecture, the Transformer, based solely on
                attention mechanisms, dispensing with recurrence and convolutions
                entirely. Experiments on two machine translation tasks show these
                models to be superior in quality while being more parallelizable
                and requiring significantly less time to train. Our model
                achieves 28.4 BLEU on the WMT 2014 English-to-German translation
                task, improving over the existing best results, including
                ensembles by over 2 BLEU. On the WMT 2014 English-to-French
                translation task, our model establishes a new single-model
                state-of-the-art BLEU score of 41.8 after training for 3.5 days
                on eight GPUs, a small fraction of the training costs of the best
                models from the literature. We show that the Transformer
                generalizes well to other tasks by applying it successfully to
                English constituency parsing both with large and limited training
                data.},
    archiveprefix = {arXiv},
    keywords = {⛔ No DOI found,Computer Science - Computation and Language,
                Computer Science - Machine Learning},
    file = {/home/cjber/drive/pdf/Vaswani et al_2017_Attention Is All You
            Need.pdf;/home/cjber/drive/zotero/storage/37NNQGYQ/1706.html},
}

@article{willems2012,
    title = {Rainfall in the Urban Context: Forecasting, Risk and Climate Change
             },
    shorttitle = {Rainfall in the Urban Context},
    author = {Willems, Patrick and Molnar, Peter and Einfalt, Thomas and {
              Arnbjerg-Nielsen}, Karsten and Onof, Christian and Nguyen,
              Van-Thanh-Van and Burlando, Paolo},
    year = {2012},
    month = jan,
    journal = {Atmospheric Research},
    volume = {103},
    pages = {1--3},
    issn = {01698095},
    doi = {10/bv2qqn},
    abstract = {Cities became since last decades more vulnerable to flooding
                because urban drainage infrastructure has been built at large
                scale, but also due to climate change. The number of quantitative
                assessment studies of the impact of climate change on urban
                drainage remains, however, rather limited. This is partly because
                of the particular difficulties when dealing with this type of
                impact. First problem is that the results of climate models need
                be downscaled to very small space and time resolutions (e.g.
                catchments of few kilometers, time scales as low as 5 minutes).
                The impact results consequently are highly uncertain, a problem
                that becomes more challenging since the properties of extremes do
                not automatically reflect those of average precipitation.},
    langid = {english},
    file = {/home/cjber/drive/pdf/Willems et al_2012_Rainfall in the urban
            context.pdf},
}

@article{williams2013,
    title = {What Do People Study When They Study {{Twitter}}? Classifying {{
             Twitter}} Related Academic Papers},
    shorttitle = {What Do People Study When They Study {{Twitter}}?},
    author = {Williams, S.A. and Terras, M.M. and Warwick, C.},
    year = {2013},
    journal = {Journal of Documentation},
    volume = {69},
    number = {3},
    pages = {384--410},
    issn = {0022-0418},
    doi = {10/f42xw4},
    abstract = {Purpose: Since its introduction in 2006, messages posted to the
                microblogging system Twitter have provided a rich dataset for
                researchers, leading to the publication of over a thousand
                academic papers. This paper aims to identify this published work
                and to classify it in order to understand Twitter based research.
                Design/methodology/approach: Firstly the papers on Twitter were
                identified. Secondly, following a review of the literature, a
                classification of the dimensions of microblogging research was
                established. Thirdly, papers were qualitatively classified using
                open coded content analysis, based on the paper's title and
                abstract, in order to analyze method, subject, and approach.
                Findings: The majority of published work relating to Twitter
                concentrates on aspects of the messages sent and details of the
                users. A variety of methodological approaches is used across a
                range of identified domains. Research limitations/implications:
                This work reviewed the abstracts of all papers available via
                database search on the term "Twitter" and this has two major
                implications: the full papers are not considered and so works may
                be misclassified if their abstract is not clear; publications not
                indexed by the databases, such as book chapters, are not
                included. The study is focussed on microblogging, the
                applicability of the approach to other media is not considered.
                Originality/value: To date there has not been an overarching
                study to look at the methods and purpose of those using Twitter
                as a research subject. The paper's major contribution is to scope
                out papers published on Twitter until the close of 2011. The
                classification derived here will provide a framework within which
                researchers studying Twitter related topics will be able to
                position and ground their work. \textcopyright{} Emerald Group
                Publishing Limited.},
    langid = {english},
    keywords = {Abstracts,Blogs,Classification,Microblogging,Papers,Social
                network systems,Social networking sites,Twitter},
    file = {/home/cjber/drive/pdf/Williams et al_2013_What do people study when
            they study
            Twitter.pdf;/home/cjber/drive/zotero/storage/EN4Y9KAE/display.html},
}

@article{wolf2020,
    title = {{{HuggingFace}}'s {{Transformers}}: State-of-the-Art {{Natural
             Language Processing}}},
    shorttitle = {{{HuggingFace}}'s {{Transformers}}},
    author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond,
              Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric
              and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison
              , Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and
              Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le
              and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush
              , Alexander M.},
    year = {2020},
    month = jul,
    journal = {arXiv:1910.03771 [cs]},
    eprint = {1910.03771},
    eprinttype = {arxiv},
    primaryclass = {cs},
    abstract = {Recent progress in natural language processing has been driven
                by advances in both model architecture and model pretraining.
                Transformer architectures have facilitated building
                higher-capacity models and pretraining has made it possible to
                effectively utilize this capacity for a wide variety of tasks.
                Transformers is an open-source library with the goal of opening
                up these advances to the wider machine learning community. The
                library consists of carefully engineered stateof-the art
                Transformer architectures under a unified API. Backing this
                library is a curated collection of pretrained models made by and
                available for the community. Transformers is designed to be
                extensible by researchers, simple for practitioners, and fast and
                robust in industrial deployments. The library is available at
                https://github.com/ huggingface/transformers.},
    archiveprefix = {arXiv},
    langid = {english},
    keywords = {Computer Science - Computation and Language},
    file = {/home/cjber/drive/pdf/Wolf et al_2020_HuggingFace's Transformers.pdf
            },
}

@article{yin,
    title = {Using {{Social Media}} to {{Enhance Emergency Situation Awareness}}
             : Extended {{Abstract}}},
    author = {Yin, Jie and Karimi, Sarvnaz and Lampert, Andrew and Cameron, Mark
              and Robinson, Bella and Power, Robert},
    pages = {5},
    abstract = {Social media platforms, such as Twitter, offer a rich source of
                real-time information about real-world events, particularly
                during mass emergencies. Sifting valuable information from social
                media provides useful insight into time-critical situations for
                emergency officers to understand the impact of hazards and act on
                emergency responses in a timely manner. This work focuses on
                analyzing Twitter messages generated during natural disasters,
                and shows how natural language processing and data mining
                techniques can be utilized to extract situation awareness
                information from Twitter. We present key relevant approaches that
                we have investigated including burst detection, tweet filtering
                and classification, online clustering, and geotagging.},
    langid = {english},
    keywords = {⛔ No DOI found},
    file = {/home/cjber/drive/zotero/storage/TY48XY9A/Yin et al. - Using Social
            Media to Enhance Emergency Situation .pdf},
}


