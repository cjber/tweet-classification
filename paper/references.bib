
@article{2001,
  title = {How {{Much}} Does {{Place Matter}}?},
  year = {2001},
  month = aug,
  journal = {Environment and Planning A: Economy and Space},
  volume = {33},
  number = {8},
  pages = {1335--1369},
  issn = {0308-518X, 1472-3409},
  doi = {10/ccv44z},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/2001_.pdf}
}

@article{aarts2006,
  title = {Driving Speed and the Risk of Road Crashes: {{A}} Review},
  shorttitle = {Driving Speed and the Risk of Road Crashes},
  author = {Aarts, Letty and {van Schagen}, Ingrid},
  year = {2006},
  month = mar,
  journal = {Accident Analysis \& Prevention},
  volume = {38},
  number = {2},
  pages = {215--224},
  issn = {00014575},
  doi = {10/djq87b},
  abstract = {Driving speed is an important factor in road safety. Speed not only affects the severity of a crash, but is also related to the risk of being involved in a crash. This paper discusses the most important empirical studies into speed and crash rate with an emphasis on the more recent studies. The majority of these studies looked at absolute speed, either at individual vehicle level or at road section level. Respectively, they found evidence for an exponential function and a power function between speed and crash rate. Both types of studies found evidence that crash rate increases faster with an increase in speed on minor roads than on major roads. At a more detailed level, lane width, junction density, and traffic flow were found to interact with the speed\textendash crash rate relation. Other studies looked at speed dispersion and found evidence that this is also an important factor in determining crash rate. Larger differences in speed between vehicles are related to a higher crash rate. Without exception, a vehicle that moved (much) faster than other traffic around it, had a higher crash rate. With regard to the rate of a (much) slower moving vehicle, the evidence is inconclusive.},
  langid = {english},
  annotation = {ZSCC: 0000994},
  file = {/home/cjber/drive/pdf/ENVS492/Aarts_van Schagen_2006_.pdf}
}

@book{abbott2015,
  title = {Understanding {{Analysis}}},
  author = {Abbott, Stephen},
  year = {2015},
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-2712-8},
  isbn = {978-1-4939-2711-1 978-1-4939-2712-8},
  langid = {english},
  file = {/home/cjber/drive/pdf/Abbott_2015_Understanding Analysis.pdf}
}

@article{acheson2020,
  title = {Machine Learning for Cross-Gazetteer Matching of Natural Features},
  author = {Acheson, Elise and Volpi, Michele and Purves, Ross S.},
  year = {2020},
  month = apr,
  journal = {International Journal of Geographical Information Science},
  volume = {34},
  number = {4},
  pages = {708--734},
  issn = {1365-8816, 1362-3087},
  doi = {10/gfz5pn},
  abstract = {Defining and identifying duplicate records in a dataset is a challenging task which grows more complex when the modeled entities themselves are hard to delineate. In the geospatial domain, it may not be clear where a mountain, stream, or valley ends and begins, a problem carried over when such entities are catalogued in gazetteers. In this paper, we take two gazetteers, GeoNames and SwissNames3D, and perform matching \textendash{} identifying records in each that are about the same entity \textendash{} across a sample of natural feature records. We first perform rule-based matching, establishing competitive results, then apply machine learning using Random Forests, a method well-suited to the matching task. We report on the performance of a wider array of matching features than has been previously studied, including domain-specific ones such as feature type, land cover class, and elevation. Our results show an increase in performance using machine learning over rules, with a notable performance gain from considering feature types, but negligible gains from other specialized matching features. We argue that future work in this area should strive to be more reproducible and report results on a realistic testing pipeline including candidate selection, feature extraction, and classification.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Acheson et al_2020_Machine learning for cross-gazetteer matching of natural features.pdf}
}

@incollection{adams2013,
  title = {Inferring Thematic Places from Spatially Referenced Natural Language Descriptions},
  booktitle = {Crowdsourcing Geographic Knowledge},
  author = {Adams, Benjamin and McKenzie, Grant},
  year = {2013},
  pages = {201--221},
  publisher = {{Springer}},
  keywords = {\#nosource}
}

@inproceedings{adams2015,
  title = {Frankenplace: {{Interactive Thematic Mapping}} for {{Ad Hoc Exploratory Search}}},
  shorttitle = {Frankenplace},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{World Wide Web}} - {{WWW}} '15},
  author = {Adams, Benjamin and McKenzie, Grant and Gahegan, Mark},
  year = {2015},
  pages = {12--22},
  publisher = {{ACM Press}},
  address = {{Florence, Italy}},
  doi = {10/ggwjs6},
  abstract = {Ad hoc keyword search engines built using modern information retrieval methods do a good job of handling fine-grained queries. However, they perform poorly at facilitating spatial and spatially-embedded thematic exploration of the results, despite the fact that many queries, e.g. civil war, refer to different documents and topics in different places. This is not for lack of data: geographic information, such as place names, events, and coordinates are common in unstructured document collections on the web. The associations between geographic and thematic contents in these documents can provide a rich groundwork to organize information for exploratory research. In this paper we describe the architecture of an interactive thematic map search engine, Frankenplace, designed to facilitate document exploration at the intersection of theme and place. The map interface enables a user to zoom the geographic context of their query in and out, and quickly explore through thousands of search results in a meaningful way. And by combining topic models with geographically contextualized search results, users can discover related topics based on geographic context. Frankenplace utilizes a novel indexing method called geoboost for boosting terms associated with cells on a discrete global grid. The resulting index factors in the geographic scale of the place or feature mentioned in related text, the relative textual scope of the place reference, and the overall importance of the containing document in the document network. The system is currently indexed with over 5 million documents from the web, including the English Wikipedia and online travel blog entries. We demonstrate that Frankenplace can support four distinct types of exploratory search tasks while being adaptive to scale and location of interest.},
  isbn = {978-1-4503-3469-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Adams et al_2015_Frankenplace.pdf}
}

@inproceedings{adams2018,
  title = {From Spatial Representation to Processes, Relational Networks, and Thematic Roles in Geographic Information Retrieval},
  booktitle = {Proceedings of the 12th {{Workshop}} on {{Geographic Information Retrieval}} - {{GIR}}'18},
  author = {Adams, Benjamin},
  year = {2018},
  pages = {1--2},
  publisher = {{ACM Press}},
  address = {{Seattle, WA, USA}},
  doi = {10/ggwjs7},
  abstract = {Geographic information retrieval (GIR) has largely been synonymous with spatial information retrieval. However, geographic information in text is not always explicitly, nor even implicitly, spatial, and when people are seeking geographic information, it is not exlusively for the purpose of spatial analysis or understanding. Here we argue that GIR research is artificially limited by a focus on static spatial representation and we could expand GIR's domain of interest to include organization of information that is non-spatial. Specifically, we highlight three areas worth further study: 1) information about geographic processes and change in text, 2) geographic information where relational context is more important than spatial context, and 3) geographic entities referenced in text that take on thematic (non-spatial) roles.},
  isbn = {978-1-4503-6034-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Adams_2018_From spatial representation to processes, relational networks, and thematic.pdf}
}

@inproceedings{adelfio2013,
  title = {Structured Toponym Resolution Using Combined Hierarchical Place Categories},
  booktitle = {Proceedings of the 7th {{Workshop}} on {{Geographic Information Retrieval}} - {{GIR}} '13},
  author = {Adelfio, Marco D. and Samet, Hanan},
  year = {2013},
  pages = {49--56},
  publisher = {{ACM Press}},
  address = {{Orlando, Florida}},
  doi = {10/ggwjt2},
  abstract = {Determining geographic interpretations for place names, or toponyms, involves resolving multiple types of ambiguity. Place names commonly occur within lists and data tables, whose authors frequently omit qualifications (such as city or state containers) for place names because they expect the meaning of individual place names to be obvious from context. We present a novel technique for place name disambiguation (also known as toponym resolution) that uses Bayesian inference to assign categories to lists or tables containing place names, and then interprets individual toponyms based on the most likely category assignments. The categories are defined as nodes in hierarchies along three orthogonal dimensions: place types (e.g., cities, capitals, rivers, etc.), geographic containers, and prominence (e.g., based on population).},
  isbn = {978-1-4503-2241-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Adelfio_Samet_2013_Structured toponym resolution using combined hierarchical place categories.pdf}
}

@article{aflaki2018,
  title = {Challenges in {{Creating}} an {{Annotated Set}} of {{Geospatial Natural Language Descriptions}}},
  author = {Aflaki, Niloofar and Russell, Shaun},
  year = {2018},
  pages = {6},
  abstract = {In order to extract and map location information from natural language descriptions, a first step is to identify different language elements within the descriptions. In this paper, we describe a method and discuss the challenges faced in creating an annotated set of geospatial natural language descriptions using manual tagging, with the purpose of supporting validation and machine learning approaches to annotation and text interpretation.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Aﬂaki_Russell_2018_Challenges in Creating an Annotated Set of Geospatial Natural Language.pdf;/home/cjber/drive/pdf/Aﬂaki_Russell_2018_Challenges in Creating an Annotated Set of Geospatial Natural Language2.pdf}
}

@article{agarwal2006,
  title = {Social Exclusion and {{English}} Seaside Resorts},
  author = {Agarwal, Sheela and Brunt, Paul},
  year = {2006},
  month = aug,
  journal = {Tourism Management},
  volume = {27},
  number = {4},
  pages = {654--670},
  issn = {02615177},
  doi = {10/cdqp3j},
  abstract = {This paper investigates characteristics associated with social exclusion in English seaside resorts. Drawing on the Index of Multiple Deprivation (2000), a multi-relational database, an examination is undertaken of the occurrence, nature and extent of social exclusion within and between seaside resorts in England. The study findings reveal that seaside districts, wards and resorts are experiencing higher levels of multiple deprivation than might be expected. In addition, it shows that while there is remarkable similarity in the nature and extent of multiple deprivation being experienced, variations in the socio-economic characteristics of those resorts most affected suggest that their predicament is more complex than first appears. Three important implications for resort restructuring are highlighted. First, it must avoid being too tourism centric and instead should adopt a more holistic approach. Second a `one size fit' approach is inappropriate and third, there is an urgent need for further in-depth research.},
  langid = {english},
  annotation = {ZSCC: 0000105},
  file = {/home/cjber/drive/pdf/ENVS416/agarwal2006.pdf}
}

@book{aggarwal2015,
  ids = {aggarwal2015a},
  title = {Data {{Mining}}},
  author = {Aggarwal, Charu C.},
  year = {2015},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-14142-8},
  isbn = {978-3-319-14141-1 978-3-319-14142-8},
  langid = {english},
  file = {/home/cjber/drive/pdf/Aggarwal_2015_Data Mining.pdf;/home/cjber/drive/pdf/Aggarwal_2015_Data Mining2.pdf;/home/cjber/drive/pdf/Aggarwal_2015_Data Mining3.pdf;/home/cjber/drive/pdf/Aggarwal_2015_Data Mining4.pdf}
}

@book{aggarwal2018,
  ids = {aggarwal2018a},
  title = {Neural {{Networks}} and {{Deep Learning}}: {{A Textbook}}},
  shorttitle = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Aggarwal, Charu C.},
  year = {2018},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-94463-0},
  isbn = {978-3-319-94462-3 978-3-319-94463-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Aggarwal_2018_Neural Networks and Deep Learning.pdf;/home/cjber/drive/zotero/storage/DVMF363U/Aggarwal_2018_Neural Networks and Deep Learning2.pdf}
}

@article{ah2006,
  title = {Using {{Econometrics}}: {{A Practical}} Guide},
  author = {Ah, Studenmund},
  year = {2006},
  publisher = {{Pearson Education, inc., Prentice hall}},
  keywords = {⛔ No DOI found}
}

@article{akel,
  title = {Dense {{DTM Generalization Aided}} by {{Roads Extracted}} from {{LiDAR Data}}},
  author = {Akel, Nizar Abo and Kremeike, Katrin and Filin, Sagi and Sester, Monika and Doytsher, Yerach},
  pages = {6},
  abstract = {The paper concerns the generalization of DTM extracted from LiDAR data. The essence of generalization is reducing details while enhancing important features at the same time; so for the purpose of terrain surface visualization special attention has to be given to the enhancement and generalization of topographic objects like dams, roads etc. The focus of this work is laid on the extraction of road objects and their contribution into the enhancement of the generalized terrain model. An algorithm for the extraction of roads is developed and is followed by a generalization algorithm that weights together road networks and filtered LiDAR point clouds. Following the presentation of the algorithm results for this approach are shown and evaluated.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000040},
  file = {/home/cjber/drive/pdf/ENVS492/LiDAR/Akel et al_.pdf}
}

@article{akel2003,
  title = {Automatic {{DTM Extraction}} from {{Dense Raw LIDAR Data}} in {{Urban Areas}}},
  author = {Akel, Nizar ABO and Zilberstein, Ofer and Doytsher, Yerach},
  year = {2003},
  pages = {11},
  abstract = {Although LIDAR is a powerful tool for collecting data of the earth surface, the collected raw data amounts to no more than clouds of x-y-z points inexplicitly describing the surface. It lacks differentiation between points measured on the terrain itself and points measured on man-made or natural objects (buildings, roads, trees etc.). In this paper, an automatic method for DTM extraction in urban areas is presented. The method is based on an initial approximation of the DTM determined by using the road network, followed by an iterative processing technique that enables computing the final (``true'') DTM. For detection of roads, segmentations based on normal direction, edge detection and height difference are used. Based on our preliminary tests, some of which are presented in this paper, the results point to a promising solution.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000042},
  file = {/home/cjber/drive/pdf/ENVS492/Akel et al_2003_.pdf}
}

@inproceedings{akiba2019,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  pages = {2623--2631},
  publisher = {{ACM}},
  address = {{Anchorage AK USA}},
  doi = {10/gf7mzz},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to lightweight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/ optuna/).},
  isbn = {978-1-4503-6201-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Akiba et al_2019_Optuna.pdf}
}

@inproceedings{al-olimat2019,
  title = {Towards {{Geocoding Spatial Expressions}} ({{Vision Paper}})},
  booktitle = {Proceedings of the 27th {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}  - {{SIGSPATIAL}} '19},
  author = {{Al-Olimat}, Hussein S. and Shalin, Valerie L. and Thirunarayan, Krishnaprasad and Sain, Joy Prakash},
  year = {2019},
  pages = {75--78},
  publisher = {{ACM Press}},
  address = {{Chicago, IL, USA}},
  doi = {10/ggwjt5},
  abstract = {Imprecise composite location references formed using ad hoc spatial expressions in English text makes the geocoding task challenging for both inference and evaluation. Typically such spatial expressions fill in unestablished areas with new toponyms for finer spatial referents. For example, the spatial extent of the ad hoc spatial expression "north of" or "50 minutes away from" in relation to the toponym "Dayton, OH" refers to an ambiguous, imprecise area, requiring translation from this qualitative representation to a quantitative one with precise semantics using systems such as WGS84. Here we highlight the challenges of geocoding such referents and propose a general formal representation that employs background knowledge, semantic approximations and rules, and fuzzy linguistic variables. We also discuss an appropriate evaluation technique for the task that is based on human contextualized and subjective judgment.},
  isbn = {978-1-4503-6909-1},
  langid = {english},
  keywords = {Key Paper},
  file = {/home/cjber/drive/pdf/Al-Olimat et al_2019_Towards Geocoding Spatial Expressions (Vision Paper).pdf}
}

@article{alex2019,
  title = {Geoparsing Historical and Contemporary Literary Text Set in the {{City}} of {{Edinburgh}}},
  author = {Alex, Beatrice and Grover, Claire and Tobin, Richard and Oberlander, Jon},
  year = {2019},
  month = dec,
  journal = {Language Resources and Evaluation},
  volume = {53},
  number = {4},
  pages = {651--675},
  issn = {1574-020X, 1574-0218},
  doi = {10/ggwsd4},
  abstract = {While a reasonable amount of work has gone into automatically geoparsing text at the city or higher levels of granularity for different types of texts in different domains, there is relatively little research on geoparsing fine-grained locations such as buildings, green spaces and street names in text. This paper reports on how the Edinburgh Geoparser performs on this task for different types of literary text set in Edinburgh, the first UNESCO City of Literature. The non-copyrighted gold standard datasets created for this purpose are released along with this article.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Alex et al_2019_Geoparsing historical and contemporary literary text set in the City of.pdf}
}

@inproceedings{algan2005,
  title = {The Roots of Low European Employment: {{Family}} Culture?},
  booktitle = {{{NBER International Seminar}} on {{Macroeconomics}}},
  author = {Algan, Yann and Cahuc, Pierre},
  year = {2005},
  pages = {65--109},
  publisher = {{Cambridge, MIT Press}},
  mendeley-groups = {ENVS418},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000146}
}

@article{algiriyage2021,
  title = {Identifying {{Disaster-related Tweets}}: {{A Large-Scale Detection Model Comparison}}},
  author = {Algiriyage, Nilani and Prasanna, Raj},
  year = {2021},
  pages = {13},
  abstract = {Social media applications such as Twitter and Facebook are fast becoming a key instrument in gaining situational awareness (understanding the bigger picture of the situation) during disasters. This has provided multiple opportunities to gather relevant information in a timely manner to improve disaster response. In recent years, identifying crisis-related social media posts is analysed as an automatic task using machine learning (ML) or deep learning (DL) techniques. However, such supervised learning algorithms require labelled training data in the early hours of a crisis. Recently, multiple manually labelled disaster-related open-source twitter datasets have been released. In this work, we collected 192, 948 tweets by combining a number of such datasets, preprocessed, filtered and duplicate removed, which resulted in 117, 954 tweets. Then we evaluated the performance of multiple ML and DL algorithms in classifying disaster-related tweets in three settings, namely ``in-disaster'', ``out-disaster'' and ``cross-disaster''. Our results show that the Bidirectional LSTM model with Word2Vec embeddings performs well for the tweet classification task in all three settings. We also make available the preprocessing steps and trained weights for future research.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Algiriyage_Prasanna_2021_Identifying Disaster-related Tweets.pdf}
}

@manual{allaire2020,
  type = {Manual},
  title = {Rmarkdown: {{Dynamic}} Documents for r},
  author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},
  year = {2020}
}

@manual{allaire2020a,
  type = {Manual},
  title = {Rticles: {{Article}} Formats for r Markdown},
  author = {Allaire, JJ and Xie, Yihui and {R Foundation} and Wickham, Hadley and {Journal of Statistical Software} and Vaidyanathan, Ramnath and {Association for Computing Machinery} and Boettiger, Carl and {Elsevier} and Broman, Karl and Mueller, Kirill and Quast, Bastiaan and Pruim, Randall and Marwick, Ben and Wickham, Charlotte and Keyes, Oliver and Yu, Miao and Emaasit, Daniel and Onkelinx, Thierry and Gasparini, Alessandro and Desautels, Marc-Andre and Leutnant, Dominik and {MDPI} and {Taylor and Francis} and ??reden, O?uzhan and Hance, Dalton and N?st, Daniel and Uvesten, Petter and Campitelli, Elio and Muschelli, John and Hayes, Alex and Kamvar, Zhian N. and Ross, Noam and Cannoodt, Robrecht and Luguern, Duncan and Kaplan, David M. and Kreutzer, Sebastian and Wang, Shixiang and Hesselberth, Jay and Dervieux, Christophe},
  year = {2020}
}

@article{allsop2011,
  title = {An Update on the Association between Setting Quantified Road Safety Targets and Road Fatality Reduction},
  author = {Allsop, Richard E. and Sze, N.N. and Wong, S.C.},
  year = {2011},
  month = may,
  journal = {Accident Analysis \& Prevention},
  volume = {43},
  number = {3},
  pages = {1279--1283},
  issn = {00014575},
  doi = {10/fcprrn},
  abstract = {This note is intended to rectify estimates provided in a previous paper (Wong et al., 2006) of the shortterm effectiveness in terms of road fatality reduction of the setting of quantified road safety targets using data from 14 OECD countries during the period 1980\textendash 1999. This work is important in measuring the association between target setting and road safety improvement, because such targets are intended to serve as a useful tool to motivate timely road safety measures by the road authorities and others. The estimates to be rectified were based on before-and-after analysis using a comparison group of countries for each country that had set a target. This note first provides a correction to the qualification test for the inclusion of a country in any particular comparison group. It then presents the numerical effects of this correction on the estimates of the effectiveness of setting quantified road safety targets, both in individual countries and across the whole group of countries that set targets in the relevant period. Finally, impacts on the findings of the previous paper are discussed, with the conclusion that the changes in those of the numerical estimates that are affected do not alter the main message of the paper.},
  langid = {english},
  annotation = {ZSCC: 0000024},
  file = {/home/cjber/drive/pdf/ENVS492/Allsop et al_2011_.pdf}
}

@inproceedings{amitay2004,
  title = {Web-a-Where: Geotagging Web Content},
  shorttitle = {Web-a-Where},
  booktitle = {Proceedings of the 27th Annual International Conference on {{Research}} and Development in Information Retrieval  - {{SIGIR}} '04},
  author = {Amitay, Einat and Har'El, Nadav and Sivan, Ron and Soffer, Aya},
  year = {2004},
  pages = {273},
  publisher = {{ACM Press}},
  address = {{Sheffield, United Kingdom}},
  doi = {10/ddkccx},
  abstract = {We describe Web-a-Where, a system for associating geography with Web pages. Web-a-Where locates mentions of places and determines the place each name refers to. In addition, it assigns to each page a geographic focus \textemdash{} a locality that the page discusses as a whole. The tagging process is simple and fast, aimed to be applied to large collections of Web pages and to facilitate a variety of location-based applications and data analyses.},
  isbn = {978-1-58113-881-8},
  langid = {english},
  file = {/home/cjber/drive/pdf/Amitay et al_2004_Web-a-where.pdf}
}

@article{andogah2010,
  title = {Geographically {{Constrained Information Retrieval}}},
  author = {Andogah, Geoffrey},
  year = {2010},
  pages = {205},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Andogah_2010_Geographically Constrained Information Retrieval.pdf}
}

@article{andrienko2007,
  title = {Geovisual Analytics for Spatial Decision Support: {{Setting}} the Research Agenda},
  shorttitle = {Geovisual Analytics for Spatial Decision Support},
  author = {Andrienko, G. and Andrienko, N. and Jankowski, P. and Keim, D. and Kraak, M.-J. and MacEachren, A. and Wrobel, S.},
  year = {2007},
  month = sep,
  journal = {International Journal of Geographical Information Science},
  volume = {21},
  number = {8},
  pages = {839--857},
  issn = {1365-8816, 1362-3087},
  doi = {10/b5mck4},
  langid = {english},
  keywords = {SDS},
  file = {/home/cjber/drive/pdf/Andrienko et al_2007_Geovisual analytics for spatial decision support.pdf}
}

@book{angeles2014,
  title = {Fundamentals of {{Robotic Mechanical Systems}}},
  author = {Angeles, Jorge},
  year = {2014},
  series = {Mechanical {{Engineering Series}}},
  volume = {124},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-01851-5},
  isbn = {978-3-319-01850-8 978-3-319-01851-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Angeles_2014_Fundamentals of Robotic Mechanical Systems.pdf}
}

@article{anonymous2016,
  title = {I Take Your 999 Calls, but {{I}} Don't Know Where You Are},
  author = {Anonymous},
  year = {2016},
  month = mar,
  journal = {The Guardian},
  issn = {0261-3077},
  abstract = {So much of my job as a fire control operator is based on local knowledge. Now that we have to take calls from other counties, I fear for your safety},
  chapter = {Public Leaders Network},
  langid = {british},
  keywords = {ES},
  file = {/home/cjber/drive/zotero/storage/WUYYUV66/999-calls-fire-control-operator-local-knowledge.html}
}

@article{anselin2003,
  title = {Spatial {{Externalities}}, {{Spatial Multipliers}}, {{And Spatial Econometrics}}},
  author = {Anselin, Luc},
  year = {2003},
  month = apr,
  journal = {International Regional Science Review},
  volume = {26},
  number = {2},
  pages = {153--166},
  issn = {0160-0176, 1552-6925},
  doi = {10/ffrtwj},
  langid = {english},
  annotation = {ZSCC: 0001100},
  file = {/home/cjber/drive/pdf/ENVS453/anselin2003.pdf}
}

@article{anselin2007,
  title = {Spatial {{Regression Analysis}} in {{R A Workbook}}},
  author = {Anselin, Luc},
  year = {2007},
  pages = {68},
  abstract = {This paper is concerned with methods for analysing spatial data. After initial discussion on the nature of spatial data, including the concept of randomness, we focus most of our attention on linear regression models that involve interactions between agents across space. The introduction of spatial variables in to standard linear regression provides a flexible way of characterising these interactions, but complicates both interpretation and estimation of parameters of interest. The estimation of these models leads to three fundamental challenges: the reflection problem, the presence of omitted variables and problems caused by sorting. We consider possible solutions to these problems, with a particular focus on restrictions on the nature of interactions. We show that similar assumptions are implicit in the empirical strategies - fixed effects or spatial differencing - used to address these problems in reduced form estimation. These general lessons carry over to the policy evaluation literature.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS453/anselin2007.pdf}
}

@article{anselin2010,
  title = {Lagrange {{Multiplier Test Diagnostics}} for {{Spatial Dependence}} and {{Spatial Heterogeneity}}},
  author = {Anselin, Luc},
  year = {2010},
  month = sep,
  journal = {Geographical Analysis},
  volume = {20},
  number = {1},
  pages = {1--17},
  issn = {00167363},
  doi = {10/fvb3dv},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS453/Anselin_2010_.pdf}
}

@book{anselin2013,
  title = {Spatial Econometrics: Methods and Models},
  author = {Anselin, Luc},
  year = {2013},
  volume = {4},
  publisher = {{Springer Science \& Business Media}},
  isbn = {94-015-7799-4},
  keywords = {\#nosource},
  annotation = {ZSCC: 0012418}
}

@article{anselin2013a,
  title = {Spatial Fixed Effects and Spatial Dependence in a Single Cross-Section: {{Spatial}} Fixed Effects and Spatial Dependence},
  shorttitle = {Spatial Fixed Effects and Spatial Dependence in a Single Cross-Section},
  author = {Anselin, Luc and {Arribas-Bel}, Daniel},
  year = {2013},
  month = mar,
  journal = {Papers in Regional Science},
  volume = {92},
  number = {1},
  pages = {3--17},
  issn = {10568190},
  doi = {10/gf33rx},
  abstract = {We investigate the common conjecture in applied econometric work that the inclusion of spatial fixed effects in a regression specification for a single cross-sectional data set removes spatial dependence. We demonstrate analytically and by means of a series of simulation experiments how evidence of the removal of spatial autocorrelation by spatial fixed effects may be spurious when the true data generating processes (DGP) takes the form of a spatial lag or spatial error dependence. In addition, we also show that spatial fixed effects correctly remove spatial correlation only in the special case where the dependence is group-wise, with all observations in the same group as neighbours of each other.},
  langid = {english},
  annotation = {ZSCC: 0000088},
  file = {/home/cjber/drive/pdf/ENVS453/anselin2013.pdf}
}

@article{antoniou2010,
  ids = {antoniou},
  title = {Web 2.0 Geotagged Photos: {{Assessing}} the Spatial Dimension of the Phenomenon},
  shorttitle = {Web 2.0 Geotagged Photos},
  author = {Antoniou, Vyron and Morley, Jeremy and Haklay, Muki},
  year = {2010},
  month = jan,
  journal = {Geomatica},
  volume = {64},
  pages = {99--110},
  abstract = {Among popular Web 2.0 applications are the social networking, photo-sharing websites like Flickr, Panoramio, Picasa Web, and Geograph. The phenomenon of user-generated content and the increased presence of geographic information in such applications have motivated researchers to consider them as a source of geographic information. In this paper we question whether such web applications can serve as reliable sources of spatial content. We differentiate between spatially explicit and implicit applications, in accordance to their declared aims, and evaluate if they have an impact on the spatial distribution of geotagged photos for Great Britain. We also compare the spatial distribution of the photos submitted with population data, and the patterns of contribution to these sources over a period of 18 months. Finally, at a larger scale, we examine the spatial distribution of photos and their spatial density for 15 test areas and look into issues such as data currency and user behaviour. Our findings show that only web applications that urge users to interact directly with spatial entities can serve as reliable, universal sources of spatial content.},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Antoniou et al_2010_Web 2.pdf}
}

@article{ao2014,
  title = {Estimating the {{Locations}} of {{Emergency Events}} from {{Twitter Streams}}},
  author = {Ao, Ji and Zhang, Peng and Cao, Yanan},
  year = {2014},
  journal = {Procedia Computer Science},
  volume = {31},
  pages = {731--739},
  issn = {18770509},
  doi = {10/ggwjs3},
  abstract = {Sina Weibo, the most popular microblogging service in China, plays a significance role in sharing and exchanging information on local and global level. Actually, part of the information are correlated with some events which may be local ones such as accident, protests or natural disasters to widely ones such as events concerning well-known people or political affairs. Our research interest is to detect the location where some events are occurring by analyzing only data from weibo information. In this paper, we denote three kind of location for each weibo: (1) content-based location; (2) posting location; (3) registration location of author of weibo. There are two main contributions in our work: (a) detect event-aware location of each weibo; (b) estimate accurate event location based on sets of event-aware locations. In order to detect event-aware location, we model there kinds of location data of each weibo to tackle with detection challenge. After we obtained a set of event-aware locations, we use a hierarchical clustering to estimate the accurate event location. The result showed that our method could improve the accuracy of event location estimation. SS\textcopyright ceell22ee00cc11ttii44oonnPTuaahbnnelddiAspphueeeteedhrro--brrryese.vvEiPielesuwwebvluuiinsenhrddeeBedrr.Vrbreey.ssOppEooplnesnnessiviabbicieilclrieittsByys.ouVofnf.dtthehereCOOCrrgBgaaYnn-iizNziCinng-gNCDCoolmimcmemnistiette.teeeooffITITQQMM22001144. .},
  langid = {english},
  file = {/home/cjber/drive/pdf/Ao et al_2014_Estimating the Locations of Emergency Events from Twitter Streams.pdf}
}

@article{arampatzis2006,
  title = {Web-Based Delineation of Imprecise Regions},
  author = {Arampatzis, Avi and {van Kreveld}, Marc and Reinbacher, Iris and Jones, Christopher B. and Vaid, Subodh and Clough, Paul and Joho, Hideo and Sanderson, Mark},
  year = {2006},
  month = jul,
  journal = {Computers, Environment and Urban Systems},
  volume = {30},
  number = {4},
  pages = {436--459},
  issn = {01989715},
  doi = {10/cbdz86},
  abstract = {This paper describes several steps in the derivation of boundaries of imprecise regions using the Web as the information source. We discuss how to obtain locations that are part of and locations that are not part of the region to be delineated, and then we propose methods to compute the region algorithmically. The methods introduced are evaluated to judge the potential of the approach.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Arampatzis et al_2006_Web-based delineation of imprecise regions.pdf}
}

@article{ardanuy2020,
  title = {A {{Deep Learning Approach}} to {{Geographical Candidate Selection}} through {{Toponym Matching}}},
  author = {Ardanuy, Mariona Coll and Hosseini, Kasra and McDonough, Katherine and Krause, Amrey and {van Strien}, Daniel and Nanni, Federico},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.08114 [cs]},
  eprint = {2009.08114},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recognizing toponyms and resolving them to their real-world referents is required for providing advanced semantic access to textual data. This process is often hindered by the high degree of variation in toponyms. Candidate selection is the task of identifying the potential entities that can be referred to by a toponym previously recognized. While it has traditionally received little attention in the research community, it has been shown that candidate selection has a significant impact on downstream tasks (i.e. entity resolution), especially in noisy or non-standard text. In this paper, we introduce a flexible deep learning method for candidate selection through toponym matching, using state-of-the-art neural network architectures. We perform an intrinsic toponym matching evaluation based on several new realistic datasets, which cover various challenging scenarios (cross-lingual and regional variations, as well as OCR errors). We report its performance on candidate selection in the context of the downstream task of toponym resolution, both on existing datasets and on a new manually-annotated resource of nineteenth-century English OCR'd text.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Digital Libraries,Computer Science - Information Retrieval},
  file = {/home/cjber/drive/pdf/Ardanuy et al_2020_A Deep Learning Approach to Geographical Candidate Selection through Toponym.pdf;/home/cjber/drive/zotero/storage/CQ2P5U5V/2009.html}
}

@article{arnbjerg-nielsen2013,
  title = {Impacts of Climate Change on Rainfall Extremes and Urban Drainage Systems: A Review},
  shorttitle = {Impacts of Climate Change on Rainfall Extremes and Urban Drainage Systems},
  author = {{Arnbjerg-Nielsen}, K. and Willems, P. and Olsson, J. and Beecham, S. and Pathirana, A. and B{\"u}low Gregersen, I. and Madsen, H. and Nguyen, V.-T.-V.},
  year = {2013},
  month = jul,
  journal = {Water Science and Technology},
  volume = {68},
  number = {1},
  pages = {16--28},
  issn = {0273-1223, 1996-9732},
  doi = {10/f44bgx},
  abstract = {A review is made of current methods for assessing future changes in urban rainfall extremes and their effects on urban drainage systems, due to anthropogenic-induced climate change. The review concludes that in spite of significant advances there are still many limitations in our understanding of how to describe precipitation patterns in a changing climate in order to design and operate urban drainage infrastructure. Climate change may well be the driver that ensures that changes in urban drainage paradigms are identified and suitable solutions implemented. Design and optimization of urban drainage infrastructure considering climate change impacts and co-optimizing these with other objectives will become ever more important to keep our cities habitable into the future.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Arnbjerg-Nielsen et al_2013_Impacts of climate change on rainfall extremes and urban drainage systems.pdf}
}

@manual{arnold2019,
  type = {Manual},
  title = {Ggthemes: {{Extra}} Themes, Scales and Geoms for 'Ggplot2'},
  author = {Arnold, Jeffrey B.},
  year = {2019}
}

@misc{arthisuresh2020,
  title = {Arthisuresh/Coreference-Resolution},
  author = {{arthisuresh}},
  year = {2020},
  month = dec,
  abstract = {Efficient and clean PyTorch reimplementation of "End-to-end Neural Coreference Resolution" (Lee et al., EMNLP 2017).}
}

@article{arthur2018,
  title = {Social Sensing of Floods in the {{UK}}},
  author = {Arthur, Rudy and Boulton, Chris A. and Shotton, Humphrey and Williams, Hywel T. P.},
  editor = {Schumann, Guy J-P.},
  year = {2018},
  month = jan,
  journal = {PLOS ONE},
  volume = {13},
  number = {1},
  pages = {e0189327},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0189327},
  abstract = {Social sensing'' is a form of crowd-sourcing that involves systematic analysis of digital communications to detect real-world events. Here we consider the use of social sensing for observing natural hazards. In particular, we present a case study that uses data from a popular social media platform (Twitter) to detect and locate flood events in the UK. In order to improve data quality we apply a number of filters (timezone, simple text filters and a naive Bayes `relevance' filter) to the data. We then use place names in the user profile and message text to infer the location of the tweets. These two steps remove most of the irrelevant tweets and yield orders of magnitude more located tweets than we have by relying on geotagged data. We demonstrate that high resolution social sensing of floods is feasible and we can produce high-quality historical and real-time maps of floods using Twitter.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Arthur et al_2018_Social sensing of floods in the UK.pdf}
}

@book{asher2015,
  title = {Strengthening Social Protection in {{East Asia}}},
  author = {Asher, Mukul G and Kimura, Fukunari},
  year = {2015},
  publisher = {{Routledge}},
  isbn = {1-317-51067-4},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000006}
}

@article{ashktorab2014,
  title = {Tweedr: {{Mining Twitter}} to {{Inform}}},
  author = {Ashktorab, Zahra and Brown, Christopher and Nandi, Manojit and Culotta, Aron},
  year = {2014},
  pages = {5},
  abstract = {In this paper, we introduce Tweedr, a Twitter-mining tool that extracts actionable information for disaster relief workers during natural disasters. The Tweedr pipeline consists of three main parts: classification, clustering and extraction. In the classification phase, we use a variety of classification methods (sLDA, SVM, and logistic regression) to identify tweets reporting damage or casualties. In the clustering phase, we use filters to merge tweets that are similar to one another; and finally, in the extraction phase, we extract tokens and phrases that report specific information about different classes of infrastructure damage, damage types, and casualties. We empirically validate our approach with tweets collected from 12 different crises in the United States since 2006.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Ashktorab et al_2014_Tweedr.pdf}
}

@article{astarita2012,
  title = {A {{Mobile Application}} for {{Road Surface Quality Control}}: {{UNIquALroad}}},
  author = {Astarita, Vittorio},
  year = {2012},
  pages = {10},
  doi = {10/gf4vvk},
  abstract = {The monitoring of road surface conditions plays a key role in ensuring safety and comfort to the various road users, from pedestrians to drivers. Furthermore, having information on infrastructure quality allows road managers to guarantee an adequate maintenance. These data can be given and used at the same time by users, by means of mobile devices, widespread in Italy and in the World.},
  langid = {english},
  annotation = {ZSCC: 0000067},
  file = {/home/cjber/drive/pdf/ENVS492/Astarita_2012_.pdf}
}

@article{atefeh2015,
  title = {A {{Survey}} of {{Techniques}} for {{Event Detection}} in {{Twitter}}},
  author = {Atefeh, Farzindar and Khreich, Wael},
  year = {2015},
  journal = {Computational Intelligence},
  volume = {31},
  number = {1},
  pages = {132--164},
  issn = {1467-8640},
  doi = {10/f62chq},
  abstract = {Twitter is among the fastest-growing microblogging and online social networking services. Messages posted on Twitter (tweets) have been reporting everything from daily life stories to the latest local and global news and events. Monitoring and analyzing this rich and continuous user-generated content can yield unprecedentedly valuable information, enabling users and organizations to acquire actionable knowledge. This article provides a survey of techniques for event detection from Twitter streams. These techniques aim at finding real-world occurrences that unfold over space and time. In contrast to conventional media, event detection from Twitter streams poses new challenges. Twitter streams contain large amounts of meaningless messages and polluted content, which negatively affect the detection performance. In addition, traditional text mining techniques are not suitable, because of the short length of tweets, the large number of spelling and grammatical errors, and the frequent use of informal and mixed language. Event detection techniques presented in literature address these issues by adapting techniques from various fields to the uniqueness of Twitter. This article classifies these techniques according to the event type, detection task, and detection method and discusses commonly used features. Finally, it highlights the need for public benchmarks to evaluate the performance of different detection approaches and various features.},
  langid = {english},
  keywords = {event detection,event identification,microblogs,monitoring social media,Twitter data stream},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/coin.12017},
  file = {/home/cjber/drive/pdf/Atefeh_Khreich_2015_A Survey of Techniques for Event Detection in Twitter.pdf;/home/cjber/drive/zotero/storage/SVVZTP57/coin.html}
}

@incollection{auer2007,
  title = {{{DBpedia}}: {{A Nucleus}} for a {{Web}} of {{Open Data}}},
  shorttitle = {{{DBpedia}}},
  booktitle = {The {{Semantic Web}}},
  author = {Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and {Cudr{\'e}-Mauroux}, Philippe},
  year = {2007},
  volume = {4825},
  pages = {722--735},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-76298-0_52},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  isbn = {978-3-540-76297-3 978-3-540-76298-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Auer et al_2007_DBpedia.pdf}
}

@article{axelsson1999,
  title = {Processing of Laser Scanner Data\textemdash Algorithms and Applications},
  author = {Axelsson, Peter},
  year = {1999},
  month = jul,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {54},
  number = {2-3},
  pages = {138--147},
  issn = {09242716},
  doi = {10/c4g47v},
  abstract = {Airborne laser scanning systems are opening new possibilities for surveys and documentation of difficult areas and objects, such as dense city areas, forest areas and electrical power lines. Laser scanner systems available on the market are presently in a fairly mature state of art while the processing of airborne laser scanner data still is in an early phase of development. To come from irregular 3D point clouds to useful representations and formats for an end-user requires continued research and development of methods and algorithms for interpretation and modelling. This paper presents some methods and algorithms concerning filtering for determining the ground surface, DEM, classification of buildings for 3D City Models and the detection of electrical power lines. The classification algorithms are based on the Minimum Description Length criterion. The use of reflectance data and multiple echoes from the laser scanner is examined and found to be useful in many applications. q 1999 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  annotation = {ZSCC: 0000940},
  file = {/home/cjber/drive/pdf/ENVS492/Axelsson_1999_.pdf;/home/cjber/drive/pdf/ENVS492/Axelsson_1999_2.pdf}
}

@book{axler2015,
  title = {Linear {{Algebra Done Right}}},
  author = {Axler, Sheldon},
  year = {2015},
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-11080-6},
  isbn = {978-3-319-11079-0 978-3-319-11080-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Axler_2015_Linear Algebra Done Right.pdf}
}

@article{azizi2014,
  title = {Forest {{Road Detection Using LiDAR Data}}},
  author = {Azizi, Zahra and Najafi, Akbar and Sadeghian, Saeed},
  year = {2014},
  month = dec,
  journal = {Journal of Forestry Research},
  volume = {25},
  number = {4},
  pages = {975--980},
  issn = {1007-662X, 1993-0607},
  doi = {10/f6nx23},
  abstract = {We developed a three-step classification approach for forest road extraction utilizing LiDAR data. The first step employed the IDW method to interpolate LiDAR point data (first and last pulses) to achieve DSM, DTM and DNTM layers (at 1 m resolution). For this interpolation RMSE was 0.19 m. In the second step, the Support Vector Machine (SVM) was employed to classify the LiDAR data into two classes, road and non-road. For this classification, SVM indicated the merged distance layer with intensity data and yielded better identification of the road position. Assessments of the obtained results showed 63\% correctness, 75\% completeness and 52\% quality of classification. In the next step, road edges were defined in the LiDAR-extracted layers, enabling accurate digitizing of the centerline location. More than 95\% of the LiDAR-derived road was digitized within 1.3 m to the field surveyed normal. The proposed approach can provide thorough and accurate road inventory data to support forest management.},
  langid = {english},
  annotation = {ZSCC: 0000030},
  file = {/home/cjber/drive/pdf/ENVS492/Azizi et al_2014_.pdf}
}

@book{azzopardi2019,
  title = {Advances in Information Retrieval},
  author = {Azzopardi, Lief and Stein, Benno and Fuhr, Norbert and Mayr, Philipp},
  year = {2019},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{New York, NY}},
  isbn = {978-3-030-15718-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Azzopardi et al_2019_Advances in information retrieval.pdf}
}

@article{ba2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.06450 [cs, stat]},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/cjber/drive/pdf/Ba et al_2016_Layer Normalization.pdf;/home/cjber/drive/zotero/storage/L55TTRQC/1607.html}
}

@article{baevski2019,
  title = {Cloze-Driven {{Pretraining}} of {{Self-attention Networks}}},
  author = {Baevski, Alexei and Edunov, Sergey and Liu, Yinhan and Zettlemoyer, Luke and Auli, Michael},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.07785 [cs]},
  eprint = {1903.07785},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with the concurrently introduced BERT model. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Baevski et al_2019_Cloze-driven Pretraining of Self-attention Networks.pdf;/home/cjber/drive/zotero/storage/DF9WMWN9/1903.html}
}

@inproceedings{bakshi2016,
  title = {Opinion Mining and Sentiment Analysis},
  booktitle = {2016 3rd {{International Conference}} on {{Computing}} for {{Sustainable Global Development}} ({{INDIACom}})},
  author = {Bakshi, Rushlene Kaur and Kaur, Navneet and Kaur, Ravneet and Kaur, Gurpreet},
  year = {2016},
  pages = {452--455},
  publisher = {{IEEE}},
  isbn = {93-80544-21-9}
}

@inproceedings{baldinisoares2019,
  title = {Matching the {{Blanks}}: {{Distributional Similarity}} for {{Relation Learning}}},
  shorttitle = {Matching the {{Blanks}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Baldini Soares, Livio and FitzGerald, Nicholas and Ling, Jeffrey and Kwiatkowski, Tom},
  year = {2019},
  pages = {2895--2905},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10/ggwj3h},
  abstract = {General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Baldini Soares et al_2019_Matching the Blanks.pdf;/home/cjber/drive/pdf/Baldini Soares et al_2019_Matching the Blanks2.pdf}
}

@article{baldridge,
  title = {Methods and {{Applications}} of {{Text-Driven Toponym Resolution}} with {{Indirect Supervision}}},
  author = {Baldridge, Jason},
  pages = {173},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Baldridge_Methods and Applications of Text-Driven Toponym Resolution with Indirect.pdf}
}

@article{ballatore,
  title = {Prolegomena for an Ontology of Place},
  author = {Ballatore, Andrea},
  pages = {16},
  abstract = {The computational representation of place is one of the key research areas for the advancement of geographic information science (GIScience), bridging the gap between place-based human cognition and experience, and space-centered information systems. While many conceptual schemas, vocabularies and ontologies contain some notion of place, the concept is either left implicit or articulated in widely divergent ways. Because of its ubiquity, an ontological clarification of place seems overdue. Adopting the perspective of ontology engineering, and not that of philosophical Ontology, this article paves the way towards the formalization of a place ontology in two steps. First, it provides a critical survey of how this concept is currently represented from lightweight vocabularies to formal ontologies. Second, it presents a set of prolegomena for a place ontology that would overcome the limitations of current approaches. Acknowledging the cultural dependency of place, I argue that such an ontology should be seen as a module positioned between foundational and domain ontologies. This place ontology would provide (i) a conceptual tool to support the modeling of place in any domain, and (ii) a widely applicable ontology, whose deployment would increase the interoperability of datasets, particularly in the context of Linked Data.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Ballatore_Prolegomena for an ontology of place.pdf;/home/cjber/drive/pdf/Ballatore_Prolegomena for an ontology of place2.pdf}
}

@article{ballatore2013,
  title = {Geographic Knowledge Extraction and Semantic Similarity in {{OpenStreetMap}}},
  author = {Ballatore, Andrea and Bertolotto, Michela and Wilson, David C.},
  year = {2013},
  month = oct,
  journal = {Knowledge and Information Systems},
  volume = {37},
  number = {1},
  pages = {61--81},
  issn = {0219-1377, 0219-3116},
  doi = {10/f5bfcf},
  abstract = {In recent years a web phenomenon known as Volunteered Geographic Information (VGI) has produced large crowdsourced geographic datasets. OpenStreetMap (OSM), the leading VGI project, aims at building an open-content world map through user contributions. OSM semantics consists of a set of properties (called `tags') describing geographic classes, whose usage is defined by project contributors on a dedicated Wiki website. Because of its simple and open semantic structure, the OSM approach often results in noisy and ambiguous data, limiting its usability for analysis in information retrieval, recommender systems, and data mining. Devising a mechanism for computing the semantic similarity of the OSM geographic classes can help alleviate this semantic gap. The contribution of this paper is twofold. It consists of (i) the development of the OSM Semantic Network by means of a web crawler tailored to the OSM Wiki website; this semantic network can be used to compute semantic similarity through co-citation measures, providing a novel semantic tool for OSM and GIS communities; (ii) a study of the cognitive plausibility (i.e. the ability to replicate human judgement) of co-citation algorithms when applied to the computation of semantic similarity of geographic concepts. Empirical evidence supports the usage of co-citation algorithms \textendash{} SimRank showing the highest plausibility \textendash{} to compute concept similarity in a crowdsourced semantic network.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Ballatore et al_2013_Geographic knowledge extraction and semantic similarity in OpenStreetMap.pdf;/home/cjber/drive/pdf/Ballatore et al_2013_Geographic knowledge extraction and semantic similarity in OpenStreetMap2.pdf}
}

@article{ballatore2015,
  title = {Extracting {{Place Emotions}} from {{Travel Blogs}}},
  author = {Ballatore, Andrea and Adams, Benjamin},
  year = {2015},
  pages = {5},
  abstract = {Place is a central category in the human experience. Across cultures, individuals describe experiences, express opinions, narrate stories set in and about places. The web provides a large, dynamic corpus of documents describing places from a myriad of viewpoints. Emotions and their expression play an important role in these representations of places, making some places appear joyful and beautiful, and others scary, sad, or even disgusting. In this paper we propose to tap the corpus of place descriptions from the emotional viewpoint, aiming at the development of a framework to model, extract, and analyze emotions relative to places. As first steps in this direction, we focus on place classes, i.e. the types of places that are discussed, such as city, forest, and road. To identify such classes, we design the Place Vocabulary, a linked semantic resource that contains nouns in English that are used to identify natural and built places. Subsequently, we propose a natural language processing technique to extract a multi-dimensional model of place emotion, based on the vocabulary in WordNet-Affect. The technique is applied to a corpus of about 100,000 travel blog posts from travelblog.org, enabling the exploration of the emotional structure of place classes.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Ballatore_Adams_2015_Extracting Place Emotions from Travel Blogs.pdf}
}

@incollection{ballatore2018,
  title = {Charting the {{Geographies}} of {{Crowdsourced Information}} in {{Greater London}}},
  booktitle = {Geospatial {{Technologies}} for {{All}}},
  author = {Ballatore, Andrea and De Sabbata, Stefano},
  editor = {Mansourian, Ali and Pilesj{\"o}, Petter and Harrie, Lars and {van Lammeren}, Ron},
  year = {2018},
  pages = {149--168},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-78208-9_8},
  abstract = {Crowdsourcing platforms and social media produce distinctive geographies of informational content. The production process is enabled and influenced by a variety of socio-economic and demographic factors, shaping the place representation, i.e., the amount and type of information available in an area. In this study, we explore and explain the geographies of Twitter and Wikipedia in Greater London, highlighting the relationships between the crowdsourced data and the local geodemographic characteristics of the areas where they are located. Through a set of robust regression models on a sample of 1.6M tweets and about 22,000 Wikipedia articles, we identify level of education, presence of people aged 30-44, and property prices as the most important explanatory factors for place representation at the urban scale. To some extent, this confirms the received knowledge of such data being created primarily by relatively wealthy, young, and educated users. However, about half of the variability is left unexplained, suggesting that a broader inclusion of potential factors is necessary.},
  isbn = {978-3-319-78207-2 978-3-319-78208-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/Ballatore_De Sabbata_2018_Charting the Geographies of Crowdsourced Information in Greater London.pdf}
}

@article{ballatore2019,
  title = {Placing {{Wikimapia}}: An Exploratory Analysis},
  shorttitle = {Placing {{Wikimapia}}},
  author = {Ballatore, Andrea and Jokar Arsanjani, Jamal},
  year = {2019},
  month = aug,
  journal = {International Journal of Geographical Information Science},
  volume = {33},
  number = {8},
  pages = {1633--1650},
  issn = {1365-8816, 1362-3087},
  doi = {10/gjwbf2},
  abstract = {Wikimapia is a major privately-owned volunteered geographic information (VGI) project to collect information about places. Over the past ten years, Wikimapia has attracted hundreds of thousands of contributors and collected millions of data points, including towns, restaurants, lakes, and tourist attractions (http://wikimapia.org). Unlike OpenStreetMap, Wikimapia adopts a "placial" perspective, favouring rich descriptions over detailed geometries and encouraging the collection of textual and visual content about places with approximate footprints. In this article, we first trace the origin and development of Wikimapia as a for-profit project, intimately linked with search engine advertising. Drawing on an in-depth interview with a former developer, we analyse project's data model and characteristics of its community. As Wikimapia discussions are rife with copyright issues, we discuss the project's intellectual property, as well as its strategies for quality management. Second, we focus on the popularity of the project, which is crucial to the longevity and sustainability of VGI projects. Using behavioural data from Google Trends, we trace a geography of interest in Wikimapia, comparing with that in OpenStreetMap, from a temporal and spatial perspective. While OpenStreetMap attracts more interest in high-income countries, Wikimapia emerges as relatively more popular in low- and middle-income countries, countering the received notion of VGI as a Global North phenomenon. Our study suggests that Wikimapia's popularity is steadily declining.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Ballatore_Jokar Arsanjani_2019_Placing Wikimapia.pdf}
}

@article{bandyopadhyay2016,
  title = {Development of Agent Based Model for Predicting Emergency Response Time},
  author = {Bandyopadhyay, Mainak and Singh, Varun},
  year = {2016},
  month = sep,
  journal = {Perspectives in Science},
  volume = {8},
  pages = {138--141},
  issn = {22130209},
  doi = {10/ggwjsx},
  abstract = {Determining the time to reach any incident location by an emergency service is a very important aspect for emergency management. In most of the developing countries road network is considered as a main infrastructure for transporting emergency services. Therefore in order to predict the response time consideration must be given to the characteristics of road segments and driving behaviour of emergency vehicle drivers. In this paper real time driving data by Fire emergency service of Allahabad city is collected using GPS logger HOLUX M1000C. The spatial trajectories collected from GPS logger are analysed in GIS along with road network, population density and landuse data to determine the driver's route deciding behaviour. Based on the integrated analysis the Fire Emergency Vehicle Agent is designed. The Agent based model is simulated to determine the response time which is subsequently compared with the real response time.},
  langid = {english},
  keywords = {ABI,ES},
  file = {/home/cjber/drive/pdf/Bandyopadhyay_Singh_2016_Development of agent based model for predicting emergency response time.pdf}
}

@article{bar2016,
  title = {Graphetteer \textendash{} {{A}} Conceptual Model for a Graph Driven Gazetteer},
  author = {B{\"a}r, Manuel},
  year = {2016},
  pages = {9},
  abstract = {In this paper, I describe the state of the art in geographic information retrieval (GIR) and present a conceptual model for a fast, scalable graph based gazetteer, to be used in various domains of geographic information retrieval. The majority of available gazetteers are stored as relational databases, providing easy access but limiting the complexity of queries. In this paper I create and discuss a conceptual model of a gazetteer using a Neo4J graph database, which allows queries of high complexity to be efficiently run with low response time. A Neo4J graph database is installed onto a Linux server and is populated with a limited dataset to illustrate different aspects of the conceptual model. Queries are written using the generic Neo4J querying language CYPHER. Special focus is given to storing and querying semantic relationships between locations to allow queries about vague spatial relations on different scales. The proposed conceptual model has the potential to solve various state of the art limitations of available gazetteers discussed in this paper, but is accompanied by new limitations needing further attention.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Bär_2016_Graphetteer – A conceptual model for a graph driven gazetteer.pdf}
}

@article{barbieri2020,
  title = {{{TweetEval}}: {{Unified Benchmark}} and {{Comparative Evaluation}} for {{Tweet Classification}}},
  shorttitle = {{{TweetEval}}},
  author = {Barbieri, Francesco and {Camacho-Collados}, Jose and Neves, Leonardo and {Espinosa-Anke}, Luis},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.12421 [cs]},
  eprint = {2010.12421},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domainspecific data. In this paper, we propose a new evaluation framework (TWEETEVAL) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pretrained generic language models, and continue training them on Twitter corpora.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Social and Information Networks},
  file = {/home/cjber/drive/pdf/Barbieri et al_2020_TweetEval.pdf}
}

@article{barbosa2007,
  title = {Who Benefits from Access to Green Space? {{A}} Case Study from {{Sheffield}}, {{UK}}},
  shorttitle = {Who Benefits from Access to Green Space?},
  author = {Barbosa, Olga and Tratalos, Jamie A. and Armsworth, Paul R. and Davies, Richard G. and Fuller, Richard A. and Johnson, Pat and Gaston, Kevin J.},
  year = {2007},
  month = nov,
  journal = {Landscape and Urban Planning},
  volume = {83},
  number = {2-3},
  pages = {187--195},
  issn = {01692046},
  doi = {10/c6t32j},
  abstract = {Green spaces play a crucial role in supporting urban ecological and social systems, a fact recognised in public policy commitments in both the UK and Europe. The amount of provision, the distribution of green space and the ease of access to such spaces are key contributors to social and ecological function in urban environments. We measured distance along the transport network to public green space available to households in Sheffield, and compared this with the distribution of private garden space. In addition, we used a geodemographic database, Mosaic UK, to examine how access to green space varies across different sectors of society. Public green spaces are chronically underprovided relative to recommended targets. For example, 64\% of Sheffield households fail to meet the recommendation of the regulatory agency English Nature (EN), that people should live no further than 300 m from their nearest green space. Moreover, this figure rises to 72\% if we restrict attention to municipal parks recognised by the local council. There is an overall reduction in coverage by green space when moving from neighbourhoods where green space is primarily publicly provided to those where it is privately provided. While access to public green space varies significantly across different social groups, those enjoying the greatest access include more deprived groups and older people. This study highlights the need for additional green space to be created and existing green space to be protected in light of increasing development pressure.},
  langid = {english},
  annotation = {ZSCC: 0000493},
  file = {/home/cjber/drive/pdf/ENVS416/barbosa2007.pdf}
}

@article{bartholomew2011,
  title = {Hedonic Price Effects of Pedestrian-and Transit-Oriented Development},
  author = {Bartholomew, Keith and Ewing, Reid},
  year = {2011},
  journal = {Journal of Planning Literature},
  volume = {26},
  number = {1},
  pages = {18--34},
  issn = {0885-4122},
  doi = {10/dv3jmk},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000189}
}

@article{bartley2004,
  title = {Employment Status, Employment Conditions, and Limiting Illness: Prospective Evidence from the {{British}} Household Panel Survey 1991-2001},
  shorttitle = {Employment Status, Employment Conditions, and Limiting Illness},
  author = {Bartley, M},
  year = {2004},
  month = jun,
  journal = {Journal of Epidemiology \& Community Health},
  volume = {58},
  number = {6},
  pages = {501--506},
  issn = {0143-005X},
  doi = {10/dqdr8r},
  abstract = {Objectives: To assess the relation of the incidence of, and recovery from, limiting illness to employment status, occupational social class, and income over time in an initially healthy sample of working age men and women. Methods: Cox proportional hazards models. Results: There were large differences in the risk of limiting illness according to occupational social class, with men and women in the least favourable employment conditions nearly four times more likely to become ill than those in the most favourable. Unemployment and economic inactivity also had a powerful effect on illness incidence. Limiting illness was not a permanent state for most participants in the study. Employment status was also related to recovery. Conclusions: Having secure employment in favourable working conditions greatly reduces the risk of healthy people developing limiting illness. Secure employment increases the likelihood of recovery. These findings have considerable implications for both health inequality and economic policies.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS418/Bartley_2004_.pdf}
}

@article{baruya1998,
  title = {{{MASTER}}: {{Speed-accident}} Relationship on {{European}} Roads},
  author = {Baruya, A},
  year = {1998},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000014},
  file = {/home/cjber/drive/pdf/ENVS492/Baruya_1998_.pdf}
}

@article{bastianelli,
  title = {{{UNITOR-HMM-TK}}: {{Structured Kernel-based}} Learning for {{Spatial Role Labeling}}},
  author = {Bastianelli, Emanuele and Croce, Danilo and Basili, Roberto and Nardi, Daniele},
  pages = {7},
  abstract = {In this paper the UNITOR-HMM-TK system participating in the Spatial Role Labeling task at SemEval 2013 is presented. The spatial roles classification is addressed as a sequence-based word classification problem: the SVMhmm learning algorithm is applied, based on a simple feature modeling and a robust lexical generalization achieved through a Distributional Model of Lexical Semantics. In the identification of spatial relations, roles are combined to generate candidate relations, later verified by a SVM classifier. The Smoothed Partial Tree Kernel is applied, i.e. a convolution kernel that enhances both syntactic and lexical properties of the examples, avoiding the need of a manual feature engineering phase. Finally, results on three of the five tasks of the challenge are reported.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Bastianelli et al_UNITOR-HMM-TK.pdf}
}

@article{bateman2010,
  title = {A Linguistic Ontology of Space for Natural Language Processing},
  author = {Bateman, John A. and Hois, Joana and Ross, Robert and Tenbrink, Thora},
  year = {2010},
  month = sep,
  journal = {Artificial Intelligence},
  volume = {174},
  number = {14},
  pages = {1027--1071},
  issn = {00043702},
  doi = {10/bswg36},
  abstract = {We present a detailed semantics for linguistic spatial expressions supportive of computational processing that draws substantially on the principles and tools of ontological engineering and formal ontology. We cover language concerned with space, actions in space and spatial relationships and develop an ontological organization that relates such expressions to general classes of fixed semantic import. The result is given as an extension of a linguistic ontology, the Generalized Upper Model, an organization which has been used for over a decade in natural language processing applications. We describe the general nature and features of this ontology and show how we have extended it for working particularly with space. Treaitng the semantics of natural language expressions concerning space in this way offers a substantial simplification of the general problem of relating natural spatial language to its contextualized interpretation. Example specifications based on natural language examples are presented, as well as an evaluation of the ontology's coverage, consistency, predictive power, and applicability.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Bateman et al_2010_A linguistic ontology of space for natural language processing.pdf}
}

@book{batty2010,
  title = {The New Deal for Communities Experience: A Final Assessment},
  shorttitle = {The New Deal for Communities Experience},
  author = {Batty, Elaine},
  year = {2010},
  publisher = {{Communities and Local Government}},
  address = {{London}},
  langid = {english},
  annotation = {OCLC: 640076005},
  file = {/home/cjber/drive/pdf/ENVS416/batty2010.pdf}
}

@article{bbc2012,
  title = {Plan for 40mph Country Road Limit},
  author = {BBC},
  year = {2012},
  month = jul,
  journal = {BBC News},
  abstract = {Speed limits: 40mph plan for country roads},
  chapter = {UK},
  langid = {british},
  annotation = {ZSCC: 0000000[s1]},
  file = {/home/cjber/drive/zotero/storage/MEBDYBIU/uk-18840110.html}
}

@article{beck2020,
  title = {Representation {{Problems}} in {{Linguistic Annotations}}: {{Ambiguity}}, {{Variation}}, {{Uncertainty}}, {{Error}} and {{Bias}}},
  author = {Beck, Christin and Booth, Hannah and {El-Assady}, Mennatallah and Butt, Miriam},
  year = {2020},
  pages = {14},
  abstract = {The development of linguistic corpora is fraught with various problems of annotation and representation. These constitute a very real challenge for the development and use of annotated corpora, but as yet not much literature exists on how to address the underlying problems. In this paper, we identify and discuss five sources of representation problems, which are independent though interrelated: ambiguity, variation, uncertainty, error and bias. We outline and characterize these sources, discussing how their improper treatment can have stark consequences for research outcomes. Finally, we discuss how an adequate treatment can inform corpus-related linguistic research, both computational and theoretical, improving the reliability of research results and NLP models, as well as informing the more general reproducibility issue.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Beck et al_2020_Representation Problems in Linguistic Annotations.pdf}
}

@article{belanger2019,
  title = {Recent Optimization Models and Trends in Location, Relocation, and Dispatching of Emergency Medical Vehicles},
  author = {B{\'e}langer, V. and Ruiz, A. and Soriano, P.},
  year = {2019},
  month = jan,
  journal = {European Journal of Operational Research},
  volume = {272},
  number = {1},
  pages = {1--23},
  issn = {03772217},
  doi = {10/gfzb3w},
  abstract = {Over the past 10 years, a considerable amount of research has been devoted to the development of models to support decision making in the particular yet important context of Emergency Medical Services (EMS). More specifically, the need for advanced strategies to take into account the uncertainty and dynamism inherent to EMS, as well as the pertinence of socially oriented objectives, such as equity, and patient medical outcomes, have brought new and exciting challenges to the field. In this context, this paper summarizes and discusses modern modeling approaches to address problems related to ambulance fleet management, particularly those related to vehicle location and relocation, as well as dispatching decisions. Although it reviews early works on static ambulance location problems, this review concentrates on recent approaches to address tactical and operational decisions, and the interaction between these two types of decisions. Finally, it concludes on the current state of the art and identifies promising research avenues in the field.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Bélanger et al_2019_Recent optimization models and trends in location, relocation, and dispatching.pdf}
}

@manual{bengtsson2020,
  type = {Manual},
  title = {R.Utils: {{Various}} Programming Utilities},
  author = {Bengtsson, Henrik},
  year = {2020}
}

@article{bennett2007,
  title = {Semantic {{Categories Underlying}} the {{Meaning}} of `{{Place}}'},
  author = {Bennett, Brandon and Agarwal, Pragya},
  year = {2007},
  pages = {18},
  doi = {10/bv8bms},
  abstract = {This paper analyses the semantics of natural language expressions that are associated with the intuitive notion of `place'. We note that the nature of such terms is highly contested, and suggest that this arises from two main considerations: 1) there are a number of logically distinct categories of place expression, which are not always clearly distinguished in discourse about `place'; 2) the many non-substantive place count nouns (such as `place', `region', `area', etc.) employed in natural language are highly ambiguous. With respect to consideration 1), we propose that place-related expressions should be classified into the following distinct logical types: a) `place-like' count nouns (further subdivided into abstract, spatial and substantive varieties), b) proper names of `place-like' objects, c) locative property phrases, and d) definite descriptions of `place-like' objects. We outline possible formal representations for each of these. To address consideration 2), we examine meanings, connotations and ambiguities of the English vocabulary of abstract and generic place count nouns, and identify underlying elements of meaning, which explain both similarities and differences in the sense and usage of the various terms.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Bennett_Agarwal_2007_Semantic Categories Underlying the Meaning of ‘Place’.pdf;/home/cjber/drive/pdf/Bennett_Agarwal_2007_Semantic Categories Underlying the Meaning of ‘Place’2.pdf}
}

@article{benoit2011,
  title = {Linear {{Regression Models}} with {{Logarithmic Transformations}}},
  author = {Benoit, Kenneth},
  year = {2011},
  pages = {8},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Benoit_2011_Linear Regression Models with Logarithmic Transformations.pdf}
}

@article{bensalem2010,
  title = {Toponym {{Disambiguation}} by {{Arborescent Relationships}}},
  author = {{Bensalem}},
  year = {2010},
  month = jun,
  journal = {Journal of Computer Science},
  volume = {6},
  number = {6},
  pages = {653--659},
  issn = {1549-3636},
  doi = {10/d5mkf9},
  abstract = {Problem statement: The way of referring to a place in the geographical space can be formal, based on the spatial coordinates, or informal, which we use in natural language by using toponyms (place names). A toponym can represent several geographical places. This ambiguity made problematic its conversion towards a unique formal representation. Toponym disambiguation in text is the task of assigning a unique location to an ambiguous place name in a given textual context. Approach: Several toponym disambiguation heuristics assumed a geographical proximity between the toponyms of the same context. This proximity can be in terms of spatial distance or in terms of arborsecent relationships, i.e., proximity in the hierarchical tree of the world places. This study presented a new toponym disambiguation heuristic in text based on the quantification of the arborescent proximity between toponyms. This quantification was done by a new measure of geographical correlation that we call the Geographical Density. Results: Our method was compared to the state of the art methods using GeoSemCor corpus and it has outperformed them in term of recall (87.4\%) and coverage (99.0\%). The results showed that the toponyms of the same context are much closer in terms of arborescent relationships than in terms of spatial relationships. Conclusion: We believe that the quantification of arborescent relationships between toponyms of the same textual context is a good way to improve the recall of TD task. However, all the arborescent relationships' types must be considered and not only the meronymy, which is the relation the most exploited in the existing TD methods.},
  langid = {english},
  keywords = {TD},
  file = {/home/cjber/drive/pdf/Bensalem_2010_Toponym Disambiguation by Arborescent Relationships.pdf}
}

@incollection{bergstra2011,
  title = {Algorithms for {{Hyper-Parameter Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Bergstra, James S. and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  editor = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {2546--2554},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/cjber/drive/pdf/Bergstra et al_2011_Algorithms for Hyper-Parameter Optimization.pdf;/home/cjber/drive/zotero/storage/M2I5AT3K/4443-algorithms-for-hyper-parameter-optimization.html}
}

@book{berk2016,
  title = {Statistical {{Learning}} from a {{Regression Perspective}}},
  author = {Berk, Richard A.},
  year = {2016},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-44048-4},
  isbn = {978-3-319-44047-7 978-3-319-44048-4},
  langid = {english},
  file = {/home/cjber/drive/pdf/Berk_2016_Statistical Learning from a Regression Perspective.pdf}
}

@article{bertsimas2019,
  title = {Robust and Stochastic Formulations for Ambulance Deployment and Dispatch},
  author = {Bertsimas, Dimitris and Ng, Yeesian},
  year = {2019},
  month = dec,
  journal = {European Journal of Operational Research},
  volume = {279},
  number = {2},
  pages = {557--571},
  issn = {03772217},
  doi = {10/ggwjtr},
  abstract = {In Emergency Medical Systems, operators deploy a fleet of ambulances to a set of locations before dispatching them in response to emergency calls, with the goal of minimizing the fraction of calls with late response times. We propose stochastic and robust formulations for the ambulance deployment problem that use data on emergency calls to model uncertainty. By incorporating advances in column and constraint generation, our formulations are solved to exact optimality within minutes. In extensive computational experiments on Washington DC, our approach outperforms previous approaches (i.e. the MEXCLP and MALP) that rely on probabilistic assumptions about the availability of ambulances. Our formulations achieve a reduction of 19 to 28\% in number of shortfalls, requiring only 70\% of the total number of ambulances required in probabilistic models to attain comparable out-of-sample performance.},
  langid = {english},
  keywords = {ES},
  file = {/home/cjber/drive/pdf/Bertsimas_Ng_2019_Robust and stochastic formulations for ambulance deployment and dispatch.pdf}
}

@article{bevan2009,
  title = {Hitting and Missing Targets by Ambulance Services for Emergency Calls: Effects of Different Systems of Performance Measurement within the {{UK}}},
  shorttitle = {Hitting and Missing Targets by Ambulance Services for Emergency Calls},
  author = {Bevan, Gwyn and Hamblin, Richard},
  year = {2009},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {172},
  number = {1},
  pages = {161--190},
  issn = {09641998, 1467985X},
  doi = {10/fsvznd},
  abstract = {Following devolution, differences developed between UK countries in systems of measuring performance against a common target that ambulance services ought to respond to 75\% of calls for what may be immediately life threatening emergencies (category A calls) within 8 minutes. Only in England was this target integral to a ranking system of `star rating', which inflicted reputational damage on services that failed to hit targets, and only in England has this target been met. In other countries, the target has been missed by such large margins that services would have been publicly reported as failing, if they had been covered by the English system of star ratings. The paper argues that this case-study adds to evidence from comparisons of different systems of hospital performance measurement that, to have an effect, these systems need to be designed to inflict reputational damage on those that have performed poorly; and it explores implications of this hypothesis. The paper also asks questions about the adequacy of systems of performance measurement of ambulance services in UK countries.},
  langid = {english},
  keywords = {ES},
  file = {/home/cjber/drive/pdf/Bevan_Hamblin_2009_Hitting and missing targets by ambulance services for emergency calls.pdf}
}

@misc{bhps1991,
  title = {British {{Household Panel Survey}} ({{BHPS}}) - {{Institute}} for {{Social}} and {{Economic Research}} ({{ISER}})},
  author = {BHPS},
  year = {1991},
  howpublished = {https://www.iser.essex.ac.uk/bhps},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/zotero/storage/J5FQRTT7/bhps.html}
}

@inproceedings{bilhaut2003,
  title = {Geographic Reference Analysis for Geographic Document Querying},
  booktitle = {Proceedings of the {{HLT-NAACL}} 2003 Workshop on {{Analysis}} of Geographic References  -},
  author = {Bilhaut, Fr{\'e}d{\'e}rik and Charnois, Thierry and Enjalbert, Patrice and Mathet, Yann},
  year = {2003},
  volume = {1},
  pages = {55--62},
  publisher = {{Association for Computational Linguistics}},
  address = {{Not Known}},
  doi = {10/bgzwv3},
  abstract = {The work presented in this paper concerns Information Retrieval from geographical documents, i.e. documents with a major geographic component. The final aim, in response to an informational query of the user, is to return a ranked list of relevant passages in selected documents, allowing text browsing within them. We consider in this paper the spatial component of the texts and the queries. The idea is to perform an off-line linguistic analysis of the document, extracting spatial expressions (i.e. expressions denoting geographical localisations). The point is that such expressions are (in general) much more complex than simple place names. We present a linguistic analyser which recognises them, performing a semantic analysis and computing symbolic representations of their "content". These representations, stored in the text thanks to XML annotation, will act as indexes of passages with which queries are compared. The matching of queries with text expressions is a complex process, needing several kinds of numeric and symbolic computations. A prospective outline of it is described.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Bilhaut et al_2003_Geographic reference analysis for geographic document querying.pdf}
}

@article{bitter2007,
  title = {Incorporating Spatial Variation in Housing Attribute Prices: A Comparison of Geographically Weighted Regression and the Spatial Expansion Method},
  shorttitle = {Incorporating Spatial Variation in Housing Attribute Prices},
  author = {Bitter, Christopher and Mulligan, Gordon F. and Dall'erba, Sandy},
  year = {2007},
  month = mar,
  journal = {Journal of Geographical Systems},
  volume = {9},
  number = {1},
  pages = {7--27},
  issn = {1435-5930, 1435-5949},
  doi = {10/fh7vrd},
  abstract = {Hedonic house price models typically impose a constant price structure on housing characteristics throughout an entire market area. However, there is increasing evidence that the marginal prices of many important attributes vary over space, especially within large markets. In this paper, we compare two approaches to examine spatial heterogeneity in housing attribute prices within the Tucson, Arizona housing market: the spatial expansion method and geographically weighted regression (GWR). Our results provide strong evidence that the marginal price of key housing characteristics varies over space. GWR outperforms the spatial expansion method in terms of explanatory power and predictive accuracy.},
  langid = {english},
  annotation = {ZSCC: 0000238},
  file = {/home/cjber/drive/pdf/ENVS453/Bitter et al_2007_.pdf}
}

@article{blaschke2010,
  title = {Object Based Image Analysis for Remote Sensing},
  author = {Blaschke, T.},
  year = {2010},
  month = jan,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {65},
  number = {1},
  pages = {2--16},
  issn = {09242716},
  doi = {10/d4ksqf},
  abstract = {Remote sensing imagery needs to be converted into tangible information which can be utilised in conjunction with other data sets, often within widely used Geographic Information Systems (GIS). As long as pixel sizes remained typically coarser than, or at the best, similar in size to the objects of interest, emphasis was placed on per-pixel analysis, or even sub-pixel analysis for this conversion, but with increasing spatial resolutions alternative paths have been followed, aimed at deriving objects that are made up of several pixels. This paper gives an overview of the development of object based methods, which aim to delineate readily usable objects from imagery while at the same time combining image processing and GIS functionalities in order to utilize spectral and contextual information in an integrative way. The most common approach used for building objects is image segmentation, which dates back to the 1970s. Around the year 2000 GIS and image processing started to grow together rapidly through object based image analysis (OBIA - or GEOBIA for geospatial object based image analysis). In contrast to typical Landsat resolutions, high resolution images support several scales within their images. Through a comprehensive literature review several thousand abstracts have been screened, and more than 820 OBIA-related articles comprising 145 journal papers, 84 book chapters and nearly 600 conference papers, are analysed in detail. It becomes evident that the first years of the OBIA/GEOBIA developments were characterised by the dominance of `grey' literature, but that the number of peer-reviewed journal articles has increased sharply over the last four to five years. The pixel paradigm is beginning to show cracks and the OBIA methods are making considerable progress towards a spatially explicit information extraction workflow, such as is required for spatial planning as well as for many monitoring programmes.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS492/Blaschke_2010_.pdf}
}

@article{blaschke2014,
  title = {Geographic {{Object-Based Image Analysis}} \textendash{} {{Towards}} a New Paradigm},
  author = {Blaschke, Thomas and Hay, Geoffrey J. and Kelly, Maggi and Lang, Stefan and Hofmann, Peter and Addink, Elisabeth and Queiroz Feitosa, Raul and {van der Meer}, Freek and {van der Werff}, Harald and {van Coillie}, Frieke and Tiede, Dirk},
  year = {2014},
  month = jan,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {87},
  pages = {180--191},
  issn = {09242716},
  doi = {10/f3spfk},
  abstract = {The amount of scientific literature on (Geographic) Object-based Image Analysis \textendash{} GEOBIA has been and still is sharply increasing. These approaches to analysing imagery have antecedents in earlier research on image segmentation and use GIS-like spatial analysis within classification and feature extraction approaches. This article investigates these development and its implications and asks whether or not this is a new paradigm in remote sensing and Geographic Information Science (GIScience). We first discuss several limitations of prevailing per-pixel methods when applied to high resolution images. Then we explore the paradigm concept developed by Kuhn (1962) and discuss whether GEOBIA can be regarded as a paradigm according to this definition. We crystallize core concepts of GEOBIA, including the role of objects, of ontologies and the multiplicity of scales and we discuss how these conceptual developments support important methods in remote sensing such as change detection and accuracy assessment. The ramifications of the different theoretical foundations between the `per-pixel paradigm' and GEOBIA are analysed, as are some of the challenges along this path from pixels, to objects, to geo-intelligence. Based on several paradigm indications as defined by Kuhn and based on an analysis of peer-reviewed scientific literature we conclude that GEOBIA is a new and evolving paradigm.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Blaschke et al_2014_.pdf;/home/cjber/drive/pdf/ENVS492/Blaschke et al_2014_2.pdf;/home/cjber/drive/pdf/ENVS492/Blaschke et al_2014_3.pdf;/home/cjber/drive/pdf/ENVS492/Blaschke et al_2014_4.pdf}
}

@article{blincoe2006,
  title = {Speeding Drivers' Attitudes and Perceptions of Speed Cameras in Rural {{England}}},
  author = {Blincoe, Kate M. and Jones, Andrew P. and Sauerzapf, Violet and Haynes, Robin},
  year = {2006},
  month = mar,
  journal = {Accident Analysis \& Prevention},
  volume = {38},
  number = {2},
  pages = {371--378},
  issn = {00014575},
  doi = {10/bxdsdd},
  abstract = {There is evidence that excessive speed leads to an increased frequency and severity of road traffic accidents, but it is not clear how speeds may be reduced. To increase understanding of why drivers exceed the speed limits, the views of a sample of road users who had been prosecuted for exceeding the speed limit in the rural county of Norfolk England were sought.},
  langid = {english},
  annotation = {00071},
  file = {/home/cjber/drive/pdf/ENVS492/Road Assessment/blincoe2006.pdf}
}

@book{blomqvist2016,
  title = {Knowledge {{Engineering}} and {{Knowledge Management}}: 20th {{International Conference}}, {{EKAW}} 2016, {{Bologna}}, {{Italy}}, {{November}} 19-23, 2016, {{Proceedings}}},
  shorttitle = {Knowledge {{Engineering}} and {{Knowledge Management}}},
  editor = {Blomqvist, Eva and Ciancarini, Paolo and Poggi, Francesco and Vitali, Fabio},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {10024},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-49004-5},
  isbn = {978-3-319-49003-8 978-3-319-49004-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Blomqvist et al_2016_Knowledge Engineering and Knowledge Management.pdf}
}

@book{bonamente2017,
  title = {Statistics and {{Analysis}} of {{Scientific Data}}},
  author = {Bonamente, Massimiliano},
  year = {2017},
  series = {Graduate {{Texts}} in {{Physics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-6572-4},
  isbn = {978-1-4939-6570-0 978-1-4939-6572-4},
  langid = {english},
  file = {/home/cjber/drive/pdf/Bonamente_2017_Statistics and Analysis of Scientific Data.pdf}
}

@article{bongaarts2006,
  title = {The {{Proximate Determinants}} of {{Fertility}} in {{Sub-Saharan Africa}}},
  author = {Bongaarts, John and Frank, Odile and Lesthaeghe, Ron},
  year = {2006},
  journal = {Population and Development Review},
  issn = {00987921},
  doi = {10/ddgps3},
  abstract = {Although levels of fertility in most sub-Saharan Africa have not shown significant trends in the past few decades, substantial fertility differentials exist between countries and between regions and socioeconomic groups within countries. This paper examines the proximate determinants of fertility that are responsible for these variations in fertility. Particular attention is given to the biological and behavioral factors, such as postpartum abstinence, prolonged breastfeeding, and pathological sterility, which are crucial determinants of fertility in sub-Saharan Africa. Using a simple analytic model, the relative fertility-inhibiting effects of the proximate determinants are quantified, and from this analysis an assessment is made of prospects for future trends in fertility. It is concluded that rapid declines in fertility are unlikely to occur in the near future, partly because desired family size is very high and partly because upward pressure on fertility levels will result from the erosion of traditional childspacing practices of postpartum abstinence and prolonged breastfeeding or from declines in levels of pathological sterility in response to public health measures.},
  mendeley-groups = {ENVS418},
  keywords = {\#nosource},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{bontcheva2003,
  title = {{{GATE}}: {{A Unicode-based Infrastructure Supporting Multilingual Information Extraction}}},
  author = {Bontcheva, Kalina and Maynard, Diana and Tablan, Valentin and Cunningham, Hamish},
  year = {2003},
  pages = {8},
  abstract = {NLP infrastructures with comprehensive multilingual support can substantially decrease the overhead of developing Information Extraction (IE) systems in new languages by offering support for different character encodings, languageindependent components, and clean separation between linguistic data and the algorithms that use it. This paper will present GATE \textendash{} a Unicode-aware infrastructure that offers extensive support for multilingual Information Extraction with a special emphasis on lowoverhead portability between languages. GATE has been used in many research and commercial projects at Sheffield and elsewhere, including Information Extraction in Bulgarian, Romanian, Russian, and many other languages.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Bontcheva et al_2003_GATE.pdf;/home/cjber/drive/pdf/Bontcheva et al_2003_GATE2.pdf}
}

@book{borbinha2000,
  title = {Research and Advanced Technology for Digital Libraries: 4th {{European Conference}}, {{ECDL}} 2000, {{Lisbon}}, {{Portugal}}, {{September}} 18-20, 2000: Proceedings},
  shorttitle = {Research and Advanced Technology for Digital Libraries},
  editor = {Borbinha, Jos{\'e} and Baker, Thomas},
  year = {2000},
  series = {Lecture Notes in Computer Science},
  number = {1923},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  isbn = {978-3-540-41023-2},
  langid = {english},
  lccn = {ZA4080 .E28 2000},
  keywords = {Congresses,Digital libraries,Europe},
  file = {/home/cjber/drive/pdf/Borbinha_Baker_2000_Research and advanced technology for digital libraries.pdf}
}

@article{borlund2003,
  title = {The Concept of Relevance in {{IR}}},
  author = {Borlund, Pia},
  year = {2003},
  month = aug,
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {54},
  number = {10},
  pages = {913--925},
  issn = {1532-2882, 1532-2890},
  doi = {10/c37kw7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Borlund_2003_The concept of relevance in IR.pdf}
}

@book{borthwick2016,
  title = {Introduction to {{Partial Differential Equations}}},
  author = {Borthwick, David},
  year = {2016},
  series = {Universitext},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-48936-0},
  isbn = {978-3-319-48934-6 978-3-319-48936-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Borthwick_2016_Introduction to Partial Differential Equations.pdf}
}

@article{bosona2011,
  title = {Cluster Building and Logistics Network Integration of Local Food Supply Chain},
  author = {Bosona, T.G. and Gebresenbet, G.},
  year = {2011},
  month = apr,
  journal = {Biosystems Engineering},
  volume = {108},
  number = {4},
  pages = {293--302},
  issn = {15375110},
  doi = {10/dg2mcd},
  langid = {english},
  annotation = {ZSCC: 0000139},
  file = {/home/cjber/drive/pdf/ENVS492/Bosona_Gebresenbet_2011_.pdf}
}

@article{boulton2010,
  title = {Digital {{Scotland}}},
  author = {Boulton, G},
  year = {2010},
  journal = {Report by Royal Society of Edinburgh},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000004}
}

@article{bowen2001,
  title = {Theoretical and Empirical Considerations Regarding Space in Hedonic Housing Price Model Applications},
  author = {Bowen, William M and Mikelbank, Brian A and Prestegaard, Dean M},
  year = {2001},
  journal = {Growth and change},
  volume = {32},
  number = {4},
  pages = {466--490},
  issn = {0017-4815},
  doi = {10/fr4x2j},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000146}
}

@article{bowling2009,
  title = {A Logistic Approximation to the Cumulative Normal Distribution},
  author = {Bowling, Shannon R. and Khasawneh, Mohammad T. and Kaewkuekool, Sittichai and Cho, Byung Rae},
  year = {2009},
  month = jul,
  journal = {Journal of Industrial Engineering and Management},
  volume = {2},
  number = {1},
  pages = {114--127},
  issn = {2013-0953},
  doi = {10/b89r64},
  abstract = {This paper develops a logistic approximation to the cumulative normal distribution. Although the literature contains a vast collection of approximate functions for the normal distribution, they are very complicated, not very accurate, or valid for only a limited range. This paper proposes an enhanced approximate function. When comparing the proposed function to other approximations studied in the literature, it can be observed that the proposed logistic approximation has a simpler functional form and that it gives higher accuracy, with the maximum error of less than 0.00014 for the entire range. This is, to the best of the authors' knowledge, the lowest level of error reported in the literature. The proposed logistic approximate function may be appealing to researchers, practitioners and educators given its functional simplicity and mathematical accuracy.},
  langid = {english},
  annotation = {ZSCC: 0000087},
  file = {/home/cjber/drive/pdf/ENVS492/Bowling et al_2009_.pdf}
}

@book{bowman1997,
  title = {Applied Smoothing Techniques for Data Analysis: The Kernel Approach with {{S-Plus}} Illustrations},
  author = {Bowman, Adrian W and Azzalini, Adelchi},
  year = {1997},
  volume = {18},
  publisher = {{OUP Oxford}},
  isbn = {0-19-154569-4},
  keywords = {\#nosource}
}

@book{bramer2016,
  ids = {bramer2016a},
  title = {Principles of {{Data Mining}}},
  author = {Bramer, Max},
  year = {2016},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-7307-6},
  isbn = {978-1-4471-7306-9 978-1-4471-7307-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Bramer_2016_Principles of Data Mining.pdf;/home/cjber/drive/zotero/storage/YG7HVS53/Bramer_2016_Principles of Data Mining2.pdf}
}

@book{brandt2014,
  title = {Data {{Analysis}}},
  author = {Brandt, Siegmund},
  year = {2014},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-03762-2},
  isbn = {978-3-319-03761-5 978-3-319-03762-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Brandt_2014_Data Analysis.pdf;/home/cjber/drive/pdf/Brandt_2014_Data Analysis2.pdf}
}

@article{brants2003,
  title = {A {{System}} for {{New Event Detection}}},
  author = {Brants, Thorsten and Chen, Francine and Farahat, Ayman},
  year = {2003},
  pages = {8},
  doi = {10/fcmb5q},
  abstract = {We present a new method and system for performing the New Event Detection task, i.e., in one or multiple streams of news stories, all stories on a previously unseen (new) event are marked. The method is based on an incremental TF-IDF model. Our extensions include: generation of source-specific models, similarity score normalization based on document-specific averages, similarity score normalization based on source-pair specific averages, term reweighting based on inverse event frequencies, and segmentation of the documents. We also report on extensions that did not improve results. The system performs very well on TDT3 and TDT4 test data and scored second in the TDT-2002 evaluation.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Brants et al_2003_A System for New Event Detection.pdf}
}

@article{brengarth2016,
  title = {{{WEB}} 2.0: {{How}} Social Media Applications Leverage Nonprofit Responses during a Wildfire Crisis},
  shorttitle = {{{WEB}} 2.0},
  author = {Brengarth, Lauren Bacon and Mujkic, Edin},
  year = {2016},
  month = jan,
  journal = {Computers in Human Behavior},
  volume = {54},
  pages = {589--596},
  issn = {07475632},
  doi = {10/gmkgqg},
  abstract = {This study examines how Web 2.0 applications were used during a catastrophic wildfire in the Western United States that claimed two human lives, more than 18,000 acres of land and nearly 350 homes. The study sheds light on how Web 2.0 applications were applied as a tool to transmit information while the disaster was unfolding. This research highlights unique nonprofit cases that inform the role and reliability of Web 2.0 applications during a crisis, and the roles that nonprofit organizations and the general public play while facing a dire emergency. In the cases presented, Web 2.0 applications served as a bridge between first responders, the population in immediate wildfire danger, and the citizens who were trying to help, resulting in saved lives, property, and natural resources. By combining existing literature and collected qualitative data, the researchers argue that Web 2.0 applications represent flexible communication tools for transmission of timely information during a crisis situation.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Brengarth_Mujkic_2016_WEB 2.pdf}
}

@inproceedings{bridson2005,
  title = {Simulation of Clothing with Folds and Wrinkles},
  booktitle = {{{ACM SIGGRAPH}} 2005 {{Courses}} on   - {{SIGGRAPH}} '05},
  author = {Bridson, R. and Marino, S. and Fedkiw, R.},
  year = {2005},
  pages = {3},
  publisher = {{ACM Press}},
  address = {{Los Angeles, California}},
  doi = {10/c8qsq9},
  abstract = {Clothing is a fundamental part of a character's persona, a key storytelling tool used to convey an intended impression to the audience. Draping, folding, wrinkling, stretching, etc. all convey meaning, and thus each is carefully controlled when filming live actors. When making films with computer simulated cloth, these subtle but important elements must be captured. In this paper we present several methods essential to matching the behavior and look of clothing worn by digital stand-ins to their real world counterparts. Novel contributions include a mixed explicit/implicit time integration scheme, a physically correct bending model with (potentially) nonzero rest angles for pre-shaping wrinkles, an interface forecasting technique that promotes the development of detail in contact regions, a post-processing method for treating cloth-character collisions that preserves folds and wrinkles, and a dynamic constraint mechanism that helps to control large scale folding. The common goal of all these techniques is to produce a cloth simulation with many folds and wrinkles improving the realism.},
  langid = {english},
  annotation = {ZSCC: 0000550},
  file = {/home/cjber/drive/pdf/ENVS492/Bridson et al_2005_.pdf}
}

@article{brindley2018,
  title = {Generating Vague Neighbourhoods through Data Mining of Passive Web Data},
  author = {Brindley, P. and Goulding, J. and Wilson, M. L.},
  year = {2018},
  month = mar,
  journal = {International Journal of Geographical Information Science},
  volume = {32},
  number = {3},
  pages = {498--523},
  issn = {1365-8816, 1362-3087},
  doi = {10/gn63nw},
  abstract = {Neighbourhoods have been described as `the building blocks of public services society'. Their subjective nature, however, and the resulting difficulties in collecting data, means that in many countries there are no officially defined neighbourhoods either in terms of names or boundaries. This has implications not only for policy but also business and social decisions as a whole. With the absence of neighbourhood boundaries many studies resort to using standard administrative units as proxies. Such administrative geographies, however, often have a poor fit with those perceived by residents. Our approach detects these important social boundaries by automatically mining the Web en masse for passively declared neighbourhood data within postal addresses. Focusing on the United Kingdom (UK), this research demonstrates the feasibility of automated extraction of urban neighbourhood names and their subsequent mapping as vague entities. Importantly, and unlike previous work, our process does not require any neighbourhood names to be established a priori.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Brindley et al_2018_Generating vague neighbourhoods through data mining of passive web data.pdf}
}

@article{britton1990,
  title = {The Influence of Socio-Economic and Environmental Factors on Geographic Variation in Mortality},
  author = {Britton, M and Fox, AJ and Goldblatt, P and Jones, DR and Rosato, M},
  year = {1990},
  journal = {Mortality \& Geography A review in the mid-1980s, Office of Population, Censuses and Surveys: London},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000043}
}

@book{brockwell2016,
  title = {Introduction to {{Time Series}} and {{Forecasting}}},
  author = {Brockwell, Peter J. and Davis, Richard A.},
  year = {2016},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-29854-2},
  isbn = {978-3-319-29852-8 978-3-319-29854-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Brockwell_Davis_2016_Introduction to Time Series and Forecasting.pdf}
}

@article{brooks-gunn1993,
  title = {Do Neighborhoods Influence Child and Adolescent Development?},
  author = {{Brooks-Gunn}, Jeanne and Duncan, Greg J and Klebanov, Pamela Kato and Sealand, Naomi},
  year = {1993},
  journal = {American journal of sociology},
  volume = {99},
  number = {2},
  pages = {353--395},
  issn = {0002-9602},
  doi = {10/dw4vdx},
  file = {/home/cjber/drive/pdf/ENVS416/Brooks-Gunn et al_1993_.pdf}
}

@article{brouwer2017,
  title = {Probabilistic Flood Extent Estimates from Social Media Flood Observations},
  author = {Brouwer, Tom and Eilander, Dirk and {van Loenen}, Arnejan and Booij, Martijn J. and Wijnberg, Kathelijne M. and Verkade, Jan S. and Wagemaker, Jurjen},
  year = {2017},
  month = may,
  journal = {Natural Hazards and Earth System Sciences},
  volume = {17},
  number = {5},
  pages = {735--747},
  issn = {1684-9981},
  doi = {10/gcdh2v},
  abstract = {The increasing number and severity of floods, driven by phenomena such as urbanization, deforestation, subsidence and climate change, create a growing need for accurate and timely flood maps. In this paper we present and evaluate a method to create deterministic and probabilistic flood maps from Twitter messages that mention locations of flooding. A deterministic flood map created for the December 2015 flood in the city of York (UK) showed good performance (F (2) = 0.69; a statistic ranging from 0 to 1, with 1 expressing a perfect fit with validation data). The probabilistic flood maps we created showed that, in the York case study, the uncertainty in flood extent was mainly induced by errors in the precise locations of flood observations as derived from Twitter data. Errors in the terrain elevation data or in the parameters of the applied algorithm contributed less to flood extent uncertainty. Although these maps tended to overestimate the actual probability of flooding, they gave a reasonable representation of flood extent uncertainty in the area. This study illustrates that inherently uncertain data from social media can be used to derive information about flooding.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Brouwer et al_2017_Probabilistic flood extent estimates from social media flood observations.pdf}
}

@inproceedings{bruce1998,
  title = {Word-{{Sense Distinguishability}} and {{Inter-Coder Agreement}}},
  booktitle = {Proceedings of the {{Third Conference}} on {{Empirical Methods}} for {{Natural Language Processing}}},
  author = {Bruce, Rebecca and Wiebe, Janyce},
  year = {1998},
  month = jun,
  pages = {53--60},
  publisher = {{Association for Computational Linguistics}},
  address = {{Palacio de Exposiciones y Congresos, Granada, Spain}},
  file = {/home/cjber/drive/pdf/Bruce_Wiebe_1998_Word-Sense Distinguishability and Inter-Coder Agreement.pdf}
}

@inproceedings{brunner2008,
  title = {Spatial Autocorrelation and Toponym Ambiguity},
  booktitle = {Proceeding of the 2nd International Workshop on {{Geographic}} Information Retrieval - {{GIR}} '08},
  author = {Brunner, Tobias Josef and Purves, Ross Stuart},
  year = {2008},
  pages = {25},
  publisher = {{ACM Press}},
  address = {{Napa Valley, California, USA}},
  doi = {10/d4vgdj},
  abstract = {In this paper, we explore the spatial distribution of the referents of ambiguous toponyms and compare it to the distribution of randomly selected unambiguous toponym pairs. We show that for a number of gazetteers, ambiguous toponyms are spatially autocorrelated and that typical autocorrelations are similar to the size of document scopes for a newspaper corpus.},
  isbn = {978-1-60558-253-5},
  langid = {english},
  keywords = {TD},
  file = {/home/cjber/drive/pdf/Brunner_Purves_2008_Spatial autocorrelation and toponym ambiguity.pdf}
}

@book{buchmann2014,
  title = {Knowledge {{Science}}, {{Engineering}} and {{Management}}: 7th {{International Conference}}, {{KSEM}} 2014, {{Sibiu}}, {{Romania}}, {{October}} 16-18, 2014. {{Proceedings}}},
  shorttitle = {Knowledge {{Science}}, {{Engineering}} and {{Management}}},
  editor = {Buchmann, Robert and Kifor, Claudiu Vasile and Yu, Jian},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {8793},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-12096-6},
  isbn = {978-3-319-12095-9 978-3-319-12096-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Buchmann et al_2014_Knowledge Science, Engineering and Management.pdf;/home/cjber/drive/pdf/Buchmann et al_2014_Knowledge Science, Engineering and Management2.pdf}
}

@article{buitinck2013,
  title = {{{API}} Design for Machine Learning Software: Experiences from the Scikit-Learn Project},
  shorttitle = {{{API}} Design for Machine Learning Software},
  author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"e}l},
  year = {2013},
  month = sep,
  journal = {arXiv:1309.0238 [cs]},
  eprint = {1309.0238},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Mathematical Software},
  file = {/home/cjber/drive/pdf/Buitinck et al_2013_API design for machine learning software.pdf;/home/cjber/drive/zotero/storage/PPYMTMZG/1309.html}
}

@book{bunt2011,
  ids = {bunt2011a},
  title = {Proceedings of the 6th {{Joint ISO-ACL SIGSEM Workshop}} on {{Interoperable Semantic Annotation}} ({{ISA-6}})},
  author = {Bunt, H.C},
  year = {2011},
  publisher = {{University of Oxford}},
  isbn = {978-90-74029-35-3},
  langid = {english},
  annotation = {OCLC: 6893472594},
  file = {/home/cjber/drive/pdf/Bunt_2011_Proceedings of the 6th Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic.pdf;/home/cjber/drive/pdf/Bunt_2011_Proceedings of the 6th Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic2.pdf;/home/cjber/drive/pdf/Bunt_2011_Proceedings of the 6th Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic3.pdf}
}

@book{burger2016,
  title = {Digital {{Image Processing}}: {{An Algorithmic Introduction Using Java}}},
  shorttitle = {Digital {{Image Processing}}},
  author = {Burger, Wilhelm and Burge, Mark J.},
  year = {2016},
  series = {Texts in {{Computer Science}}},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-6684-9},
  isbn = {978-1-4471-6683-2 978-1-4471-6684-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/Burger_Burge_2016_Digital Image Processing.pdf}
}

@book{burke2014,
  title = {Search {{Methodologies}}},
  editor = {Burke, Edmund K. and Kendall, Graham},
  year = {2014},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4614-6940-7},
  isbn = {978-1-4614-6939-1 978-1-4614-6940-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Burke_Kendall_2014_Search Methodologies.pdf}
}

@article{burnett2003,
  title = {A Multi-Scale Segmentation/Object Relationship Modelling Methodology for Landscape Analysis},
  author = {Burnett, C and Blaschke, Thomas},
  year = {2003},
  month = oct,
  journal = {Ecological Modelling},
  volume = {168},
  number = {3},
  pages = {233--249},
  issn = {03043800},
  doi = {10/dkntbn},
  abstract = {Natural complexity can best be explored using spatial analysis tools based on concepts of landscape as process continuums that can be partially decomposed into objects or patches. We introduce a five-step methodology based on multi-scale segmentation and object relationship modelling. Hierarchical patch dynamics (HPD) is adopted as the theoretical framework to address issues of heterogeneity, scale, connectivity and quasi-equilibriums in landscapes. Remote sensing has emerged as the most useful data source for characterizing land use/land cover but a vast majority of applications rely on basic image processing concepts developed in the 1970s: one spatial scale, per-pixel classification of a multi-scale spectral feature space. We argue that this methodology does not make sufficient use of spatial concepts of neighbourhood, proximity or homogeneity. In contrast, the authors demonstrate in this article the utility of the HPD framework as a theoretical basis for landscape analysis in two different projects using alternative image processing methodologies, which try to overcome the `pixel-centred' view.},
  langid = {english},
  annotation = {ZSCC: 0000691},
  file = {/home/cjber/drive/pdf/ENVS492/Burnett_Blaschke_2003_.pdf;/home/cjber/drive/pdf/ENVS492/Burnett_Blaschke_2003_2.pdf}
}

@article{burstein1980,
  title = {Chapter 4: {{The Analysis}} of {{Multilevel Data}} in {{Educational Research}} and {{Evaluation}}},
  author = {Burstein, Leigh},
  year = {1980},
  month = jan,
  journal = {Review of Research in Education},
  volume = {8},
  number = {1},
  pages = {158--233},
  issn = {0091-732X},
  doi = {10/dmjq3f},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000624}
}

@article{burton2004,
  title = {What Works in Community Involvement in Area-Based Initiatives? {{A}} Systematic Review of the Literature},
  author = {Burton, Paul and Goodlad, Robina and Croft, Jacqui and Abbott, Jo and Hastings, Annette and Macdonald, Geraldine and Slater, Tom},
  year = {2004},
  pages = {90},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/burton2004.pdf}
}

@article{buscaldi2008,
  title = {A Conceptual Density-based Approach for the Disambiguation of Toponyms},
  author = {Buscaldi, Davide and Rosso, Paulo},
  year = {2008},
  month = mar,
  journal = {International Journal of Geographical Information Science},
  volume = {22},
  number = {3},
  pages = {301--313},
  issn = {1365-8816, 1362-3087},
  doi = {10.1080/13658810701626251},
  langid = {english},
  keywords = {Ontology,TD},
  file = {/home/cjber/drive/pdf/Buscaldi_Rosso_2008_A conceptual density‐based approach for the disambiguation of toponyms.pdf}
}

@inproceedings{buscaldi2008a,
  title = {Map-Based vs. Knowledge-Based Toponym Disambiguation},
  booktitle = {Proceeding of the 2nd International Workshop on {{Geographic}} Information Retrieval - {{GIR}} '08},
  author = {Buscaldi, Davide and Rosso, Paolo},
  year = {2008},
  pages = {19},
  publisher = {{ACM Press}},
  address = {{Napa Valley, California, USA}},
  doi = {10.1145/1460007.1460011},
  abstract = {Toponym Disambiguation, i.e. the task of assigning to place name their correct reference in the world, is getting more attention from many researchers. Many methods have been proposed since now, making use of different resources, techniques and sense inventories. Unfortunately, a gold standard for the evaluation of those methods is not yet available; therefore, it is difficult to verify the performance of such methods. Recently, a georeferenced version of WordNet has been developed, a resource that can be used to compare methods that are based on geographical data with methods that use textual information. In this paper we carry out a comparison between two of these methods. The results show that the knowledge-based method allowed us to obtain better results with a smaller context size. On the other hand, we observed that the map-based method needs a large context to obtain a good accuracy.},
  isbn = {978-1-60558-253-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Buscaldi_Rosso_2008_Map-based vs.pdf}
}

@inproceedings{buscaldi2010,
  ids = {buscaldi2010a},
  title = {Grounding Toponyms in an {{Italian}} Local News Corpus},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Geographic Information Retrieval}} - {{GIR}} '10},
  author = {Buscaldi, Davide and Magnini, Bernardo},
  year = {2010},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Zurich, Switzerland}},
  doi = {10/btcb8z},
  abstract = {In this paper we present a study carried out over toponyms contained in an Italian news collection, in order to determine the degree of ambiguity of toponyms and how difficult could be to resolve such ambiguities. The results show that frequent toponyms are usually less ambiguous than rare toponyms. The resolution of ambiguities on a sample of 1, 042 toponyms with different features confirms that ambiguous toponyms are spatially autocorrelated.},
  isbn = {978-1-60558-826-1},
  langid = {english},
  keywords = {TD},
  file = {/home/cjber/drive/pdf/Buscaldi_Magnini_2010_Grounding toponyms in an Italian local news corpus.pdf;/home/cjber/drive/pdf/Buscaldi_Magnini_2010_Grounding toponyms in an Italian local news corpus2.pdf;/home/cjber/drive/pdf/Buscaldi_Magnini_2010_Grounding toponyms in an Italian local news corpus3.pdf}
}

@article{buscaldi2011,
  title = {Approaches to Disambiguating Toponyms},
  author = {Buscaldi, Davide},
  year = {2011},
  month = jul,
  journal = {SIGSPATIAL Special},
  volume = {3},
  number = {2},
  pages = {16--19},
  issn = {19467729},
  doi = {10/dzphpd},
  langid = {english},
  keywords = {TD},
  file = {/home/cjber/drive/pdf/Buscaldi_2011_Approaches to disambiguating toponyms.pdf}
}

@article{calinski1974,
  title = {A Dendrite Method for Cluster Analysis},
  author = {Calinski, T. and Harabasz, J.},
  year = {1974},
  journal = {Communications in Statistics - Theory and Methods},
  volume = {3},
  number = {1},
  pages = {1--27},
  issn = {0361-0926},
  doi = {10/dsds7k},
  langid = {english},
  file = {/home/cjber/drive/pdf/Calinski_Harabasz_1974_A dendrite method for cluster analysis.pdf}
}

@article{caltagirone2018,
  title = {{{LIDAR-Camera Fusion}} for {{Road Detection Using Fully Convolutional Neural Networks}}},
  author = {Caltagirone, Luca and Bellone, Mauro and Svensson, Lennart and Wahde, Mattias},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.07941 [cs]},
  eprint = {1809.07941},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work, a deep learning approach has been developed to carry out road detection by fusing LIDAR point clouds and camera images. An unstructured and sparse point cloud is first projected onto the camera image plane and then upsampled to obtain a set of dense 2D images encoding spatial information. Several fully convolutional neural networks (FCNs) are then trained to carry out road detection, either by using data from a single sensor, or by using three fusion strategies: early, late, and the newly proposed cross fusion. Whereas in the former two fusion approaches, the integration of multimodal information is carried out at a predefined depth level, the cross fusion FCN is designed to directly learn from data where to integrate information; this is accomplished by using trainable cross connections between the LIDAR and the camera processing branches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/cjber/drive/pdf/ENVS492/Caltagirone et al_2018_.pdf}
}

@article{caragea2011,
  title = {Classifying {{Text Messages}} for the {{Haiti Earthquake}}},
  author = {Caragea, Cornelia and McNeese, Nathan and Jaiswal, Anuj and Traylor, Greg and Kim, Hyun-Woo and Mitra, Prasenjit and Wu, Dinghao and Tapia, Andrea H and Giles, Lee and Jansen, Bernard J and Yen, John},
  year = {2011},
  pages = {10},
  abstract = {In case of emergencies (e.g., earthquakes, flooding), rapid responses are needed in order to address victims' requests for help. Social media used around crises involves self-organizing behavior that can produce accurate results, often in advance of official communications. This allows affected population to send tweets or text messages, and hence, make them heard. The ability to classify tweets and text messages automatically, together with the ability to deliver the relevant information to the appropriate personnel are essential for enabling the personnel to timely and efficiently work to address the most urgent needs, and to understand the emergency situation better. In this study, we developed a reusable information technology infrastructure, called Enhanced Messaging for the Emergency Response Sector (EMERSE). The components of EMERSE are: (i) an iPhone application; (ii) a Twitter crawler component; (iii) machine translation; and (iv) automatic message classification. While each component is important in itself and deserves a detailed analysis, in this paper we focused on the automatic classification component, which classifies and aggregates tweets and text messages about the Haiti disaster relief so that they can be easily accessed by non-governmental organizations, relief workers, people in Haiti, and their friends and families.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Caragea et al_2011_Classifying Text Messages for the Haiti Earthquake.pdf}
}

@article{caragea2016,
  title = {Identifying {{Informative Messages}} in {{Disaster Events}} Using {{Convolutional Neural Networks}}},
  author = {Caragea, Cornelia and Silvescu, Adrian and Tapia, Andrea H},
  year = {2016},
  pages = {8},
  abstract = {Social media is a vital source of information during any major event, especially natural disasters. Data produced through social networking sites is seen as ubiquitous, rapid and accessible, and it is believed to empower average citizens to become more situationally aware during disasters and coordinate to help themselves. However, with the exponential increase in the volume of social media data, so comes the increase in data that are irrelevant to a disaster, thus, diminishing peoples' ability to find the information that they need in order to organize relief efforts, find help, and potentially save lives. In this paper, we present an approach to identifying informative messages in social media streams during disaster events. Our approach is based on Convolutional Neural Networks and shows significant improvement in performance over models that use the ``bag of words'' and n-grams as features on several datasets of messages from flooding events.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Caragea et al_2016_Identifying Informative Messages in Disaster Events using Convolutional Neural.pdf}
}

@misc{cardiffuniversity2009,
  title = {Website {{Yourplacenames}}.Com Also Known as {{People}}\dbend s {{Place Names}} - {{CONVERIS Research Information System}} by {{Thomson Reuters}}: - {{Converis Standard Config}}},
  author = {{Cardiff University}},
  year = {2009},
  howpublished = {https://research.cardiff.ac.uk/},
  file = {/home/cjber/drive/zotero/storage/4RTEECY9/12056105.html}
}

@article{carley2016,
  title = {Crowd Sourcing Disaster Management: {{The}} Complex Nature of {{Twitter}} Usage in {{Padang Indonesia}}},
  shorttitle = {Crowd Sourcing Disaster Management},
  author = {Carley, Kathleen M. and Malik, Momin and Landwehr, Peter M. and Pfeffer, J{\"u}rgen and Kowalchuck, Michael},
  year = {2016},
  month = dec,
  journal = {Safety Science},
  volume = {90},
  pages = {48--61},
  issn = {09257535},
  doi = {10/gc7q4j},
  langid = {english},
  file = {/home/cjber/drive/pdf/Carley et al_2016_Crowd sourcing disaster management.pdf}
}

@manual{carr2020,
  type = {Manual},
  title = {Hexbin: {{Hexagonal}} Binning Routines},
  author = {Carr, Dan and {Lewin-Koh}, ported by Nicholas and Maechler, Martin and Sarkar, contains copies of lattice functions written by Deepayan},
  year = {2020}
}

@article{carter2009,
  title = {Digital {{Britain}}},
  author = {Carter, Steven},
  year = {2009},
  journal = {London: HM Government, Department of Business Innovation \& Skills and Department for Culture, Media \& Sport},
  keywords = {\#nosource,⛔ No DOI found}
}

@book{castillo2016,
  title = {Big Crisis Data: Social Media in Disasters and Time-Critical Situations},
  author = {Castillo, Carlos},
  year = {2016},
  publisher = {{Cambridge University Press}},
  isbn = {1-316-69457-7},
  file = {/home/cjber/drive/pdf/Castillo_2016_Big crisis data.pdf}
}

@article{champion2004,
  title = {Patterns of {{Net Migration}} in {{England}} in the {{Context}} of the {{Urban}}\_{{Rural Definition}}, {{Census}} 2001},
  author = {Champion, AG},
  year = {2004},
  journal = {Rural Evidence Research Centre},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000002}
}

@article{chanta2014,
  title = {Improving Emergency Service in Rural Areas: A Bi-Objective Covering Location Model for {{EMS}} Systems},
  shorttitle = {Improving Emergency Service in Rural Areas},
  author = {Chanta, Sunarin and Mayorga, Maria E. and McLay, Laura A.},
  year = {2014},
  month = oct,
  journal = {Annals of Operations Research},
  volume = {221},
  number = {1},
  pages = {133--159},
  issn = {0254-5330, 1572-9338},
  doi = {10/chd5q3},
  abstract = {Emergency medical service (EMS) systems are public services that often provide the first line of response to urgent health care needs within a community. Unfortunately, it has been widely documented that large disparities in access to care exist between rural and urban communities. While rural EMS is provided through a variety of resources (e.g. air ambulances, volunteer corps, etc.), in this paper we focus on ground ambulatory care. In particular our goal is to balance the level of first-response ambulatory service provided to patients in urban and rural areas by locating ambulances at appropriate stations. In traditional covering location models the objective is to maximize demand that can be covered; consequently, these models favor locating ambulances in more densely populated areas, resulting in longer response times for patients in more rural areas. To address the issue of fairness in semi-rural/semi-urban communities, we propose three bi-objective covering location models that directly consider fairness via a secondary objective. Results are discussed and compared which provide a menu of alternatives to policy makers.},
  langid = {english},
  keywords = {ES},
  file = {/home/cjber/drive/pdf/Chanta et al_2014_Improving emergency service in rural areas.pdf}
}

@inproceedings{charaniya2004,
  title = {Supervised {{Parametric Classification}} of {{Aerial LiDAR Data}}},
  booktitle = {2004 {{Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshop}}},
  author = {Charaniya, A.P. and Manduchi, R. and Lodha, S.K.},
  year = {2004},
  pages = {30--30},
  publisher = {{IEEE}},
  address = {{Washington, DC, USA}},
  doi = {10/d2qv8m},
  langid = {english},
  annotation = {ZSCC: 0000166},
  file = {/home/cjber/drive/pdf/ENVS492/Charaniya et al_2004_.pdf}
}

@article{charlton2016,
  title = {Risk in Our Midst: {{Centrelines}}, Perceived Risk, and Speed Choice},
  shorttitle = {Risk in Our Midst},
  author = {Charlton, Samuel G. and Starkey, Nicola J.},
  year = {2016},
  month = oct,
  journal = {Accident Analysis \& Prevention},
  volume = {95},
  pages = {192--201},
  issn = {00014575},
  doi = {10/gf6qwm},
  abstract = {The idea that drivers' perceptions of risk affect their decisions and choices, particularly as regards their speed, is at the heart of many years of our education, engineering, and enforcement strategies to improve road safety. Our previous research has shown that horizontal curvature, road width, vertical curvature and separation from on-coming traffic are principal determinants to perceptions of risk on rural roads. The present study examined the relationship between drivers' perceptions of risk and the speeds they choose to drive. Participants drove high definition videos of familiar rural roads in a driving simulator and a smaller group of participants drove the same roads in a university fleet vehicle similar to the one used in the simulator. The results showed that double yellow and wide centreline markings were associated with lower speed choices and higher perceptions of risk, an effect magnified under high traffic conditions. Similarly, in both the simulator and on the roads, driving on narrow roads was associated with significantly lower speeds and increased risk ratings, while wider roads showed a small but significant increase in speeds as compared to standard width control roads. Finally, a range of other road and traffic conditions such as one-lane bridges, level crossings, police cars, and crash area warning signs were also found to be associated with lower speed choices and higher risk perceptions.},
  langid = {english},
  annotation = {ZSCC: 0000017},
  file = {/home/cjber/drive/pdf/ENVS492/Charlton_Starkey_2016_.pdf}
}

@article{charlton2017,
  title = {Driving on Urban Roads: {{How}} We Come to Expect the `Correct' Speed},
  shorttitle = {Driving on Urban Roads},
  author = {Charlton, Samuel G. and Starkey, Nicola J.},
  year = {2017},
  month = nov,
  journal = {Accident Analysis \& Prevention},
  volume = {108},
  pages = {251--260},
  issn = {00014575},
  doi = {10/gchj2j},
  abstract = {The subjective categories that drivers use to distinguish between different road types have been shown to influence the speeds they choose to drive but as yet we do not understand the road features that drivers use to make their discriminations. To better understand how drivers describe and categorise the roads they drive, 55 participants were recruited to drive a video of familiar urban roads in a driving simulator at the speed they would drive these roads in their own cars (using the accelerator and brake pedal in the driving simulator to adjust their speed). The participants were then asked to sort photos of the roads they had just driven into piles so that their driving would be the same on all roads in one pile but different to the other piles. Finally, they answered a series of questions about each road to indicate what speed they would drive, the safe speed for the road, their speed limit belief as well as providing ratings of comfort, difficulty and familiarity. Overall, drivers' categorisation of roads was informed by a number of factors including speed limit belief, road features and markings (including medians), road width, and presence of houses, driveways and footpaths. The participants' categories were congruent with what they thought the speed limits were, but not necessarily the actual speed limits. Mismatches between actual speed limits and speed limit beliefs appeared to result from category-level expectations about speed limits that took precedence over recent experience in the simulator. Roads that historically had a 50 km/h speed limit but had been reduced to 40 km/h were still regarded as 50 km/h roads by the participants, underscoring the point that simply posting a sign with a lower speed limit is not enough to overcome drivers' expectations and habits associated with the visual appearance of a road. The findings provided insights into how drivers view and categorise roads, and identify specific areas that could be used to improve speed limit credibility.},
  langid = {english},
  annotation = {ZSCC: 0000007},
  file = {/home/cjber/drive/pdf/ENVS492/Charlton_Starkey_2017_.pdf}
}

@article{charlton2018,
  title = {Using Road Markings as a Continuous Cue for Speed Choice},
  author = {Charlton, Samuel G. and Starkey, Nicola J. and Malhotra, Neha},
  year = {2018},
  month = aug,
  journal = {Accident Analysis \& Prevention},
  volume = {117},
  pages = {288--297},
  issn = {00014575},
  doi = {10/gdvrxj},
  abstract = {The potential for using road markings to indicate speed limits was investigated in a driving simulator over the course of two sessions. Two types of experimental road markings, an ``Attentional'' set designed to provide visually distinct cues to indicate speed limits of 60, 80 and 100 km/h, and a ``Perceptual'' set designed to also affect drivers' perception of speed, were compared to a standard undifferentiated set of markings. Participants (n = 20 per group) were assigned to one of four experimental groups (Attentional-Explicit, Attentional-Implicit, Perceptual-Explicit, Perceptual-Implicit) or a Control group (n = 22; standard road markings). The Explicit groups were instructed about the meaning of the road markings while those in the Implicit and Control groups did not receive any explanation. Participants drove five 10 km simulated roads containing three speed zones (60, 80 and 100 km/h) during the first session. The participants returned to the laboratory approximately 3 days later to drive five more trials including roads they had not seen before, a trial that included a secondary task, and a trial where speed signs were removed and only markings were present. The findings indicated that both types of road markings improved drivers' compliance with speed limits compared to the control group, but that explicit instruction as to the meaning of the markings was needed to realise their full benefit. Although previous research has indicated the benefit of road markings used as warnings to indicate speed reductions in advance of horizontal or vertical curves, the findings of the present experiment also suggest that systematically associating road markings with specific speed limits may be a useful way to improve speed limit compliance and increase speed homogeneity.},
  langid = {english},
  annotation = {ZSCC: 0000005},
  file = {/home/cjber/drive/pdf/ENVS492/Charlton et al_2018_.pdf}
}

@article{chatterton2000,
  title = {Bringing {{Britain Together}}?: {{The}} Limitations of Area-Based Regeneration Policies in Addressing Deprivation},
  shorttitle = {Bringing {{Britain Together}}?},
  author = {Chatterton, Paul and Bradley, David},
  year = {2000},
  month = jul,
  journal = {Local Economy: The Journal of the Local Economy Policy Unit},
  volume = {15},
  number = {2},
  pages = {98--111},
  issn = {0269-0942, 1470-9325},
  doi = {10/d4rgn4},
  langid = {english},
  annotation = {ZSCC: 0000098},
  file = {/home/cjber/drive/pdf/ENVS416/Chatterton_Bradley_2000_.pdf}
}

@article{chaudhry,
  title = {Rural and {{Urban Road Network Generalisation Deriving}} 1:250,000 from {{OS MasterMap}}},
  author = {Chaudhry, Omair and Mackaness, William},
  pages = {12},
  abstract = {Roads are essential component of topographic maps and spatial databases. The challenge in automated generalisation of road networks is to derive a connected network while maintaining the structure for the intended target scale and to achieve this with minimum user intervention. A lot of methods to select, displace and simplify roads have been presented; the focus here is on the generalisation of networks using visual perception techniques. This paper presents a framework based on visual perception that uses minimum attributes for generalisation of both `rural' and `urban' roads over large scale change. The system incorporated graph theoretic techniques to explicitly model the topology of the network as it was generalized. The model uses a fine scale map (1:1250 or 1:2500) as input and generates small scale (1:250,000) maps directly from it without creating intermediate small scale maps. The results compared favorably with paper maps (Ordnance Surveys Stratgie dataset (1:250,000)).},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000043},
  file = {/home/cjber/drive/pdf/ENVS492/Road Detection/chaudhry.pdf}
}

@article{chen2003,
  title = {(73) {{Assignee}}: {{International Business Machines Corporation}}, {{Armonk}}, {{NY}} ({{US}})},
  author = {Chen, Feng-wei and Cutlip, Robert R and Db, Ibm},
  year = {2003},
  pages = {26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Chen et al_2003_(73) Assignee.pdf}
}

@article{chen2014,
  title = {Mining {{Social Media Data}} for {{Understanding Students}}' {{Learning Experiences}}},
  author = {Chen, Xin and Vorvoreanu, Mihaela and Madhavan, Krishna},
  year = {2014},
  month = jul,
  journal = {IEEE Transactions on Learning Technologies},
  volume = {7},
  number = {3},
  pages = {246--259},
  issn = {1939-1382},
  doi = {10/f6swvp},
  abstract = {Students' informal conversations on social media (e.g., Twitter, Facebook) shed light into their educational experiences-opinions, feelings, and concerns about the learning process. Data from such uninstrumented environments can provide valuable knowledge to inform student learning. Analyzing such data, however, can be challenging. The complexity of students' experiences reflected from social media content requires human interpretation. However, the growing scale of data demands automatic data analysis techniques. In this paper, we developed a workflow to integrate both qualitative analysis and large-scale data mining techniques. We focused on engineering students' Twitter posts to understand issues and problems in their educational experiences. We first conducted a qualitative analysis on samples taken from about 25,000 tweets related to engineering students' college life. We found engineering students encounter problems such as heavy study load, lack of social engagement, and sleep deprivation. Based on these results, we implemented a multi-label classification algorithm to classify tweets reflecting students' problems. We then used the algorithm to train a detector of student problems from about 35,000 tweets streamed at the geo-location of Purdue University. This work, for the first time, presents a methodology and results that show how informal social media data can provide insights into students' experiences.},
  keywords = {Classification algorithms,computers and education,Cultural differences,Data mining,Education,Educational institutions,Engineering students,Media,social networking,Twitter,web text analysis},
  file = {/home/cjber/drive/pdf/Chen et al_2014_Mining Social Media Data for Understanding Students’ Learning Experiences.pdf;/home/cjber/drive/zotero/storage/UWJUU6T4/6697807.html}
}

@article{chen2018,
  title = {Georeferencing Places from Collective Human Descriptions Using Place Graphs},
  author = {Chen, Hao and Winter, Stephan and Vasardani, Maria},
  year = {2018},
  month = dec,
  journal = {Journal of Spatial Information Science},
  number = {17},
  pages = {31--62},
  issn = {1948-660X},
  doi = {10/ggwjtc},
  abstract = {Place descriptions in everyday communication or in online text provide a rich source of spatial knowledge about places. Such descriptions typically consist of references to places and spatial relationships between them. An important step to utilize such knowledge in information systems is georeferencing the referred places. Beside place name disambiguation, another challenge is that a significant proportion of place references in such descriptions are not official place names indexed by gazetteers, thus cannot be resolved easily. This paper presents a novel approach for georeferencing places from collective descriptions using place graphs, regardless of whether they are referred to by gazetteered names or not. The approach leverages spatial relation models for approximate locating and matching. Different models are proposed and evaluated using several metrics.},
  langid = {english},
  keywords = {Key Paper},
  file = {/home/cjber/drive/pdf/Chen et al_2018_Georeferencing places from collective human descriptions using place graphs.pdf}
}

@article{chen2018a,
  title = {A {{Graph Database Model}} for {{Knowledge Extracted}} from {{Place Descriptions}}},
  author = {Chen, Hao and Vasardani, Maria and Winter, Stephan and Tomko, Martin},
  year = {2018},
  month = jun,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {7},
  number = {6},
  pages = {221},
  issn = {2220-9964},
  doi = {10.3390/ijgi7060221},
  abstract = {Everyday place descriptions provide a rich source of knowledge about places and their relative locations. This research proposes a place graph model for modelling this spatial, non-spatial, and contextual knowledge from place descriptions. The model extends a prior place graph, and overcomes a number of limitations. The model is implemented using a graph database, and a management system has also been developed that allows operations including querying, mapping, and visualizing the stored knowledge in an extended place graph. Then three experimental tasks, namely georeferencing, reasoning, and querying, are selected to demonstrate the superiority of the extended model.},
  langid = {english},
  keywords = {Key Paper},
  file = {/home/cjber/drive/pdf/Chen et al_2018_A Graph Database Model for Knowledge Extracted from Place Descriptions.pdf}
}

@article{chen2021,
  title = {{{KE-CNN}}: {{A}} New Social Sensing Method for Extracting Geographical Attributes from Text Semantic Features and Its Application in {{Wuhan}}, {{China}}},
  shorttitle = {{{KE-CNN}}},
  author = {Chen, Nengcheng and Zhang, Yan and Du, Wenying and Li, Yingbing and Chen, Min and Zheng, Xiang},
  year = {2021},
  month = jul,
  journal = {Computers, Environment and Urban Systems},
  volume = {88},
  pages = {101629},
  issn = {01989715},
  doi = {10/gjn8qw},
  abstract = {Social sensing is an analytical method to study the interaction between human and space through extracting reliable information from massive volunteered information data. During the ongoing COVID-19 pandemic, there are a large number of Internet social sensing data. However, most of them lack geographic attribute. In order to resolve this problem, this paper proposes a convolutional neural network geographic classification model based on keyword extraction and synonym substitution (KE-CNN) which could determine the geographic attribute by extracting the semantic features from text data. Besides, we realizes the non-contact pandemic social sensing and construct the co-word complex network by capturing the spatiotemporal behaviour of a large number of people. Our research found that (1) mining co-word network can obtain most public opinion information of pandemic events, (2) KE-CNN model improves the accuracy by 5\%\textendash 15\% compared with the traditional machine learning method. Through this method, we could effectively establish medical, catering, railway station, education and other types of text feature set, supplement the missing spatial data tags, and achieve a good geographical seamless social sensing.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Chen et al_2021_KE-CNN.pdf}
}

@article{chi2016,
  title = {Geolocation {{Prediction}} in {{Twitter Using Location Indicative Words}} and {{Textual Features}}},
  author = {Chi, Lianhua and Lim, Kwan Hui and Alam, Nebula and Butler, Christopher J},
  year = {2016},
  pages = {8},
  abstract = {Knowing the location of a social media user and their posts is important for various purposes, such as the recommendation of location-based items/services, and locality detection of crisis/disasters. This paper describes our submission to the shared task ``Geolocation Prediction in Twitter'' of the 2nd Workshop on Noisy User-generated Text. In this shared task, we propose an algorithm to predict the location of Twitter users and tweets using a multinomial Naive Bayes classifier trained on Location Indicative Words and various textual features (such as city/country names, \#hashtags and @mentions). We compared our approach against various baselines based on Location Indicative Words, city/country names, \#hashtags and @mentions as individual feature sets, and experimental results show that our approach outperforms these baselines in terms of classification accuracy, mean and median error distance.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Chi et al_2016_Geolocation Prediction in Twitter Using Location Indicative Words and Textual.pdf;/home/cjber/drive/pdf/Chi et al_2016_Geolocation Prediction in Twitter Using Location Indicative Words and Textual2.pdf}
}

@article{clarke2006,
  title = {Young Driver Accidents in the {{UK}}: {{The}} Influence of Age, Experience, and Time of Day},
  shorttitle = {Young Driver Accidents in the {{UK}}},
  author = {Clarke, David D. and Ward, Patrick and Bartle, Craig and Truman, Wendy},
  year = {2006},
  month = sep,
  journal = {Accident Analysis \& Prevention},
  volume = {38},
  number = {5},
  pages = {871--878},
  issn = {00014575},
  doi = {10/fpzq6q},
  abstract = {Young drivers, especially males, have relatively more accidents than other drivers. Young driver accidents also have somewhat different characteristics to those of other drivers; they include single vehicle accidents involving loss of control; excess speed for conditions; accidents during darkness; accidents on single carriageway rural roads; and accidents while making cross-flow turns (i.e. turning right in the UK, equivalent to a left turn in the US and continental Europe).},
  langid = {english},
  annotation = {ZSCC: 0000202},
  file = {/home/cjber/drive/pdf/ENVS492/Road Assessment/clarke2006.pdf}
}

@article{clasper2018,
  title = {Exploring {{Vernacular Perceptions}} of {{Spatial Entities}}: {{Using Twitter Data}} and {{R}} for {{Delimiting Vague}}, {{Dinformal Neighbourhoods}} in {{Inner London}}, {{UK}}},
  shorttitle = {Exploring {{Vernacular Perceptions}} of {{Spatial Entities}}},
  author = {Clasper, Luke Thomas},
  year = {2018},
  journal = {GI\_Forum},
  volume = {1},
  pages = {316--335},
  issn = {2308-1708, 2308-1708},
  doi = {10.1553/giscience2018_01_s316},
  abstract = {The informal and unofficial nature of how citizens discuss and conceive geographical entities such as neighbourhoods has traditionally been difficult to capture. Ambient Geographic Information (AGI) from social media services offers researchers an opportunity to collect large amounts of geo-referenced information concerning vernacular geography. Twitter data was harvested and analysed in R statistical software in order demonstrate whether using geodata from social media is a feasible method for spatially defining vague, vernacular neighbourhoods in Inner London, UK. The results suggest that social media data can be a valuable source for capturing vernacular geography from which vernacular neighbourhoods could be delimited. The study also revealed factors which may have contributed to vernacular neighbourhood demarcation. Twitter data was seen to both mirror the physical form of the underlying topography and reflect the social character of the city's land use. This work builds upon previous attempts to investigate vernacular geography which used more traditional methods, such as sketch maps and interviews. It also examines how manual qualitative coding can improve data quality and demonstrates how R statistical software can be used to capture, analyse and present geospatial data.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Clasper_2018_Exploring Vernacular Perceptions of Spatial Entities.pdf}
}

@book{clef2007,
  title = {Evaluation of Multilingual and Multi-Modal Information Retrieval: 7th {{Workshop}} of the {{Cross-Language Evaluation Forum}}, {{CLEF}} 2006, {{Alicante}}, {{Spain}}, {{September}} 20-22, 2006: Revised Selected Papers},
  shorttitle = {Evaluation of Multilingual and Multi-Modal Information Retrieval},
  author = {{CLEF}},
  editor = {Peters, C.},
  year = {2007},
  series = {Lecture Notes in Computer Science},
  number = {4730},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  isbn = {978-3-540-74998-1},
  langid = {english},
  lccn = {Z667.5 .C75 2006},
  keywords = {Congresses,Cross-language information retrieval},
  annotation = {OCLC: ocn173071245},
  file = {/home/cjber/drive/pdf/CLEF_Peters_2007_Evaluation of multilingual and multi-modal information retrieval.pdf}
}

@book{cleophas2011,
  title = {Statistical {{Analysis}} of {{Clinical Data}} on a {{Pocket Calculator}}},
  author = {Cleophas, Ton J. and Zwinderman, Aeilko H.},
  year = {2011},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-007-1211-9},
  isbn = {978-94-007-1210-2 978-94-007-1211-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/Cleophas_Zwinderman_2011_Statistical Analysis of Clinical Data on a Pocket Calculator.pdf}
}

@book{cleophas2015,
  title = {Machine {{Learning}} in {{Medicine}} - a {{Complete Overview}}},
  author = {Cleophas, Ton J. and Zwinderman, Aeilko H.},
  year = {2015},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-15195-3},
  isbn = {978-3-319-15194-6 978-3-319-15195-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Cleophas_Zwinderman_2015_Machine Learning in Medicine - a Complete Overview.pdf}
}

@book{cleophas2016,
  title = {Clinical {{Data Analysis}} on a {{Pocket Calculator}}},
  author = {Cleophas, Ton J. and Zwinderman, Aeilko H.},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-27104-0},
  isbn = {978-3-319-27103-3 978-3-319-27104-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Cleophas_Zwinderman_2016_Clinical Data Analysis on a Pocket Calculator.pdf}
}

@article{clode2004,
  title = {The {{Automatic Extractin}} of {{Roads}} from {{LiDAR Data}}},
  author = {Clode, Simon and Kootsookos, Peter and Rottensteiner, Franz},
  year = {2004},
  pages = {7},
  abstract = {A method for the automatic detection of roads from airborne laser scanner data is presented. Traditionally, intensity information has not been used in feature extraction from LIDAR data because the data is too noisy. This article deals with using as much of the recorded laser information as possible thus both height and intensity are used. To extract roads from a LIDAR point cloud, a hierarchical classification technique is used to classify the LIDAR points progressively into road or non-road. Initially, an accurate digital terrain model (DTM) model is created by using successive morphological openings with different structural element sizes. Individual laser points are checked for both a valid intensity range and height difference from the subsequent DTM. A series of filters are then passed over the road candidate image to improve the accuracy of the classification. The success rate of road detection and the level of detail of the resulting road image both depend on the resolution of the laser scanner data and the types of roads expected to be found. The presence of road-like features within the survey area such as private roads and car parks is discussed and methods to remove this information are entertained. All algorithms used are described and applied to an example urban test site.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/cjber/drive/pdf/ENVS492/Clode et al_2004_.pdf}
}

@article{clode2007,
  title = {Detection and {{Vectorization}} of {{Roads}} from {{Lidar Data}}},
  author = {Clode, Simon and Rottensteiner, Franz and Kootsookos, Peter and Zelniker, Emanuel},
  year = {2007},
  month = may,
  journal = {Photogrammetric Engineering \& Remote Sensing},
  volume = {73},
  number = {5},
  pages = {517--535},
  issn = {00991112},
  doi = {10/gf33md},
  abstract = {A method for the automatic detection and vectorization of roads from lidar data is presented. To extract roads from a lidar point cloud, a hierarchical classification technique is used to classify the lidar points progressively into road and non-road points. During the classification process, both intensity and height values are initially used. Due to the homogeneous and consistent nature of roads, a local point density is introduced to finalize the classification. The resultant binary classification is then vectorized by convolving a complex-valued disk named the Phase Coded Disk (PCD) with the image to provide three separate pieces of information about the road. The centerline and width of the road are obtained from the resultant magnitude image while the direction is determined from the corresponding phase image, thus completing the vectorized road model. All algorithms used are described and applied to two urban test sites. Completeness values of 0.88 and 0.79 and correctness values of 0.67 and 0.80 were achieved for the classification phase of the process. The vectorization of the classified results yielded RMS values of 1.56 m and 1.66 m, completeness values of 0.84 and 0.81 and correctness values of 0.75 and 0.80 for two different data sets.},
  langid = {english},
  annotation = {ZSCC: 0000140},
  file = {/home/cjber/drive/pdf/ENVS492/LiDAR/Clode et al_2007_.pdf}
}

@article{cohn1997,
  title = {Qualitative {{Spatial Representation}} and {{Reasoning}} with the {{Region Connection Calculus}}},
  author = {Cohn, Anthony G. and Bennett, Brandon and Gooday, John and Gotts, Nicholas Mark},
  year = {1997},
  month = oct,
  journal = {GeoInformatica},
  volume = {1},
  number = {3},
  pages = {275},
  publisher = {{Springer Nature}},
  issn = {13846175},
  doi = {10/bff7mp},
  abstract = {This paper surveys the work of the qualitative spatial reasoning group at the University of Leeds. The group has developed a number of logical calculi for representing and reasoning with qualitative spatial relations over regions. We motivate the use of regions as the primary spatial entity and show how a rich language can be built up from surprisingly few primitives. This language can distinguish between convex and a variety of concave shapes and there is also an extension which handles regions with uncertain boundaries. We also present a variety of reasoning techniques, both for static and dynamic situations. A number of possible application areas are briefly mentioned.},
  keywords = {ENGLAND,LEEDS (England),LOGIC,QUALITATIVE reasoning,qualitative spatial reasoning,REASONING,shape,spatial logics,topology,UNIVERSITIES \& colleges,UNIVERSITY of Leeds,vague boundaries},
  file = {/home/cjber/drive/pdf/Cohn et al_1997_Qualitative Spatial Representation and Reasoning with the Region Connection.pdf}
}

@article{collins,
  title = {The {{Forward-Backward Algorithm}}},
  author = {Collins, Michael},
  pages = {4},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Collins_The Forward-Backward Algorithm.pdf}
}

@manual{cooley2020,
  type = {Manual},
  title = {Sfheaders: {{Converts}} between r Objects and Simple Feature Objects},
  author = {Cooley, David},
  year = {2020}
}

@article{corben2005,
  title = {Cost-Effective Measures to Improve Crash and Injury Risk at Rural Intersections.},
  author = {Corben, Bruce and Oxley, Jennifer and Koppel, Sjaanie and Johnston, Ian},
  year = {2005},
  pages = {10},
  abstract = {Crashes on rural roads are a major road safety problem, accounting for up to twothirds of deaths and serious injuries worldwide. Rural intersections, in particular, are dangerous locations, accounting for over 30 percent of these rural crashes. Most collisions at intersections occur in high-speed settings, at intersections that are uncontrolled or controlled by stop or give-way signs, and often on low-volume, single-carriageway roads. This paper provides an overview of a systematic review of international literature, identifies `best-practice' measures to reduce crash and injury risk at rural intersections and describes cutting-edge strategies and evaluation of infrastructure measures for managing safety at rural intersections. Cost-effective measures include: measures to reduce speed and speeding and the injury consequences, speed perception measures, roundabouts, traffic signals, gradeseparation, channelisation, signing to clarify priority, removal of sight distance obstructions, provision of medians, skid-resistant pavements and limited access from side roads and driveways. The paper highlights the need to address key crash problems at rural intersections by developing and implementing a system-wide and comprehensive approach that fundamentally improves the operation and design of these locations.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000004},
  file = {/home/cjber/drive/pdf/ENVS492/Corben et al_2005_.pdf}
}

@article{costea2016,
  title = {Aerial Image Geolocalization from Recognition and Matching of Roads and Intersections},
  author = {Costea, Dragos and Leordeanu, Marius},
  year = {2016},
  month = may,
  journal = {arXiv:1605.08323 [cs]},
  eprint = {1605.08323},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Aerial image analysis at a semantic level is important in many applications with strong potential impact in industry and consumer use, such as automated mapping, urban planning, real estate and environment monitoring, or disaster relief. The problem is enjoying a great interest in computer vision and remote sensing, due to increased computer power and improvement in automated image understanding algorithms. In this paper we address the task of automatic geolocalization of aerial images from recognition and matching of roads and intersections. Our proposed method is a novel contribution in the literature that could enable many applications of aerial image analysis when GPS data is not available. We offer a complete pipeline for geolocalization, from the detection of roads and intersections, to the identification of the enclosing geographic region by matching detected intersections to previously learned manually labeled ones, followed by accurate geometric alignment between the detected roads and the manually labeled maps. We test on a novel dataset with aerial images of two European cities and use the publicly available OpenStreetMap project for collecting ground truth roads annotations. We show in extensive experiments that our approach produces highly accurate localizations in the challenging case when we train on images from one city and test on the other and the quality of the aerial images is relatively poor. We also show that the the alignment between detected roads and pre-stored manual annotations can be effectively used for improving the quality of the road detection results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000019},
  file = {/home/cjber/drive/pdf/ENVS492/Costea_Leordeanu_2016_.pdf}
}

@article{crampton2013,
  title = {Beyond the Geotag: Situating `Big Data'and Leveraging the Potential of the Geoweb. {{Cartogr Geogr Inf Sci}} 40 (2): 130\textendash 139},
  author = {Crampton, JW and Graham, M and Poorthuis, A and Shelton, T and Stephens, M and Wilson, MW and Zook, M},
  year = {2013},
  keywords = {⛔ No DOI found}
}

@article{cranshaw2012,
  title = {The {{Livehoods Project}}: {{Utilizing Social Media}} to {{Understand}} the {{Dynamics}} of a {{City}}},
  author = {Cranshaw, Justin and Schwartz, Raz and Hong, Jason I and Sadeh, Norman},
  year = {2012},
  pages = {8},
  abstract = {Studying the social dynamics of a city on a large scale has traditionally been a challenging endeavor, often requiring long hours of observation and interviews, usually resulting in only a partial depiction of reality. To address this difficulty, we introduce a clustering model and research methodology for studying the structure and composition of a city on a large scale based on the social media its residents generate. We apply this new methodology to data from approximately 18 million check-ins collected from users of a location-based online social network. Unlike the boundaries of traditional municipal organizational units such as neighborhoods, which do not always reflect the character of life in these areas, our clusters, which we call Livehoods, are representations of the dynamic areas that comprise the city. We take a qualitative approach to validating these clusters, interviewing 27 residents of Pittsburgh, PA, to see how their perceptions of the city project onto our findings there. Our results provide strong support for the discovered clusters, showing how Livehoods reveal the distinctly characterized areas of the city and the forces that shape them.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Cranshaw et al_2012_The Livehoods Project.pdf}
}

@article{crawley2015,
  title = {Statistics. {{An Introduction}} Using {{R}}. 2nd Rd},
  author = {Crawley, MJ},
  year = {2015},
  publisher = {{Wiley}},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{currie2010,
  title = {Quantifying Spatial Gaps in Public Transport Supply Based on Social Needs},
  author = {Currie, Graham},
  year = {2010},
  month = jan,
  journal = {Journal of Transport Geography},
  volume = {18},
  number = {1},
  pages = {31--41},
  issn = {09666923},
  doi = {10/dmv2nd},
  abstract = {This paper concerns a research project to identify spatial gaps in public transport provision for people who are socially disadvantaged. The paper outlines the research context for measurement of public transport supply and needs, and then describes the methodology developed for an application in Melbourne, Australia. Results of the application are described including key findings on spatial gaps in services relative to social needs. The research identifies significant gaps between services supplied and social needs for transport services. Consistency of these findings with research in other Australian cities are noted. Implications for policy development are suggested.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS492/Currie_2010_.pdf}
}

@inproceedings{dahlkamp2006,
  title = {Self-Supervised {{Monocular Road Detection}} in {{Desert Terrain}}},
  booktitle = {Robotics: {{Science}} and {{Systems II}}},
  author = {Dahlkamp, H. and Kaehler, A. and Stavens, D. and Thrun, S. and Bradski, G.},
  year = {2006},
  month = aug,
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10/gf6d38},
  abstract = {We present a method for identifying drivable surfaces in difficult unpaved and offroad terrain conditions as encountered in the DARPA Grand Challenge robot race. Instead of relying on a static, pre-computed road appearance model, this method adjusts its model to changing environments. It achieves robustness by combining sensor information from a laser range finder, a pose estimation system and a color camera. Using the first two modalities, the system first identifies a nearby patch of drivable surface. Computer Vision then takes this patch and uses it to construct appearance models to find drivable surface outward into the far range. This information is put into a drivability map for the vehicle path planner. In addition to evaluating the method's performance using a scoring framework run on real-world data, the system was entered, and won, the 2005 DARPA Grand Challenge. Post-race log-file analysis proved that without the Computer Vision algorithm, the vehicle would not have driven fast enough to win.},
  isbn = {978-0-262-69348-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS492/Dahlkamp et al_2006_.pdf}
}

@article{dan,
  title = {From {{Spatial Relations}} to {{Spatial Configurations}}},
  author = {Dan, Soham and Kordjamshidi, Parisa and Bonn, Julia and Bhatia, Archna and Cai, Zheng and Palmer, Martha and Roth, Dan},
  pages = {10},
  abstract = {Spatial Reasoning from language is essential for natural language understanding. Supporting it requires a representation scheme that can capture spatial phenomena encountered in language as well as in images and videos. Existing spatial representations are not sufficient for describing spatial configurations used in complex tasks. This paper extends the capabilities of existing spatial representation languages and increases coverage of the semantic aspects that are needed to ground spatial meaning of natural language text in the world. Our spatial relation language is able to represent a large, comprehensive set of spatial concepts crucial for reasoning and is designed to support composition of static and dynamic spatial configurations. We integrate this language with the Abstract Meaning Representation (AMR) annotation schema and present a corpus annotated by this extended AMR. To exhibit the applicability of our representation scheme, we annotate text taken from diverse datasets and show how we extend the capabilities of existing spatial representation languages with fine-grained decomposition of semantics and blend it seamlessly with AMRs of sentences and discourse representations as a whole.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Dan et al_From Spatial Relations to Spatial Configurations.pdf}
}

@inproceedings{darmawati2008,
  title = {Utilization of Multiple Echo Information for Classification of Airborne Laser Scanning Data},
  author = {Darmawati, AT},
  year = {2008},
  publisher = {{ITC}},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000013}
}

@book{dathan2015,
  ids = {dathan2015a},
  title = {Object-{{Oriented Analysis}}, {{Design}} and {{Implementation}}},
  author = {Dathan, Brahma and Ramnath, Sarnath},
  year = {2015},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24280-4},
  isbn = {978-3-319-24278-1 978-3-319-24280-4},
  langid = {english},
  file = {/home/cjber/drive/pdf/Dathan_Ramnath_2015_Object-Oriented Analysis, Design and Implementation.pdf;/home/cjber/drive/zotero/storage/IW8952IW/Dathan_Ramnath_2015_Object-Oriented Analysis, Design and Implementation2.pdf}
}

@book{datta2017,
  title = {{{LaTeX}} in 24 {{Hours}}},
  author = {Datta, Dilip},
  year = {2017},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-47831-9},
  isbn = {978-3-319-47830-2 978-3-319-47831-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/Datta_2017_LaTeX in 24 Hours.pdf}
}

@article{davies2009,
  title = {User {{Needs}} and {{Implications}} for {{Modelling Vague Named Places}}},
  author = {Davies, Clare and Holt, Ian and Green, Jenny and Harding, Jenny and Diamond, Lucy},
  year = {2009},
  month = aug,
  journal = {Spatial Cognition \& Computation},
  volume = {9},
  number = {3},
  pages = {174--194},
  issn = {1387-5868, 1542-7633},
  doi = {10/dnjkbj},
  abstract = {Focusing on vague and vernacular aspects of place, results are reported from a qualitative empirical study of workplace end users of geographic information. The study revealed certain patterns of need for place and placename data; these are compared with current sources of place information (maps and gazetteers) for GIS, and with various proposed methods for modelling vagueness, uncertainty and multiple names and extents in spatial data. Conclusions centre on the need for multiple methods in specific use contexts, and for further requirements-driven research into the cognitive phenomenon of place itself.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Davies et al_2009_User Needs and Implications for Modelling Vague Named Places.pdf}
}

@book{deberg2008,
  title = {Computational {{Geometry}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Computational {{Geometry}}},
  author = {{de Berg}, Mark and Cheong, Otfried and {van Kreveld}, Marc and Overmars, Mark},
  year = {2008},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-77974-2},
  isbn = {978-3-540-77973-5 978-3-540-77974-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/de Berg et al_2008_Computational Geometry.pdf}
}

@article{debruijn2018,
  ids = {debruijn2018a},
  title = {{{TAGGS}}: {{Grouping Tweets}} to {{Improve Global Geoparsing}} for {{Disaster Response}}},
  shorttitle = {{{TAGGS}}},
  author = {{de Bruijn}, Jens A. and {de Moel}, Hans and Jongman, Brenden and Wagemaker, Jurjen and Aerts, Jeroen C. J. H.},
  year = {2018},
  month = jun,
  journal = {Journal of Geovisualization and Spatial Analysis},
  volume = {2},
  number = {1},
  pages = {2},
  issn = {2509-8810, 2509-8829},
  doi = {10/ggwjt3},
  abstract = {Timely and accurate information about ongoing events are crucial for relief organizations seeking to effectively respond to disasters. Recently, social media platforms, especially Twitter, have gained traction as a novel source of information on disaster events. Unfortunately, geographical information is rarely attached to tweets, which hinders the use of Twitter for geographical applications. As a solution, geoparsing algorithms extract and can locate geographical locations referenced in a tweet's text. This paper describes TAGGS, a new algorithm that enhances location disambiguation by employing both metadata and the contextual spatial information of groups of tweets referencing the same location regarding a specific disaster type. Validation demonstrated that TAGGS approximately attains a recall of 0.82 and precision of 0.91. Without lowering precision, this roughly doubles the number of correctly found administrative subdivisions and cities, towns, and villages as compared to individual geoparsing. We applied TAGGS to 55.1 million flood-related tweets in 12 languages, collected over 3 years. We found 19.2 million tweets mentioning one or more flood locations, which can be towns (11.2 million), administrative subdivisions (5.1 million), or countries (4.6 million). In the future, TAGGS could form the basis for a global event detection system.},
  langid = {english},
  file = {/home/cjber/drive/pdf/de Bruijn et al_2018_TAGGS.pdf;/home/cjber/drive/pdf/de Bruijn et al_2018_TAGGS2.pdf}
}

@article{debruijn2020,
  title = {Improving the Classification of Flood Tweets with Contextual Hydrological Information in a Multimodal Neural Network},
  author = {{de Bruijn}, Jens A. and {de Moel}, Hans and Weerts, Albrecht H. and {de Ruiter}, Marleen C. and Basar, Erkan and Eilander, Dirk and Aerts, Jeroen C.J.H.},
  year = {2020},
  month = jul,
  journal = {Computers \& Geosciences},
  volume = {140},
  pages = {104485},
  issn = {00983004},
  doi = {10/gk8gzg},
  abstract = {While text classification can classify tweets, assessing whether a tweet is related to an ongoing flood event or not, based on its text, remains difficult. Inclusion of contextual hydrological information could improve the perfor\- mance of such algorithms. Here, a multilingual multimodal neural network is designed that can effectively use both textual and hydrological information. The classification data was obtained from Twitter using flood-related keywords in English, French, Spanish and Indonesian. Subsequently, hydrological information was extracted from a global precipitation dataset based on the tweet's timestamp and locations mentioned in its text. Three experiments were performed analyzing precision, recall and F1-scores while comparing a neural network that uses hydrological information against a neural network that does not. Results showed that F1-scores improved significantly across all experiments. Most notably, when optimizing for precision the neural network with hy\- drological information could achieve a precision of 0.91 while the neural network without hydrological infor\- mation failed to effectively optimize. Moreover, this study shows that including hydrological information can assist in the translation of the classification algorithm to unseen languages.},
  langid = {english},
  file = {/home/cjber/drive/pdf/de Bruijn et al_2020_Improving the classification of flood tweets with contextual hydrological.pdf}
}

@article{dedrick2009,
  title = {Multilevel {{Modeling}}: {{A Review}} of {{Methodological Issues}} and {{Applications}}},
  shorttitle = {Multilevel {{Modeling}}},
  author = {Dedrick, Robert F. and Ferron, John M. and Hess, Melinda R. and Hogarty, Kristine Y. and Kromrey, Jeffrey D. and Lang, Thomas R. and Niles, John D. and Lee, Reginald S.},
  year = {2009},
  month = mar,
  journal = {Review of Educational Research},
  volume = {79},
  number = {1},
  pages = {69--102},
  issn = {0034-6543, 1935-1046},
  doi = {10/db8js2},
  abstract = {This study analyzed the reporting of multilevel modeling applications of a sample of 99 articles from 13 peer-reviewed journals in education and the social sciences. A checklist, derived from the methodological literature on multilevel modeling and focusing on the issues of model development and specification, data considerations, estimation, and inference, was used to analyze the articles. The most common applications were two-level models where individuals were nested within contexts. Most studies were nonexperimental and used nonprobability samples. The amount of data at each level varied widely across studies, as did the number of models examined. Analyses of reporting practices indicated some clear problems, with many articles not reporting enough information for a reader to critique the reported analyses. For example, in many articles, one could not determine how many models were estimated, what covariance structure was assumed, what type of centering if any was used, whether the data were consistent with assumptions, whether outliers were present, or how the models were estimated. Guidelines for researchers reporting multilevel analyses are provided.},
  langid = {english},
  annotation = {ZSCC: 0000160},
  file = {/home/cjber/drive/pdf/ENVS416/Dedrick et al_2009_.pdf}
}

@article{deeter2009,
  title = {Real-Time Traveler Information Systems. {{NCHRP}} Report 399},
  author = {Deeter, D},
  year = {2009},
  journal = {Transport Research Board, USA},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000004}
}

@article{deisenroth,
  ids = {deisenrotha},
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  pages = {417},
  doi = {10/ggwjvc},
  langid = {english},
  file = {/home/cjber/drive/pdf/Deisenroth et al_Mathematics for Machine Learning.pdf;/home/cjber/drive/pdf/Deisenroth et al_Mathematics for Machine Learning2.pdf;/home/cjber/drive/pdf/Deisenroth et al_Mathematics for Machine Learning3.pdf;/home/cjber/drive/pdf/Deisenroth et al_Mathematics for Machine Learning4.pdf}
}

@book{dekking2005,
  title = {A Modern Introduction to Probability and Statistics: Understanding Why and How},
  shorttitle = {A Modern Introduction to Probability and Statistics},
  editor = {Dekking, Michel},
  year = {2005},
  series = {Springer Texts in Statistics},
  publisher = {{Springer}},
  address = {{London}},
  isbn = {978-1-85233-896-1},
  langid = {english},
  lccn = {QA273 .M645 2005},
  keywords = {Mathematical statistics,Probabilities,Textbooks},
  file = {/home/cjber/drive/pdf/Dekking_2005_A modern introduction to probability and statistics.pdf}
}

@article{delboni2007,
  title = {Semantic {{Expansion}} of {{Geographic Web Queries Based}} on {{Natural Language Positioning Expressions}}},
  author = {Delboni, Tiago M and Borges, Karla A V and Laender, Alberto H F and Davis, Clodoveu A},
  year = {2007},
  month = jun,
  journal = {Transactions in GIS},
  volume = {11},
  number = {3},
  pages = {377--397},
  issn = {1361-1682, 1467-9671},
  doi = {10/d3cbkq},
  abstract = {The need for better Web search tools is getting increasing attention nowadays. About 20\% of the queries currently submitted to search engines include geographic references. Thus, it is particularly important to work with the semantics of such queries, both by understanding the terminology and by recognizing geographic references in natural language text. In this paper, we explore the use of natural language expressions, which we call positioning expressions, to perform geographic searches on the Web, without resorting to geocoded data or gazetteers. Such positioning expressions denote the location of a subject of interest with respect to a landmark. Our approach leads to a query expansion technique that can be explored by virtually any keyword-based search engine. Results obtained in our experiments show an expressive improvement over the traditional keyword-based search and a potential path for tackling many kinds of common geographic queries.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Delboni et al_2007_Semantic Expansion of Geographic Web Queries Based on Natural Language.pdf}
}

@article{delozier2015,
  title = {Gazetteer-{{Independent Toponym Resolution Using Geographic Word Profiles}}},
  author = {DeLozier, Grant and Baldridge, Jason and London, Loretta},
  year = {2015},
  pages = {7},
  abstract = {Toponym resolution, or grounding names of places to their actual locations, is an important problem in analysis of both historical corpora and present-day news and web content. Recent approaches have shifted from rule-based spatial minimization methods to machine learned classifiers that use features of the text surrounding a toponym. Such methods have been shown to be highly effective, but they crucially rely on gazetteers and are unable to handle unknown place names or locations. We address this limitation by modeling the geographic distributions of words over the earth's surface: we calculate the geographic profile of each word based on local spatial statistics over a set of geo-referenced language models. These geo-profiles can be further refined by combining in-domain data with background statistics from Wikipedia. Our resolver computes the overlap of all geo-profiles in a given text span; without using a gazetteer, it performs on par with existing classifiers. When combined with a gazetteer, it achieves state-of-the-art performance for two standard toponym resolution corpora (TR-CoNLL and Civil War). Furthermore, it dramatically improves recall when toponyms are identified by named entity recognizers, which often (correctly) find non-standard variants of toponyms.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/zotero/storage/92Q8INF9/DeLozier et al. - Gazetteer-Independent Toponym Resolution Using Geo.pdf}
}

@book{departmentforcommunitiesandlocalgovernment2017,
  title = {National Evaluation of the {{Troubled Families Programme}} 2015-2020: Progress so Far.},
  shorttitle = {National Evaluation of the {{Troubled Families Programme}} 2015-2020},
  author = {{Department for Communities and Local Government}},
  year = {2017},
  isbn = {978-1-4741-4075-1},
  langid = {english},
  annotation = {OCLC: 1018002181},
  file = {/home/cjber/drive/zotero/storage/IA4JPQ2S/Department for Communities and Local Government_2017_National evaluation of the Troubled Families Programme 2015-22.pdf}
}

@book{departmentforcommunitiesandlocalgovernment2017a,
  title = {National Evaluation of the {{Troubled Families Programme}} 2015-2020: Family Outcomes -National and Local Datasets: Part 1.},
  shorttitle = {National Evaluation of the {{Troubled Families Programme}} 2015-2020},
  author = {{Department for Communities and Local Government}},
  year = {2017},
  isbn = {978-1-4741-4077-5},
  langid = {english},
  annotation = {OCLC: 1018011783},
  file = {/home/cjber/drive/pdf/Department for Communities and Local Government_2017_National evaluation of the Troubled Families Programme 2015-2020.pdf}
}

@misc{departmentfortransport2006,
  title = {Speed {{Assessment Framework}}},
  author = {{Department for Transport}},
  year = {2006},
  howpublished = {http://www2.dft.gov.uk/},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2006_.pdf}
}

@article{departmentfortransport2011,
  title = {Strategic Framework for Road Safety},
  author = {{Department for Transport}},
  year = {2011},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000006}
}

@article{departmentfortransport2012,
  title = {Guidance on Road Classification and the Primary Route Network},
  author = {{Department for Transport}},
  year = {2012},
  pages = {26},
  langid = {english},
  annotation = {ZSCC: 0000005},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2012_.pdf}
}

@misc{departmentfortransport2012a,
  title = {Reported {{Road Casualties Great Britain}} 2011: {{Annual Report}}},
  author = {{Department for Transport}},
  year = {2012},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2012_2.pdf}
}

@article{departmentfortransport2013,
  title = {Revision of the {{Speed Limit Circular}} - {{Summary}} of {{Consultation Responses}}},
  author = {{Department for Transport}},
  year = {2013},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2013_.pdf}
}

@article{departmentfortransport2013a,
  title = {Setting Local Speed Limits},
  author = {{Department for Transport}},
  year = {2013},
  pages = {42},
  langid = {english},
  annotation = {ZSCC: 0000004[s1]},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2013_3.pdf}
}

@article{departmentfortransport2013b,
  title = {The {{Speed Limit Appraisal Tool}}: {{User Guidance}}},
  author = {{Department for Transport}},
  year = {2013},
  pages = {93},
  langid = {english},
  annotation = {ZSCC: 0000000[s1]},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2013_2.pdf}
}

@misc{departmentfortransport2016,
  title = {Overall Measure of Accessibility of Services},
  author = {{Department for Transport}},
  year = {2016},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2016_.pdf}
}

@misc{departmentfortransport2018,
  title = {Journey {{Time Statistics}}},
  author = {{Department for Transport}},
  year = {2018},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2018_2.pdf}
}

@article{departmentfortransport2018a,
  title = {Road {{Safety Management Capacity Review}}},
  author = {{Department for Transport}},
  year = {2018},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2018_.pdf}
}

@article{departmentfortransport2018b,
  title = {Road {{Conditions}} in {{England}} 2017},
  author = {{Department for Transport}},
  year = {2018},
  pages = {8},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Department for Transport_2018_2.pdf}
}

@misc{departmentfortransport2019,
  title = {Road Traffic Statistics - {{Summary}} Statistics},
  author = {{Department for Transport}},
  year = {2019},
  howpublished = {https://roadtraffic.dft.gov.uk/summary},
  langid = {english},
  annotation = {ZSCC: 0000000[s1]},
  file = {/home/cjber/drive/zotero/storage/82CFS3LX/summary.html}
}

@inproceedings{derczynski2017,
  title = {Results of the {{WNUT2017 Shared Task}} on {{Novel}} and {{Emerging Entity Recognition}}},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Noisy User-generated Text}}},
  author = {Derczynski, Leon and Nichols, Eric and {van Erp}, Marieke and Limsopatham, Nut},
  year = {2017},
  pages = {140--147},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10/ggwb5p},
  abstract = {This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet ``so.. kktny in 30 mins?!'' \textendash{} even human experts find the entity kktny hard to detect and resolve. The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities. The task as described in this paper evaluated the ability of participating entries to detect and classify novel and emerging named entities in noisy text.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Derczynski et al_2017_Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition.pdf}
}

@inproceedings{derczynski2017a,
  title = {Results of the {{WNUT2017 Shared Task}} on {{Novel}} and {{Emerging Entity Recognition}}},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Noisy User-generated Text}}},
  author = {Derczynski, Leon and Nichols, Eric and {van Erp}, Marieke and Limsopatham, Nut},
  year = {2017},
  pages = {140--147},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/W17-4418},
  abstract = {This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet ``so.. kktny in 30 mins?!'' \textendash{} even human experts find the entity kktny hard to detect and resolve. The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities. The task as described in this paper evaluated the ability of participating entries to detect and classify novel and emerging named entities in noisy text.},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/UCEX38KD/Derczynski et al. - 2017 - Results of the WNUT2017 Shared Task on Novel and E.pdf}
}

@article{derungs2016,
  title = {Mining Nearness Relations from an N-Grams {{Web}} Corpus in Geographical Space},
  author = {Derungs, Curdin and Purves, Ross S.},
  year = {2016},
  month = oct,
  journal = {Spatial Cognition \& Computation},
  volume = {16},
  number = {4},
  pages = {301--322},
  issn = {1387-5868, 1542-7633},
  doi = {10/ggwjtm},
  abstract = {Interacting with spatial data effectively requires systems that not only process references to locations, but understand spatial natural language. Empirical research has demonstrated that near is vague, asymmetric and context dependent. We explore near in language using Microsoft Web n-grams for expressions of the form A near *, where A are placenames referring to different spatial granularities, ranging from points of interest to large U.S. cities and * are autocomplete suggestions for placenames. Analyzing the extracted expressions requires consideration of semantic and referent ambiguity. With more than 200,000 expressions we show not only what is considered to be near at different scales, but also produce intuitive maps of nearness for different locations.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Derungs_Purves_2016_Mining nearness relations from an n-grams Web corpus in geographical space.pdf}
}

@article{devlin2019,
  ids = {devlin2019a},
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {arXiv:1810.04805 [cs]},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Devlin et al_2019_BERT.pdf;/home/cjber/drive/pdf/Devlin et al_2019_BERT2.pdf;/home/cjber/drive/zotero/storage/XFVJWZDY/1810.html}
}

@article{dewaard2004,
  title = {How Much Visual Road Information Is Needed to Drive Safely and Comfortably?},
  author = {{de Waard}, Dick and Steyvers, Frank J.J.M and Brookhuis, Karel A},
  year = {2004},
  month = aug,
  journal = {Safety Science},
  volume = {42},
  number = {7},
  pages = {639--655},
  issn = {09257535},
  doi = {10/fnj3tm},
  abstract = {The questions ``how much visual information from the road is required for proper driving?'', and ``how do people cope with a visually ambiguous road configuration?'', were explored in an advanced driving simulator.},
  langid = {english},
  annotation = {ZSCC: 0000048},
  file = {/home/cjber/drive/pdf/ENVS492/de Waard et al_2004_.pdf}
}

@article{diezroux2001,
  title = {Investigating {{Neighborhood}} and {{Area Effects}} on {{Health}}},
  author = {Diez Roux, Ana V.},
  year = {2001},
  month = nov,
  journal = {American Journal of Public Health},
  volume = {91},
  number = {11},
  pages = {1783--1789},
  issn = {0090-0036, 1541-0048},
  doi = {10/dd6xtn},
  langid = {english},
  annotation = {ZSCC: 0001616},
  file = {/home/cjber/drive/pdf/ENVS416/Diez Roux_2001_.pdf}
}

@article{dignazio2014,
  ids = {dignazio},
  title = {{{CLIFF-CLAVIN}}: {{Determining Geographic Focus}} for {{News Articles}}},
  author = {D'Ignazio, Catherine and Bhargava, Rahul and Zuckerman, Ethan},
  year = {2014},
  pages = {5},
  abstract = {The growing diversity of news sources available online has led to a significant methodological change in field of global news coverage. Studies of media attention and framing require sophisticated analytic tools to permit analysis of a large volume of content consumed by a broad readership. Geographic focus continues to be a topic of interest to media organizations, media analysts, and media consumers. Detecting and recognizing geographic locations (toponyms) in news media is a well-established field with many commercial and open source tools available. An evaluation is performed of various existing tools to compare their accuracy and appropriateness for use within media organizations and for media analysis. The concept of focus, indicating the location an article is primarily about, is extended into the news realm and added to an existing tool to increase relevance for the aforementioned applications. Potential applications as well as initial experiments using geoparsing for news organizations are discussed, in addition to ideas for future work building on these tools.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/D’Ignazio et al_2014_CLIFF-CLAVIN.pdf;/home/cjber/drive/pdf/D’Ignazio et al_2014_CLIFF-CLAVIN2.pdf}
}

@book{dineen2014,
  title = {Multivariate {{Calculus}} and {{Geometry}}},
  author = {Dineen, Se{\'a}n},
  year = {2014},
  series = {Springer {{Undergraduate Mathematics Series}}},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-6419-7},
  isbn = {978-1-4471-6418-0 978-1-4471-6419-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Dineen_2014_Multivariate Calculus and Geometry.pdf}
}

@article{dong2019,
  title = {Inferring Neighbourhood Quality with Property Transaction Records by Using a Locally Adaptive Spatial Multi-Level Model},
  author = {Dong, Guanpeng and Wolf, Levi and Alexiou, Alekos and {Arribas-Bel}, Dani},
  year = {2019},
  journal = {Computers, Environment and Urban Systems},
  volume = {73},
  pages = {118--125},
  issn = {0198-9715},
  doi = {10/gf33r2},
  annotation = {ZSCC: 0000003},
  file = {/home/cjber/drive/pdf/ENVS453/Dong et al_2019_.pdf}
}

@article{dorling2000,
  title = {The Ghost of {{Christmas}} Past: Health Effects of Poverty in {{London}} in 1896 and 1991},
  author = {Dorling, Danny and Mitchell, Richard and Shaw, Mary and Orford, Scott and Smith, George Davey},
  year = {2000},
  journal = {Bmj},
  volume = {321},
  number = {7276},
  pages = {1547--1551},
  issn = {0959-8138},
  doi = {10/b7bv79},
  keywords = {\#nosource}
}

@article{dorling2002,
  title = {Geographies of the Agenda: Public Policy, the Discipline and Its (Re)`Turns'},
  shorttitle = {Geographies of the Agenda},
  author = {Dorling, Danny and Shaw, Mary},
  year = {2002},
  month = oct,
  journal = {Progress in Human Geography},
  volume = {26},
  number = {5},
  pages = {629--641},
  issn = {0309-1325, 1477-0288},
  doi = {10/cpd5s5},
  abstract = {In the 1980s and 1990s, poverty and inequality in Britain increased, yet the discipline of (human) geography was apparently disinterested. This paper poses the question as to why part of the discipline turned its back on public policy and particularly issues of poverty and inequality. The aim of the paper is to encourage students and advocates of geography to think a little about what they are involved in (and to think about the role of academia more generally). Recent publications in a number of geography journals have revealed much angst among prominent geographers concerning the state of human geography and, in particular, its links to contemporary policy debate. However, while geographers discuss the debate, we argue that they are not a significant part of it. We take a critical turn and look at the debate that two geographers \textendash{} Ron Martin and Doreen Massey \textendash{} have raised within the light of wider debates on public policy, politics, quantification, academia and the policy agenda. We conclude that for many reasons there is unlikely to be a large shift towards policy-orientated research within human geography.},
  langid = {english},
  annotation = {ZSCC: 0000177},
  file = {/home/cjber/drive/pdf/ENVS416/Dorling_Shaw_2002_.pdf}
}

@inproceedings{dou2012,
  title = {{{LeadLine}}: {{Interactive}} Visual Analysis of Text Data through Event Identification and Exploration},
  shorttitle = {{{LeadLine}}},
  booktitle = {2012 {{IEEE Conference}} on {{Visual Analytics Science}} and {{Technology}} ({{VAST}})},
  author = {Dou, Wenwen and Wang, Xiaoyu and Skau, Drew and Ribarsky, William and Zhou, Michelle X.},
  year = {2012},
  month = oct,
  pages = {93--102},
  doi = {10/ggd95t},
  abstract = {Text data such as online news and microblogs bear valuable insights regarding important events and responses to such events. Events are inherently temporal, evolving over time. Existing visual text analysis systems have provided temporal views of changes based on topical themes extracted from text data. But few have associated topical themes with events that cause the changes. In this paper, we propose an interactive visual analytics system, LeadLine, to automatically identify meaningful events in news and social media data and support exploration of the events. To characterize events, LeadLine integrates topic modeling, event detection, and named entity recognition techniques to automatically extract information regarding the investigative 4 Ws: who, what, when, and where for each event. To further support analysis of the text corpora through events, LeadLine allows users to interactively examine meaningful events using the 4 Ws to develop an understanding of how and why. Through representing large-scale text corpora in the form of meaningful events, LeadLine provides a concise summary of the corpora. LeadLine also supports the construction of simple narratives through the exploration of events. To demonstrate the efficacy of LeadLine in identifying events and supporting exploration, two case studies were conducted using news and social media data.},
  keywords = {Crawlers,Data mining,Event detection,Lead,Time series analysis,Twitter,Visualization},
  file = {/home/cjber/drive/pdf/Dou et al_2012_LeadLine.pdf;/home/cjber/drive/zotero/storage/TB84RDXX/6400485.html}
}

@book{douglas1964,
  title = {The Home and the School},
  author = {Douglas, James William Bruce},
  year = {1964},
  publisher = {{MacGibbon \& Kee London}},
  isbn = {0-261-61642-0},
  keywords = {\#nosource},
  annotation = {ZSCC: 0001845}
}

@article{dragut2014,
  title = {Automated Parameterisation for Multi-Scale Image Segmentation on Multiple Layers},
  author = {Dr{\u a}gu{\c t}, L. and Csillik, O. and Eisank, C. and Tiede, D.},
  year = {2014},
  month = feb,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {88},
  pages = {119--127},
  issn = {09242716},
  doi = {10/f3spfj},
  abstract = {We introduce a new automated approach to parameterising multi-scale image segmentation of multiple layers, and we implemented it as a generic tool for the eCognition\`O software. This approach relies on the potential of the local variance (LV) to detect scale transitions in geospatial data. The tool detects the number of layers added to a project and segments them iteratively with a multiresolution segmentation algorithm in a bottom-up approach, where the scale factor in the segmentation, namely, the scale parameter (SP), increases with a constant increment. The average LV value of the objects in all of the layers is computed and serves as a condition for stopping the iterations: when a scale level records an LV value that is equal to or lower than the previous value, the iteration ends, and the objects segmented in the previous level are retained. Three orders of magnitude of SP lags produce a corresponding number of scale levels. Tests on very high resolution imagery provided satisfactory results for generic applicability. The tool has a significant potential for enabling objectivity and automation of GEOBIA analysis.},
  langid = {english},
  annotation = {ZSCC: 0000330},
  file = {/home/cjber/drive/pdf/ENVS492/Drăguţ et al_2014_.pdf}
}

@article{dror2018,
  title = {Appendix - {{Recommended Statistical Significance Tests}} for {{NLP Tasks}}},
  author = {Dror, Rotem and Reichart, Roi},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.01448 [cs]},
  eprint = {1809.01448},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Statistical significance testing plays an important role when drawing conclusions from experimental results in NLP papers. Particularly, it is a valuable tool when one would like to establish the superiority of one algorithm over another. This appendix complements the guide for testing statistical significance in NLP presented in [5] by proposing valid statistical tests for the common tasks and evaluation measures in the field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Dror_Reichart_2018_Appendix - Recommended Statistical Significance Tests for NLP Tasks.pdf}
}

@inproceedings{dror2018a,
  title = {The {{Hitchhiker}}'s {{Guide}} to {{Testing Statistical Significance}} in {{Natural Language Processing}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},
  year = {2018},
  pages = {1383--1392},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1128},
  abstract = {Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied in NLP research in a statistically sound manner1.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Dror et al_2018_The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language.pdf}
}

@inproceedings{dsouza2015,
  title = {{{UTD}}: {{Ensemble-Based Spatial Relation Extraction}}},
  shorttitle = {{{UTD}}},
  booktitle = {Proceedings of the 9th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval}} 2015)},
  author = {D'Souza, Jennifer and Ng, Vincent},
  year = {2015},
  month = jun,
  pages = {862--869},
  publisher = {{Association for Computational Linguistics}},
  address = {{Denver, Colorado}},
  doi = {10/gg6d3k},
  file = {/home/cjber/drive/pdf/D'Souza_Ng_2015_UTD.pdf}
}

@article{du2017,
  title = {Classifying Natural-Language Spatial Relation Terms with Random Forest Algorithm},
  author = {Du, Shihong and Wang, Xiaonan and Feng, Chen-Chieh and Zhang, Xiuyuan},
  year = {2017},
  month = mar,
  journal = {International Journal of Geographical Information Science},
  volume = {31},
  number = {3},
  pages = {542--568},
  issn = {1365-8816, 1362-3087},
  doi = {10/gg7wbw},
  abstract = {The exponential growth of natural language text data in social media has contributed a rich data source for geographic information. However, incorporating such data source for GIS analysis faces tremendous challenges as existing GIS data tend to be geometry based while natural language text data tend to rely on natural language spatial relation (NLSR) terms. To alleviate this problem, one critical step is to translate geometric configurations into NLSR terms, but existing methods to date (e.g. mean value or decision tree algorithm) are insufficient to obtain a precise translation. This study addresses this issue by adopting the random forest (RF) algorithm to automatically learn a robust mapping model from a large number of samples and to evaluate the importance of each variable for each NLSR term. Because the semantic similarity of the collected terms reduces the classification accuracy, different grouping schemes of NLSR terms are used, with their influences on classification results being evaluated. The experiment results demonstrate that the learned model can accurately transform geometric configurations into NLSR terms, and that recognizing different groups of terms require different sets of variables. More importantly, the results of variable importance evaluation indicate that the importance of topology types determined by the 9-intersection model is weaker than metric variables in defining NLSR terms, which contrasts to the assertion of `topology matters, metric refines' in existing studies.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Du et al_2017_Classifying natural-language spatial relation terms with random forest algorithm.pdf}
}

@article{dubes1992,
  title = {Performance Evaluation for Four Classes of Textural Features},
  author = {Dubes, R and Ohanian, P},
  year = {1992},
  journal = {Pattern Recognition},
  volume = {25},
  pages = {819--833},
  doi = {10/cx7ktb},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000550}
}

@book{duckham2004,
  title = {Foundations of {{Geographic Information Science}}.},
  author = {Duckham, Matt and Worboys, Michael},
  year = {2004},
  abstract = {Geographic Information Science. The Nature and Value of Geographic Information. Communicating Geographic Information in Context. Pragmatic Information Content. Representational Commitment in Maps. Granularity in Change over Time. A Theory of Granular Partitions. On the Ontological Status of Geographical Boundaries. Regions in Geography. Neighborhoods and Landmarks. Geographical Terminology Servers. Placing Cultural Events and Documents. Geographic Activity Models.},
  isbn = {978-0-203-00954-3},
  langid = {english},
  annotation = {OCLC: 1058404323},
  file = {/home/cjber/drive/pdf/Duckham_Worboys_2004_Foundations of Geographic Information Science.pdf}
}

@article{duncan1999,
  title = {Smoking and Deprivation: Are There Neighbourhood Effects?},
  shorttitle = {Smoking and Deprivation},
  author = {Duncan, Craig and Jones, Kelvyn and Moon, Graham},
  year = {1999},
  month = feb,
  journal = {Social Science \& Medicine},
  volume = {48},
  number = {4},
  pages = {497--505},
  issn = {02779536},
  doi = {10/d9p7ks},
  langid = {english},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000352}
}

@article{duncan2001,
  title = {Neighborhoods and Adolescent Development: {{How}} Can We Determine the Links},
  author = {Duncan, Greg J and Raudenbush, Stephen W},
  year = {2001},
  journal = {Does it take a village},
  pages = {105--136},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000219}
}

@book{duncan2011,
  title = {Statistical {{Confidentiality}}: {{Principles}} and {{Practice}}},
  shorttitle = {Statistical {{Confidentiality}}},
  author = {Duncan, George T. and Elliot, Mark and {Salazar-Gonz{\'a}lez}, Juan-Jos{\'e}},
  year = {2011},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-7802-8},
  isbn = {978-1-4419-7801-1 978-1-4419-7802-8},
  langid = {english},
  file = {/home/cjber/drive/pdf/Duncan et al_2011_Statistical Confidentiality.pdf}
}

@article{dunkel2019,
  title = {A Conceptual Framework for Studying Collective Reactions to Events in Location-Based Social Media},
  author = {Dunkel, Alexander and Andrienko, Gennady and Andrienko, Natalia and Burghardt, Dirk and Hauthal, Eva and Purves, Ross},
  year = {2019},
  month = apr,
  journal = {International Journal of Geographical Information Science},
  volume = {33},
  number = {4},
  pages = {780--804},
  issn = {1365-8816, 1362-3087},
  doi = {10/ggwjsm},
  abstract = {Events are a core concept of spatial information, but locationbased social media (LBSM) provide information on reactions to events. Individuals have varied degrees of agency in initiating, reacting to or modifying the course of events, and reactions include observations of occurrence, expressions containing sentiment or emotions, or a call to action. Key characteristics of reactions include referent events and information about who reacted, when, where and how. Collective reactions are composed of multiple individual reactions sharing common referents. They can be characterized according to the following dimensions: spatial, temporal, social, thematic and interlinkage. We present a conceptual framework, which allows characterization and comparison of collective reactions. For a thematically well-defined class of event such as storms, we can explore differences and similarities in collective attribution of meaning across space and time. Other events may have very complex spatio-temporal signatures (e.g. political processes such as Brexit or elections), which can be decomposed into series of individual events (e.g. a temporal window around the result of a vote). The purpose of our framework is to explore ways in which collective reactions to events in LBSM can be described and underpin the development of methods for analysing and understanding collective reactions to events.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Dunkel et al_2019_A conceptual framework for studying collective reactions to events in.pdf}
}

@article{duro2012,
  title = {A Comparison of Pixel-Based and Object-Based Image Analysis with Selected Machine Learning Algorithms for the Classification of Agricultural Landscapes Using {{SPOT-5 HRG}} Imagery},
  author = {Duro, Dennis C. and Franklin, Steven E. and Dub{\'e}, Monique G.},
  year = {2012},
  month = mar,
  journal = {Remote Sensing of Environment},
  volume = {118},
  pages = {259--272},
  issn = {00344257},
  doi = {10/fxtjfp},
  abstract = {Pixel-based and object-based image analysis approaches for classifying broad land cover classes over agricultural landscapes are compared using three supervised machine learning algorithms: decision tree (DT), random forest (RF), and the support vector machine (SVM). Overall classification accuracies between pixelbased and object-based classifications were not statistically significant (p {$>$} 0.05) when the same machine learning algorithms were applied. Using object-based image analysis, there was a statistically significant difference in classification accuracy between maps produced using the DT algorithm compared to maps produced using either RF (p = 0.0116) or SVM algorithms (p = 0.0067). Using pixel-based image analysis, there was no statistically significant difference (p {$>$} 0.05) between results produced using different classification algorithms. Classifications based on RF and SVM algorithms provided a more visually adequate depiction of wetland, riparian, and crop land cover types when compared to DT based classifications, using either object-based or pixel-based image analysis. In this study, pixel-based classifications utilized fewer variables (15 vs. 300), achieved similar classification accuracies, and required less time to produce than object-based classifications. Object-based classifications produced a visually appealing generalized appearance of land cover classes. Based exclusively on overall accuracy reports, there was no advantage to preferring one image analysis approach over another for the purposes of mapping broad land cover types in agricultural environments using medium spatial resolution earth observation imagery.},
  langid = {english},
  annotation = {ZSCC: 0000486},
  file = {/home/cjber/drive/pdf/ENVS492/Duro et al_2012_.pdf}
}

@incollection{egenhofer1995,
  title = {Naive {{Geography}}},
  booktitle = {Spatial {{Information Theory A Theoretical Basis}} for {{GIS}}},
  author = {Egenhofer, Max J. and Mark, David M.},
  editor = {Goos, Gerhard and Hartmanis, Juris and Leeuwen, Jan and Frank, Andrew U. and Kuhn, Werner},
  year = {1995},
  volume = {988},
  pages = {1--15},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-60392-1_1},
  isbn = {978-3-540-60392-4 978-3-540-45519-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Egenhofer_Mark_1995_Naive Geography.pdf}
}

@article{egorova2019,
  title = {Cross-{{Corpora Analysis}} of {{Spatial Language}}: {{The Case}} of {{Fictive Motion}} ({{Short Paper}})},
  shorttitle = {Cross-{{Corpora Analysis}} of {{Spatial Language}}},
  author = {Egorova, Ekaterina and Aflaki, Niloofar and Fagundes, Cristiane K. Marchis and Stock, Kristin},
  year = {2019},
  pages = {8 pages},
  publisher = {{Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany}},
  doi = {10/gg68z9},
  abstract = {The way people describe where things are is one of the central questions of spatial information theory and has been the subject of considerable research. We investigate one particular type of location description, fictive motion (as in, The range runs along the coast). The use of this structure is known to highlight particular properties of the described entity, as well as to convey its configuration in physical space in an effective way. We annotated 496 fictive motion structures in seven corpora that represent different types of spatial discourse \textendash{} news, travel blogs, texts describing outdoor pursuits and local history, as well as image and location descriptions. We analysed the results not only by examining the distribution of fictive motion structures across corpora, but also by exploring and comparing the semantic categories of verbs used in fictive motion. Our findings, first, add to our knowledge of location description strategies that go beyond prototypical locative phrases. They further reveal how the use of fictive motion varies across types of spatial discourse and reflects the nature of the described environment. Methodologically, we highlight the benefits of a cross-corpora analysis in the study of spatial language use across a variety of contexts.},
  collaborator = {Wagner, Michael},
  copyright = {Creative Commons Attribution 3.0 Unported license (CC-BY 3.0)},
  langid = {english},
  keywords = {000 Computer science; knowledge; general works,Computer Science},
  file = {/home/cjber/drive/pdf/Egorova et al_2019_Cross-Corpora Analysis of Spatial Language.pdf}
}

@book{eiben2015,
  title = {Introduction to {{Evolutionary Computing}}},
  author = {Eiben, A.E. and Smith, J.E.},
  year = {2015},
  series = {Natural {{Computing Series}}},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-44874-8},
  isbn = {978-3-662-44873-1 978-3-662-44874-8},
  langid = {english},
  file = {/home/cjber/drive/pdf/Eiben_Smith_2015_Introduction to Evolutionary Computing.pdf}
}

@article{eisank2014,
  title = {Assessment of Multiresolution Segmentation for Delimiting Drumlins in Digital Elevation Models},
  author = {Eisank, Clemens and Smith, Mike and Hillier, John},
  year = {2014},
  month = jun,
  journal = {Geomorphology},
  volume = {214},
  pages = {452--464},
  issn = {0169555X},
  doi = {10/f3sqfd},
  abstract = {Mapping or ``delimiting'' landforms is one of geomorphology's primary tools. Computer-based techniques such as land-surface segmentation allow the emulation of the process of manual landform delineation. Land-surface segmentation exhaustively subdivides a digital elevation model (DEM) into morphometrically-homogeneous irregularly-shaped regions, called terrain segments. Terrain segments can be created from various land-surface parameters (LSP) at multiple scales, and may therefore potentially correspond to the spatial extents of landforms such as drumlins. However, this depends on the segmentation algorithm, the parameterization, and the LSPs. In the present study we assess the widely used multiresolution segmentation (MRS) algorithm for its potential in providing terrain segments which delimit drumlins. Supervised testing was based on five 5-m DEMs that represented a set of 173 synthetic drumlins at random but representative positions in the same landscape. Five LSPs were tested, and four variants were computed for each LSP to assess the impact of median filtering of DEMs, and logarithmic transformation of LSPs. The testing scheme (1) employs MRS to partition each LSP exhaustively into 200 coarser scales of terrain segments by increasing the scale parameter (SP), (2) identifies the spatially best matching terrain segment for each reference drumlin, and (3) computes four segmentation accuracy metrics for quantifying the overall spatial match between drumlin segments and reference drumlins. Results of 100 tests showed that MRS tends to perform best on LSPs that are regionally derived from filtered DEMs, and then logtransformed. MRS delineated 97\% of the detected drumlins at SP values between 1 and 50. Drumlin delimitation rates with values up to 50\% are in line with the success of manual interpretations. Synthetic DEMs are well-suited for assessing landform quantification methods such as MRS, since subjectivity in the reference data is avoided which increases the reliability, validity and applicability of results.},
  langid = {english},
  annotation = {ZSCC: 0000048},
  file = {/home/cjber/drive/pdf/ENVS492/Eisank et al_2014_.pdf}
}

@article{elberink2000,
  title = {The {{Use}} of {{Anisotropic Height Texture Measures}} for the {{Segmentation}} of {{Airborne Laser Scanner Data}}},
  author = {Elberink, Sander Oude and Maas, Hans-Gerd},
  year = {2000},
  pages = {8},
  abstract = {Airborne laser scanning data has proven to be a very suitable technique for the determination of digital surface models and is more and more being used for mapping and GIS data acquisition purposes, including the detection and modeling of man-made objects or vegetation. The aim of the work presented here is to segment raw laser scanner data in an unsupervised classification using anisotropic height texture measures. Anisotropic operations have the potential to discriminate between orientated and non-orientated objects. The techniques have been applied to data sets from different laser scanning systems and from different regions, mainly focussing on high-density laser scanner data. The results achieved in these pilot studies show the large potential of airborne laser scanning in the field of 3-D GIS data acquisition.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000116},
  file = {/home/cjber/drive/pdf/ENVS492/Elberink_Maas_2000_.pdf}
}

@article{elliot2016,
  title = {The {{Anonymisation Decision Making FrameworkMark Elliot}},},
  author = {Elliot, Mark and Mackey, Elaine and Editor, Caroline Tudor and O'Hara, Kieron},
  year = {2016},
  pages = {171},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Elliot et al_2016_The Anonymisation Decision Making FrameworkMark Elliot,.pdf}
}

@article{elliot2018,
  title = {Functional Anonymisation: {{Personal}} Data and the Data Environment},
  shorttitle = {Functional Anonymisation},
  author = {Elliot, Mark and O'Hara, Kieron and Raab, Charles and O'Keefe, Christine M. and Mackey, Elaine and Dibben, Chris and Gowans, Heather and Purdam, Kingsley and McCullagh, Karen},
  year = {2018},
  month = apr,
  journal = {Computer Law \& Security Review},
  volume = {34},
  number = {2},
  pages = {204--221},
  issn = {02673649},
  doi = {10/gdhs4w},
  abstract = {Anonymisation of personal data has a long history stemming from the expansion of the types of data products routinely provided by National Statistical Institutes. Variants on anonymisation have received serious criticism reinforced by much-publicised apparent failures. We argue that both the operators of such schemes and their critics have become confused by being overly focused on the properties of the data themselves. We claim that, far from being able to determine whether data are anonymous (and therefore non-personal) by looking at the data alone, any anonymisation technique worthy of the name must take account of not only the data but also their environment.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Elliot et al_2018_Functional anonymisation.pdf}
}

@inproceedings{embley1998,
  title = {Ontology-Based Extraction and Structuring of Information from Data-Rich Unstructured Documents},
  booktitle = {Proceedings of the Seventh International Conference on {{Information}} and Knowledge Management  - {{CIKM}} '98},
  author = {Embley, David W. and Campbell, Douglas M. and Smith, Randy D. and Liddle, Stephen W.},
  year = {1998},
  pages = {52--59},
  publisher = {{ACM Press}},
  address = {{Bethesda, Maryland, United States}},
  doi = {10.1145/288627.288641},
  abstract = {We present a new approach to extracting information from unstructured documents based on an application ontology that describes a domain of interest. Starting with such an ontology, we formulate rules to extract constants and context keywords from unstructured documents. For each unstructured document of interest, we extract its constants and keywords and apply a recognizer to organize extracted constants as attribute values of tuples in a generated database schema. To make our approach general, we fix all the processes and change only the ontological description for a different application domain. In experiments we conducted on two different types of unstructured documents taken from the Web, our approach attained recall ratios in the 80\% and 90\% range and precision ratios near 98\%.},
  isbn = {978-1-58113-061-4},
  langid = {english},
  keywords = {Ontology},
  file = {/home/cjber/drive/pdf/Embley et al_1998_Ontology-based extraction and structuring of information from data-rich.pdf}
}

@misc{engl,
  title = {Roads Managed by {{Highways England}}},
  author = {Eng\-l, High-ways and be advised that all types of correspondence may be monitored for {training}, co uk General enquiries 0300 123 5000 Please and Purposes, Quality Assurance},
  journal = {GOV.UK},
  abstract = {Highways England manages the strategic road network in England, comprising motorways and some A roads. This map shows areas of responsibility and contact info.},
  howpublished = {https://www.gov.uk/government/publications/roads-managed-by-highways-england},
  langid = {english},
  annotation = {ZSCC: 0000000[s1]},
  file = {/home/cjber/drive/zotero/storage/DZWSKSRB/roads-managed-by-highways-england.html}
}

@misc{environmentagency2019,
  title = {{{LiDAR}}},
  author = {{Environment Agency}},
  year = {2019},
  month = may,
  copyright = {Open Government Licence},
  howpublished = {https://data.gov.uk/dataset/977a4ca4-1759-4f26-baa7-b566bd7ca7bf/lidar-point-cloud},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/cjber/drive/zotero/storage/HP46NJNY/lidar-composite-dsm-1m.html}
}

@misc{environmentalprotectionuk1997,
  title = {Air {{Quality Policy}}},
  author = {Environmental Protection UK},
  year = {1997},
  journal = {Environmental Protection UK},
  abstract = {Local Air Quality Management Since 1997, all local authorities have been assessing the air quality in their area, under the~Local Air Quality Management (LAQM). framework, andwhere a problem is found, action plans have been developed to address the situation. The LAQM framework has been reviewed by the UK government and is again under discussion under \ldots},
  langid = {american},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/zotero/storage/9N8LULWS/air-quality-policy.html}
}

@book{ertel2017,
  title = {Introduction to {{Artificial Intelligence}}},
  author = {Ertel, Wolfgang},
  year = {2017},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-58487-4},
  isbn = {978-3-319-58486-7 978-3-319-58487-4},
  langid = {english},
  file = {/home/cjber/drive/pdf/Ertel_2017_Introduction to Artificial Intelligence.pdf}
}

@misc{esri2019,
  title = {Lidar Point Classification\textemdash{{Help}} | {{ArcGIS Desktop}}},
  author = {{ESRI}},
  year = {2019},
  howpublished = {http://desktop.arcgis.com/en/arcmap/10.3/manage-data/las-dataset/lidar-point-classification.htm},
  file = {/home/cjber/drive/zotero/storage/ZTAFLM9F/lidar-point-classification.html}
}

@article{ester1996,
  title = {A {{Density-Based Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Xu, Xiaowei},
  year = {1996},
  pages = {6},
  abstract = {Clusteringalgorithmasreattractivefor the taskof classidentification in spatial databases.Howevetrh, e applicationto large spatial databasesrises the followingrequirementfsor clustering algorithms: minimalrequirementsof domain knowledgteo determinethe input parameters,discoveryof clusters witharbitraryshapeandgoodefficiencyonlarge databases. Thewell-knowcnlusteringalgorithmsoffer nosolution to the combinatioonf theserequirementsI.n this paper, wepresent the newclustering algorithmDBSCAreNlying on a density-basednotionof clusters whichis designedto discoverclusters of arbitrary shape.DBSCrAeNquiresonly one input parameterandsupportsthe user in determiningan appropriatevaluefor it. Weperformeadn experimentaelvaluation of the effectiveness and efficiency of DBSCAusNing synthetic data and real data of the SEQUO2IA000benchmark.Theresults of our experimentsdemonstratethat (1) DBSCiAsNsignificantlymoreeffective in discoveringclusters of arbitrary shapethan the well-knowanlgorithmCLARANS,and that (2) DBSCAoNutperforms CLARANbyS factorof morethan100in termsof efficiency.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Ester et al_1996_A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases.pdf}
}

@misc{europeanseverestormslaboratoryessl,
  title = {European {{Severe Weather Database}}},
  author = {{European Severe Storms Laboratory (ESSL)}},
  howpublished = {https://www.eswd.eu/},
  file = {/home/cjber/drive/zotero/storage/7TVQCQSA/www.eswd.eu.html}
}

@misc{europeanunion2016,
  title = {{{REGULATION}}  ({{EU}})  2016/  679  {{OF}}  {{THE}}  {{EUROPEAN}}  {{PARLIAMENT}}  {{AND}}  {{OF}}  {{THE}}  {{COUNCIL}}  -  of  27  {{April}}  2016  -  on  the  Protection  of  Natural  Persons  with  Regard  to  the  Processing  of  Personal  Data  and  on  the  Free  Movement  of  Such  Data,  and  Repealing  {{Directive}} 95/  46/  {{EC}}  ({{General}}  {{Data}}  {{Protection}}  {{Regulation}})},
  author = {{European Union}},
  year = {2016},
  langid = {english},
  file = {/home/cjber/drive/pdf/European Union_2016_REGULATION (EU) 2016- 679 OF THE EUROPEAN PARLIAMENT AND OF THE.pdf}
}

@book{euzenat2013,
  title = {Ontology Matching},
  author = {Euzenat, Jerome},
  year = {2013},
  edition = {2nd edition},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-3-642-38720-3},
  langid = {english},
  keywords = {Ontology},
  file = {/home/cjber/drive/pdf/Euzenat_2013_Ontology matching.pdf}
}

@article{falconwa2019,
  title = {{{PyTorch}} Lightning},
  author = {al. Falcon, WA, et},
  year = {2019},
  journal = {GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning},
  volume = {3},
  keywords = {⛔ No DOI found}
}

@article{fambro2000,
  title = {New {{Stopping Sight Distance Model}} for {{Use}} in {{Highway Geometric Design}}},
  author = {Fambro, Daniel B. and Fitzpatrick, Kay and Koppa, Rodger J.},
  year = {2000},
  month = jan,
  journal = {Transportation Research Record: Journal of the Transportation Research Board},
  volume = {1701},
  number = {1},
  pages = {1--8},
  issn = {0361-1981, 2169-4052},
  doi = {10/d34nbb},
  langid = {english},
  annotation = {ZSCC: 0000027},
  file = {/home/cjber/drive/pdf/ENVS492/Fambro et al_2000_.pdf}
}

@article{fang2004,
  title = {Noise Reduction in Lidar Signal Based on Discrete Wavelet Transform},
  author = {Fang, Hai-Tao and Huang, De-Shuang},
  year = {2004},
  month = mar,
  journal = {Optics Communications},
  volume = {233},
  number = {1-3},
  pages = {67--76},
  issn = {00304018},
  doi = {10/crz757},
  abstract = {Lidar is an efficient tool for remotely monitoring targets or objects, but the effective range is often limited by signalto-noise ratio (SNR). The reason is that noises or interferences always badly affect the measured results. So, to detect the weak signals buried in noises is a fundamental and important problem in the Lidar systems. It has been found that digital filters are not suitable for processing Lidar signals in noises, while the wavelet transform is an efficient tool for signal analysis in time\textendash frequency domain that is very sensitive to the transient signals. In this paper, we propose a new method of the Lidar signal acquisition based on discrete wavelet transform (DWT). This method can significantly improve the SNR so that the effective measured range of Lidar is increased. The performance for our method is investigated by detecting the simulating and real Lidar signals in white noise. To contrast, the results of Butterworth filter, which is a kind of finite impulse response (FIR) filter, are also demonstrated. Finally, the experimental results show that our approach outperforms the traditional methods.},
  langid = {english},
  annotation = {ZSCC: 0000107},
  file = {/home/cjber/drive/pdf/ENVS492/Fang_Huang_2004_.pdf}
}

@article{farmer2003,
  title = {Dr. {{John}} Has Gone: Assessing Health Professionals' Contribution to Remote Rural Community Sustainability in the {{UK}}},
  shorttitle = {Dr. {{John}} Has Gone},
  author = {Farmer, Jane and Lauder, William and Richards, Helen and Sharkey, Siobhan},
  year = {2003},
  month = aug,
  journal = {Social Science \& Medicine},
  volume = {57},
  number = {4},
  pages = {673--686},
  issn = {02779536},
  doi = {10/dq6rh3},
  abstract = {Due mainly to increasing difficulties in recruiting and retaining health professionals to work in remote and peripheral areas of Scotland, there is discussion of the need to implement new models of primary health care provision. However, innovative service models may imply a reduction in the number of health professionals who live and work in remote communities. Currently decisions about remodelling service provision are being taken by National Health Service stakeholders, apparently with little consideration of the wider social and economic impacts of change. This paper aims to argue that health professionals contribute to the fabric of rural life in a number of ways and that decisions about health service redesign need to take this into account. As well as fulfilling a wide health and social care role for patients, the authors seek to show that health professionals are important to the social sustainability of rural communities as, due to their unique position, they are often at the heart of networks within and between communities. The wider economic contribution of health services in remote communities is important, but often underplayed. The authors propose that theories of capital, principally the concept of social capital, could help in investigating the wider contribution of health professionals to their local communities. Ultimately, it is proposed that health services, as embodied in nurses, doctors and others, could be highly important to the ongoing livelihood and social infrastructure of fragile remote communities. Since this area is poorly understood, there is a need for prospective primary research and evaluation of service redesign initiatives.},
  langid = {english},
  annotation = {ZSCC: 0000092},
  file = {/home/cjber/drive/pdf/ENVS492/Farmer et al_2003_.pdf}
}

@article{farrington2005,
  title = {Rural Accessibility, Social Inclusion and Social Justice: Towards Conceptualisation},
  shorttitle = {Rural Accessibility, Social Inclusion and Social Justice},
  author = {Farrington, John and Farrington, Conor},
  year = {2005},
  month = mar,
  journal = {Journal of Transport Geography},
  volume = {13},
  number = {1},
  pages = {1--12},
  issn = {09666923},
  doi = {10/fthspv},
  abstract = {Accessibility has become established as a mainstream policy goal in the service of the UK Government\~Os aims of achieving greater social inclusion and social justice. It is argued that a better understanding of the relationship between conceptualisations of accessibility and these policy aims would be of value in understanding the potential of accessibility to contribute to policy. The aim of this paper is to contribute to this understanding.},
  langid = {english},
  annotation = {ZSCC: 0000342},
  file = {/home/cjber/drive/pdf/ENVS492/Farrington_Farrington_2005_.pdf}
}

@inproceedings{ferchichi2005,
  title = {Optimization of Cluster Coverage for Road Centre-Line Extraction in High Resolution Satellite Images},
  booktitle = {{{IEEE International Conference}} on {{Image Processing}} 2005},
  author = {Ferchichi, S. and {Shengrui Wang}},
  year = {2005},
  month = sep,
  volume = {2},
  pages = {II-201},
  doi = {10/fjcjsn},
  abstract = {This paper presents a new approach for road centre-line extraction from high-resolution images, based on cluster coverage. The idea is first to segment the image; in order to obtain road candidates. Then we apply the k-means algorithm to cluster the pixels of the segmented image. The resulting clusters cover the road and the noise generated by the segmentation process. We introduce the concept of cluster coverage to remove noise and to approximate the optimal number of clusters that fit the shape of the road.},
  annotation = {ZSCC: 0000010},
  file = {/home/cjber/drive/pdf/ENVS492/Ferchichi_Shengrui Wang_2005_.pdf;/home/cjber/drive/zotero/storage/E6YY28YS/1530026.html}
}

@misc{ferguson2017,
  title = {20mph Zones See Increase in Death and Serious Injuries on Roads},
  author = {Ferguson, Sam and Ballinger, Alex},
  year = {2017},
  month = dec,
  journal = {bristolpost},
  abstract = {The number of people injured or killed on 20mph roads in Bath has risen in most areas},
  howpublished = {http://www.bristolpost.co.uk/news/local-news/baths-20mph-zones-increased-deaths-942560},
  annotation = {ZSCC: 0000000[s0]},
  file = {/home/cjber/drive/zotero/storage/WD9YAFSE/baths-20mph-zones-increased-deaths-942560.html}
}

@article{ferraz2016,
  title = {Large-Scale Road Detection in Forested Mountainous Areas Using Airborne Topographic Lidar Data},
  author = {Ferraz, Ant{\'o}nio and Mallet, Cl{\'e}ment and Chehata, Nesrine},
  year = {2016},
  month = feb,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {112},
  pages = {23--36},
  issn = {09242716},
  doi = {10/gddkjk},
  abstract = {In forested mountainous areas, the road location and characterization are invaluable inputs for various purposes such as forest management, wood harvesting industry, wildfire protection and fighting. Airborne topographic lidar has become an established technique to characterize the Earth surface. Lidar provides 3D point clouds allowing for fine reconstruction of ground topography while preserving high frequencies of the relief: fine Digital Terrain Models (DTMs) is the key product.},
  langid = {english},
  annotation = {ZSCC: 0000038},
  file = {/home/cjber/drive/pdf/ENVS492/Ferraz et al_2016_.pdf}
}

@article{filgueira2020,
  title = {Geoparsing the Historical {{Gazetteers}} of {{Scotland}}: Accurately Computing Location in Mass Digitised Texts},
  author = {Filgueira, Rosa and Grover, Claire and Terras, Melissa and Alex, Beatrice},
  year = {2020},
  pages = {7},
  abstract = {This paper describes work in progress on devising automatic and parallel methods for geoparsing large digital historical textual data by combining the strengths of three natural language processing (NLP) tools, the Edinburgh Geoparser, spaCy and defoe, and employing different tokenisation and named entity recognition (NER) techniques. We apply these tools to a large collection of nineteenth century Scottish geographical dictionaries, and describe preliminary results obtained when processing this data.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Filgueira et al_2020_Geoparsing the historical Gazetteers of Scotland.pdf;/home/cjber/drive/pdf/Filgueira et al_2020_Geoparsing the historical Gazetteers of Scotland2.pdf}
}

@misc{finch1994,
  title = {Speed, {{Speed Limits}} and {{Accidents}}},
  author = {Finch, D and Kompfner, P and Lockwood, C and Maycock, G},
  year = {1994},
  howpublished = {https://trl.co.uk/sites/default/files/PR058.pdf},
  annotation = {ZSCC: 0000234},
  file = {/home/cjber/drive/pdf/ENVS492/Finch et al_1994_.pdf}
}

@inproceedings{finkel2005,
  title = {Incorporating Non-Local Information into Information Extraction Systems by {{Gibbs}} Sampling},
  booktitle = {Proceedings of the 43rd {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}  - {{ACL}} '05},
  author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher},
  year = {2005},
  pages = {363--370},
  publisher = {{Association for Computational Linguistics}},
  address = {{Ann Arbor, Michigan}},
  doi = {10/fj76xz},
  abstract = {Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9\% over state-of-the-art systems on two established information extraction tasks.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Finkel et al_2005_Incorporating non-local information into information extraction systems by.pdf}
}

@manual{firke2020,
  type = {Manual},
  title = {Janitor: {{Simple}} Tools for Examining and Cleaning Dirty Data},
  author = {Firke, Sam},
  year = {2020}
}

@article{fisher2004,
  title = {Where Is {{Helvellyn}}? {{Fuzziness}} of Multi-Scale Landscape Morphometry},
  shorttitle = {Where Is {{Helvellyn}}?},
  author = {Fisher, Peter and Wood, Jo and Cheng, Tao},
  year = {2004},
  journal = {Transactions of the Institute of British Geographers},
  volume = {29},
  number = {1},
  pages = {106--128},
  issn = {1475-5661},
  doi = {10/cnfqpv},
  abstract = {The landscape in which people live is made up of many features, which are named and have importance for cultural reasons. Prominent among these are the naming of upland features such as mountains, but mountains are an enigmatic phenomenon which do not bear precise and repeatable definition. They have a vague spatial extent, and recent research has modelled such classes as spatial fuzzy sets. We take a specifically multi-resolution approach to the definition of the fuzzy set membership of morphometric classes of landscape. We explore this idea with respect to the identification of culturally recognized landscape features of the English Lake District. Discussion focuses on peaks and passes, and the results show that the landscape elements identified in the analysis correspond to well-known landmarks included in a place name database for the area, although many more are found in the analysis than are named in the available database. Further analysis shows that a richer interrogation of the landscape can be achieved with Geographical Information Systems when using this method than using standard approaches.},
  langid = {english},
  keywords = {fuzzy sets landforms,morphometry,multi-scale analysis mountains,the Lake District},
  annotation = {\_eprint: https://rgs-ibg.onlinelibrary.wiley.com/doi/pdf/10.1111/j.0020-2754.2004.00117.x},
  file = {/home/cjber/drive/pdf/Fisher et al_2004_Where is Helvellyn.pdf;/home/cjber/drive/zotero/storage/SF4VZD3Z/(ISSN)1475-5661.html}
}

@inproceedings{fleming2009,
  title = {Lightweight Deflectometers for Quality Assurance in Road Construction},
  booktitle = {{{IN}}: {{Tutumluer}}, {{E}}. and {{Al-Qadi}}, {{IL}} (Eds). {{Bearing Capacity}} of {{Roads}}, {{Railways}} and {{Airfields}}: {{Proceedings}} of the 8th {{International Conference}} ({{BCR2A}}'09)},
  author = {Fleming, PR and Frost, MW and Lambert, JP},
  year = {2009},
  pages = {809--818},
  publisher = {{Taylor \& Francis Group}},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000017}
}

@article{floud1961,
  title = {{{HOMES AND SCHOOLS}}: {{SOCIAL DETERMINANTS OF EDUCABILITY}}},
  author = {Floud, Jean and Halsey, A. H.},
  year = {1961},
  month = feb,
  journal = {Educational Research},
  volume = {3},
  number = {2},
  pages = {83--88},
  issn = {0013-1881},
  doi = {10/cs2j8m},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000013}
}

@article{fone2006,
  title = {Mental Health, Places and People: {{A}} Multilevel Analysis of Economic Inactivity and Social Deprivation},
  author = {Fone, David L. and Dunstan, Frank},
  year = {2006},
  month = sep,
  journal = {Health \& Place},
  volume = {12},
  number = {3},
  pages = {332--344},
  issn = {1353-8292},
  doi = {10/bct25v},
  abstract = {Using data on 24,975 respondents to the Welsh Health Survey 1998 aged 17\textendash 74 years, we investigated associations between individual mental health status measured using the SF-36 instrument, social class, economic inactivity and the electoral division Townsend deprivation score. In a multilevel modelling analysis, we found mental health was significantly associated with the Townsend score after adjusting for composition, and this effect was strongest in respondents who were economically inactive. Further contextual effects were shown by significant random variability in the slopes of the relation between mental health and economic inactivity at the electoral division level. Our results suggest that the places in which people live affect their mental health, supporting NHS policy that multi-agency planning to reduce inequalities in mental health status should address the wider determinants of health, as well as services for individual patients.},
  keywords = {\#nosource}
}

@article{fone2007,
  title = {Does Social Cohesion Modify the Association between Area Income Deprivation and Mental Health? {{A}} Multilevel Analysis},
  shorttitle = {Does Social Cohesion Modify the Association between Area Income Deprivation and Mental Health?},
  author = {Fone, David and Dunstan, Frank and Lloyd, Keith and Williams, Gareth and Watkins, John and Palmer, Stephen},
  year = {2007},
  month = apr,
  journal = {International Journal of Epidemiology},
  volume = {36},
  number = {2},
  pages = {338--345},
  issn = {1464-3685, 0300-5771},
  doi = {10/fjg48h},
  abstract = {Background Despite the increasing belief that the places where people live influence their health, there is surprisingly little consistent evidence for their associations with mental health. We investigated the joint effect of community and individual-level socio-economic deprivation and social cohesion on individual mental health status. Methods Multilevel analysis of population survey data on 10 653 adults aged 18\textendash 74 years nested within the 325 census enumeration districts in Caerphilly county borough, Wales, UK. The outcome measure was the Mental Health Inventory (MHI-5) subscale of the SF-36 instrument. A social cohesion subscale was derived from a factor analysis of responses to the Neighbourhood Cohesion scale and was modelled at individual and area level. Area income deprivation was measured by the percentage of low income households. Results Poor mental health was significantly associated with area-level income deprivation and low social cohesion after adjusting for individual risk factors. High social cohesion significantly modified the association between income deprivation and mental health: the difference between the predicted mean area mental health scores at the 10th and 90th centiles of the low income distribution was 3.7 in the low cohesion group and 0.9 in the high cohesion group (difference of the difference in means {$\frac{1}{4}$} 2.8, 95\% CI: 0.2, 5.4). Conclusions Income deprivation and social cohesion measured at community level are potentially important joint determinants of mental health. Further research on the impact of the social environment on mental health should investigate causal pathways in a longitudinal study.},
  langid = {english},
  annotation = {ZSCC: 0000235},
  file = {/home/cjber/drive/pdf/ENVS416/Fone et al_2007_.pdf}
}

@article{forney1973,
  title = {The Viterbi Algorithm},
  author = {Forney, G.D.},
  year = {1973},
  month = mar,
  journal = {Proceedings of the IEEE},
  volume = {61},
  number = {3},
  pages = {268--278},
  issn = {1558-2256},
  doi = {10/b7hr4m},
  abstract = {The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.},
  keywords = {Algorithm design and analysis,Convolutional codes,Decoding,Digital communication,Markov processes,Recursive estimation,State estimation,Stochastic processes,Viterbi algorithm},
  file = {/home/cjber/drive/pdf/Forney_1973_The viterbi algorithm.pdf;/home/cjber/drive/pdf/Forney_1973_The viterbi algorithm2.pdf;/home/cjber/drive/zotero/storage/75XIUZQN/1450960.html}
}

@article{forzieri2017,
  title = {Increasing Risk over Time of Weather-Related Hazards to the {{European}} Population: A Data-Driven Prognostic Study},
  shorttitle = {Increasing Risk over Time of Weather-Related Hazards to the {{European}} Population},
  author = {Forzieri, Giovanni and Cescatti, Alessandro and e Silva, Filipe Batista and Feyen, Luc},
  year = {2017},
  month = aug,
  journal = {The Lancet Planetary Health},
  volume = {1},
  number = {5},
  pages = {e200-e208},
  publisher = {{Elsevier}},
  issn = {2542-5196},
  doi = {10/gfz74r},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}h3{$>$}Background{$<$}/h3{$><$}p{$>$}The observed increase in the effects on human beings of weather-related disasters has been largely attributed to the rise in population exposed, with a possible influence of global warming. Yet, future risks of weather-related hazards on human lives in view of climate and demographic changes have not been comprehensively investigated.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}We assessed the risk of weather-related hazards to the European population in terms of annual numbers of deaths in 30 year intervals relative to the reference period (1981\textendash 2010) up to the year 2100 (2011\textendash 40, 2041\textendash 70, and 2071\textendash 100) by combining disaster records with high-resolution hazard and demographic projections in a prognostic modelling framework. We focused on the hazards with the greatest impacts\textemdash heatwaves and cold waves, wildfires, droughts, river and coastal floods, and windstorms\textemdash and evaluated their spatial and temporal variations in intensity and frequency under a business-as-usual scenario of greenhouse gas emissions. We modelled long-term demographic dynamics through a territorial modelling platform to represent the evolution of human exposure under a corresponding middle-of-the-road socioeconomic scenario. We appraised human vulnerability to weather extremes on the basis of more than 2300 records collected from disaster databases during the reference period and assumed it to be static under a scenario of no adaptation.{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$><$}p{$>$}We found that weather-related disasters could affect about two-thirds of the European population annually by the year 2100 (351 million people exposed per year [uncertainty range 126 million to 523 million] during the period 2071\textendash 100) compared with 5\% during the reference period (1981\textendash 2010; 25 million people exposed per year). About 50 times the number of fatalities occurring annually during the reference period (3000 deaths) could occur by the year 2100 (152 000 deaths [80 500\textendash 239 800]). Future effects show a prominent latitudinal gradient, increasing towards southern Europe, where the premature mortality rate due to weather extremes (about 700 annual fatalities per million inhabitants [482\textendash 957] during the period 2071\textendash 100 \emph{vs} 11 during the reference period) could become the greatest environmental risk factor. The projected changes are dominated by global warming (accounting for more than 90\% of the rise in risk to human beings), mainly through a rise in the frequency of heatwaves (about 2700 heat-related fatalities per year during the reference period \emph{vs} 151 500 [80 100\textendash 239 000] during the period 2071\textendash 100).{$<$}/p{$><$}h3{$>$}Interpretation{$<$}/h3{$><$}p{$>$}Global warming could result in rapidly rising costs of weather-related hazards to human beings in Europe unless adequate adaptation measures are taken. Our results could aid in prioritisation of regional investments to address the unequal burden of effects on human beings of weather-related hazards and differences in adaptation capacities.{$<$}/p{$><$}h3{$>$}Funding{$<$}/h3{$><$}p{$>$}European Commission.{$<$}/p{$>$}},
  langid = {english},
  pmid = {29851641},
  file = {/home/cjber/drive/pdf/Forzieri et al_2017_Increasing risk over time of weather-related hazards to the European population.pdf;/home/cjber/drive/zotero/storage/D73EKQYF/fulltext.html}
}

@article{fotheringham1989,
  title = {Scale-Independent Spatial Analysis},
  author = {Fotheringham, A Stewart},
  year = {1989},
  journal = {Accuracy of spatial databases},
  pages = {221--228},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000111}
}

@article{fotheringham1991,
  title = {The {{Modifiable Areal Unit Problem}} in {{Multivariate Statistical Analysis}}},
  author = {Fotheringham, A S and Wong, D W S},
  year = {1991},
  month = jul,
  journal = {Environment and Planning A: Economy and Space},
  volume = {23},
  number = {7},
  pages = {1025--1044},
  issn = {0308-518X, 1472-3409},
  doi = {10/b9h2d6},
  abstract = {In this paper the examination of the modifiable areal unit problem is extended into multivariate statistical analysis. In an investigation of the parameter estimates from a multiple linear regression model and a multiple logit regression model, conclusions are drawn about the sensitivity of such estimates to variations in scale and zoning systems. The modifiable areal unit problem is shown to be essentially unpredictable in its intensity and effects in multivariate statistical analysis and is therefore a much greater problem than in univariate or bivariate analysis. The results of this analysis are rather depressing in that they provide strong evidence of the unreliability of any multivariate analysis undertaken with data from areal units. Given that such analyses can only be expected to increase with the imminent availability of new census data both in the United Kingdom and in the USA, and the current proliferation of GIS (geographical information system) technology which permits even more access to aggregated data, this paper serves as a topical warning.},
  langid = {english},
  annotation = {ZSCC: 0001092},
  file = {/home/cjber/drive/pdf/ENVS416/fotheringham1991.pdf}
}

@book{francisco,
  title = {Diana {{Maria}} de {{Sousa Marques Pinto}} Dos {{Santos}}},
  author = {Francisco, Nuno and Cardoso, Pereira Freire and Jorge, M{\'a}rio and Silva, Costa Gaspar and Qualifica{\c c}{\~a}o, Prova De and Ci{\^e}ncias, Faculdade De},
  abstract = {null},
  file = {/home/cjber/drive/pdf/Francisco et al_Diana Maria de Sousa Marques Pinto dos Santos.pdf}
}

@manual{francois2020,
  type = {Manual},
  title = {Bibtex: {{Bibtex}} Parser},
  author = {Francois, Romain},
  year = {2020}
}

@article{frank1992,
  title = {Qualitative Spatial Reasoning about Distances and Directions in Geographic Space},
  author = {Frank, Andrew U},
  year = {1992},
  month = dec,
  journal = {Journal of Visual Languages \& Computing},
  volume = {3},
  number = {4},
  pages = {343--371},
  issn = {1045926X},
  doi = {10/dgnwt4},
  langid = {english},
  file = {/home/cjber/drive/pdf/Frank_1992_Qualitative spatial reasoning about distances and directions in geographic space.pdf}
}

@article{frejka2008,
  title = {Religion, {{Religiousness}} and {{Fertility}} in the {{US}} and in {{Europe}}: {{Religion}}, Religiosit\'e et F\'econdit\'e Aux {{Etats-Unis}} et En {{Europe}}},
  shorttitle = {Religion, {{Religiousness}} and {{Fertility}} in the {{US}} and in {{Europe}}},
  author = {Frejka, Tomas and Westoff, Charles F.},
  year = {2008},
  month = mar,
  journal = {European Journal of Population / Revue europ\'eenne de D\'emographie},
  volume = {24},
  number = {1},
  pages = {5--31},
  issn = {0168-6577, 1572-9885},
  doi = {10/fgwxm5},
  abstract = {This article aims to assess the role of religion and religiousness in engendering higher US fertility compared to Europe. Religion is important in the life of one-half of US women, whereas not even for one of six Europeans. By every available measure, American women are more religious than European women. Catholic and Protestant women have notably higher fertility than those not belonging to any denomination in the US and across Europe. In all European regions and in the United States as well as among all denominations the more devout have more children. However, women in Northern and Western Europe who are the least religious have equivalent or even higher fertility than women in the US, and notably higher fertility than those in Southern Europe. This suggests that forces other than religion and religiousness are also important in their impact on childbearing. A multivariate analysis demonstrates that relatively ``traditional'' socioeconomic covariates (age, marital status, residence, education, and income) do not substantially change the positive association of religiousness and fertility. Finally, if Europeans were as religious as Americans one might theoretically expect a small fertility increase for Europe as a whole, but considerably more for Western Europe.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS418/frejka2008.pdf}
}

@article{friedman2001,
  title = {Greedy {{Function Approximation}}: {{A Gradient Boosting Machine}}},
  shorttitle = {Greedy {{Function Approximation}}},
  author = {Friedman, Jerome H.},
  year = {2001},
  journal = {The Annals of Statistics},
  volume = {29},
  number = {5},
  pages = {1189--1232},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  keywords = {❓ Multiple DOI},
  file = {/home/cjber/drive/pdf/Friedman_2001_Greedy Function Approximation.pdf}
}

@book{friedman2001a,
  title = {The Elements of Statistical Learning},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2001},
  volume = {1},
  publisher = {{Springer series in statistics New York}}
}

@incollection{fu2005,
  title = {Ontology-{{Based Spatial Query Expansion}} in {{Information Retrieval}}},
  booktitle = {On the {{Move}} to {{Meaningful Internet Systems}} 2005: {{CoopIS}}, {{DOA}}, and {{ODBASE}}},
  author = {Fu, Gaihua and Jones, Christopher B. and Abdelmoty, Alia I.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Meersman, Robert and Tari, Zahir},
  year = {2005},
  volume = {3761},
  pages = {1466--1482},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11575801_33},
  abstract = {Ontologies play a key role in Semantic Web research. A common use of ontologies in Semantic Web is to enrich the current Web resources with some well-defined meaning to enhance the search capabilities of existing web searching systems. This paper reports on how ontologies developed in the EU Semantic Web project SPIRIT are used to support retrieval of documents that are considered to be spatially relevant to users' queries. The query expansion techniques presented in this paper are based on both a domain and a geographical ontology. The proposed techniques are distinguished from conventional ones in that a query is expanded by derivation of its geographical query footprint. The techniques are specially designed to resolve a query (such as castles near Edinburgh) that involves spatial terms (e.g. Edinburgh) and fuzzy spatial relationships (e.g. near) that qualify the spatial terms. Various factors are taken into account to support intelligent expansion of a spatial query, including, spatial terms as encoded in the geographical ontology, non-spatial terms as encoded in the domain ontology, as well as the semantics of the spatial relationships and their context of use. Some experiments have been carried out to evaluate the performance of the proposed techniques using sample realistic ontologies.},
  isbn = {978-3-540-29738-3 978-3-540-32120-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Fu et al_2005_Ontology-Based Spatial Query Expansion in Information Retrieval.pdf}
}

@book{fuller2008,
  title = {The Conditions for Inappropriate High Speed: A Review of the Research Literature from 1995 to 2006},
  shorttitle = {The Conditions for Inappropriate High Speed},
  author = {Fuller, Ray and {Great Britain} and {Department for Transport}},
  year = {2008},
  publisher = {{Dept. for Transport}},
  address = {{London}},
  isbn = {978-1-906581-32-9},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s1]  OCLC: 277067853},
  file = {/home/cjber/drive/pdf/ENVS492/Fuller et al_2008_.pdf}
}

@article{galster2010,
  title = {The {{Mechanism}}(s) of {{Neighborhood Effects Theory}}, {{Evidence}}, and {{Policy Implications}}},
  author = {Galster, George C},
  year = {2010},
  pages = {33},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000004},
  file = {/home/cjber/drive/pdf/ENVS416/galster2010.pdf}
}

@incollection{galton2005,
  title = {Anchoring: {{A New Approach}} to {{Handling Indeterminate Location}} in {{GIS}}},
  shorttitle = {Anchoring},
  booktitle = {Spatial {{Information Theory}}},
  author = {Galton, Antony and Hood, James},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Cohn, Anthony G. and Mark, David M.},
  year = {2005},
  volume = {3693},
  pages = {1--13},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11556114_1},
  abstract = {We describe a new approach to representing vague or uncertain information concerning spatial location. Locational information about objects in information space is expressed through various anchoring relations which enable us to state exactly what is known regarding the spatial location of an object without forcing us to identify that location with either a precise region in the embedding space or any precise mathematical construct from such regions, such as rough sets or fuzzy sets. We describe the motivation for introducing Anchoring, propose the beginnings of a formal theory of the anchoring relations, and illustrate some of the ideas with examples typical of the real-life use of GIS.},
  isbn = {978-3-540-28964-7 978-3-540-32020-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Galton_Hood_2005_Anchoring.pdf}
}

@article{gantner2011,
  title = {A {{Spatiotemporal Ontology}} for the {{Administrative Units}} of {{Switzerland}}},
  author = {Gantner, Felix},
  year = {2011},
  pages = {151},
  abstract = {The evolution of the administrative units (AUs) of Switzerland has accelerated dramatically in the last 20 years. Between 1990 and 2010, the Federal Statistical Office (FSO) recorded a loss of 427 municipalities, a substantial decrease compared with the decline of 75 municipalities in the 30 years before. This trend implicates an elevated rate of spatial change and makes it difficult for the Swiss Federal Institute for Forest, Snow and Landscape Research (WSL) to correctly assign biotope objects to a municipality. Such an accurate reference is necessary because the cantons often entrust the municipalities with the responsibility to protect the biotopes within their boundaries. Hence, the location of a biotope has legal and financial consequences that may change as a result of boundary changes. Despite the relevance of spatial change, there is still a lack of an information system that supports queries on spatial change of municipalities. This stems from the fact that the thematic and the geometric data originate from different sources and do not match. Moreover, AUs can become unrecognizable over time because neither the Historicized Municipality Register (HMR+) nor the geometric datasets give AUs an identity.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Gantner_2011_A Spatiotemporal Ontology for the Administrative Units of Switzerland.pdf}
}

@article{gao2013,
  title = {Towards {{Platial Joins}} and {{Buffers}} in {{Place-Based GIS}}},
  author = {Gao, Song and Janowicz, Krzysztof and McKenzie, Grant and Li, Linna},
  year = {2013},
  pages = {8},
  abstract = {Place-based GIS are still a novel research topic and break with some traditions of established systems. The typical spatial perspective is based on geometric reference systems that include coordinates, distances, topology, and directions; while the alternative platial perspective is usually characterized by place names and descriptions as well as semantic relationships between places. In past decades, spacebased geographic information systems have made significant progress in terms of theories, models, functionalities, and applications. In contrast, place-based GIS are not yet well developed, although there is an increasing interest in platial and especially relational approaches. In this paper we take an example-driven, first step towards introducing placebased versions of the well known spatial join and buffer operations, and apply them to deal with place-based semantic compression and expansion in DBpedia.},
  langid = {english},
  keywords = {⛔ No DOI found,Platial},
  file = {/home/cjber/drive/pdf/Gao et al_2013_Towards Platial Joins and Buffers in Place-Based GIS.pdf}
}

@article{gao2017,
  title = {Constructing Gazetteers from Volunteered {{Big Geo-Data}} Based on {{Hadoop}}},
  author = {Gao, Song and Li, Linna and Li, Wenwen and Janowicz, Krzysztof and Zhang, Yue},
  year = {2017},
  month = jan,
  journal = {Computers, Environment and Urban Systems},
  volume = {61},
  pages = {172--186},
  issn = {01989715},
  doi = {10/f9jhdk},
  abstract = {Traditional gazetteers are built and maintained by authoritative mapping agencies. In the age of Big Data, it is possible to construct gazetteers in a data-driven approach by mining rich volunteered geographic information (VGI) from the Web. In this research, we build a scalable distributed platform and a highperformance geoprocessing workflow based on the Hadoop ecosystem to harvest crowd-sourced gazetteer entries. Using experiments based on geotagged datasets in Flickr, we find that the MapReduce-based workflow running on the spatially enabled Hadoop cluster can reduce the processing time compared with traditional desktop-based operations by an order of magnitude. We demonstrate how to use such a novel spatial-computing infrastructure to facilitate gazetteer research. In addition, we introduce a provenance-based trust model for quality assurance. This work offers new insights on enriching future gazetteers with the use of Hadoop clusters, and makes contributions in connecting GIS to the cloud computing environment for the next frontier of Big Geo-Data analytics.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Gao et al_2017_Constructing gazetteers from volunteered Big Geo-Data based on Hadoop.pdf}
}

@article{gao2017a,
  ids = {gao2017b},
  title = {A Data-Synthesis-Driven Method for Detecting and Extracting Vague Cognitive Regions},
  author = {Gao, Song and Janowicz, Krzysztof and Montello, Daniel R. and Hu, Yingjie and Yang, Jiue-An and McKenzie, Grant and Ju, Yiting and Gong, Li and Adams, Benjamin and Yan, Bo},
  year = {2017},
  month = jun,
  journal = {International Journal of Geographical Information Science},
  volume = {31},
  number = {6},
  pages = {1245--1271},
  publisher = {{Taylor \& Francis Ltd}},
  issn = {13658816},
  doi = {10/gdrbsq},
  abstract = {Cognitive regions and places are notoriously difficult to represent in geographic information science and systems. The exact delineation of cognitive regions is challenging insofar as borders are vague, membership within the regions varies non-monotonically, and raters cannot be assumed to assess membership consistently and homogeneously. In a study published in this journal in 2014, researchers devised a novel grid-based task in which participants rated the membership of individual cells in a given region and contrasted this approach to a standard boundary-drawing task. Specifically, the authors assessed the vague cognitive regions ofNorthern CaliforniaandSouthern California. The boundary between these cognitive regions was found to have variable width, and region membership peaked not at the most northern or southern cells but at substantially less extreme latitudes. The authors thus concluded that region membership is about attitude, not just latitude. In the present work, we reproduce this study by approaching it from a computationalfourth-paradigmperspective, i.e., by the synthesis of high volumes of heterogeneous data from various sources. We compare the regions which we identify to those from the human-participants study of 2014, identifying differences and commonalities. Our results show a significant positive correlation to those in the original study. Beyond the extracted regions themselves, we compare and contrast the empirical and analytical approaches of these two methods, one a conventional human-participants study and the other an application of increasingly popular data-synthesis-driven research methods in GIScience.},
  keywords = {cognitive regions,Computational acoustics,Correlation (Statistics),Data analysis,data synthesis,Information science,latent Dirichlet allocation,Multivariate analysis,Place,vagueness},
  file = {/home/cjber/drive/pdf/Gao et al_2017_A data-synthesis-driven method for detecting and extracting vague cognitive.pdf;/home/cjber/drive/pdf/Gao et al_2017_A data-synthesis-driven method for detecting and extracting vague cognitive2.pdf}
}

@book{garcia2015,
  title = {Data {{Preprocessing}} in {{Data Mining}}},
  author = {Garc{\'i}a, Salvador and Luengo, Juli{\'a}n and Herrera, Francisco},
  year = {2015},
  series = {Intelligent {{Systems Reference Library}}},
  volume = {72},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10247-4},
  isbn = {978-3-319-10246-7 978-3-319-10247-4},
  langid = {english},
  file = {/home/cjber/drive/pdf/García et al_2015_Data Preprocessing in Data Mining.pdf}
}

@inproceedings{gardner2018,
  title = {{{AllenNLP}}: {{A Deep Semantic Natural Language Processing Platform}}},
  shorttitle = {{{AllenNLP}}},
  booktitle = {Proceedings of {{Workshop}} for {{NLP Open Source Software}} ({{NLP-OSS}})},
  author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson F. and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke},
  year = {2018},
  pages = {1--6},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10/ggddq7},
  abstract = {This paper describes AllenNLP, a platform for research on deep learning methods in natural language understanding. AllenNLP is designed to support researchers who want to build novel language understanding models quickly and easily. It is built on top of PyTorch, allowing for dynamic computation graphs, and provides (1) a flexible data API that handles intelligent batching and padding, (2) highlevel abstractions for common operations in working with text, and (3) a modular and extensible experiment framework that makes doing good science easy. It also includes reference implementations of high quality approaches for both core semantic problems (e.g. semantic role labeling (Palmer et al., 2005)) and language understanding applications (e.g. machine comprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source effort maintained by engineers and researchers at the Allen Institute for Artificial Intelligence.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Gardner et al_2018_AllenNLP.pdf}
}

@manual{garnier2018,
  type = {Manual},
  title = {Viridis: {{Default}} Color Maps from 'Matplotlib'},
  author = {Garnier, Simon},
  year = {2018}
}

@article{gelernter2011,
  ids = {gelernter2011a},
  title = {Geo-Parsing {{Messages}} from {{Microtext}}: {{Geo-parsing Messages}} from {{Microtext}}},
  shorttitle = {Geo-Parsing {{Messages}} from {{Microtext}}},
  author = {Gelernter, Judith and Mushegian, Nikolai},
  year = {2011},
  month = dec,
  journal = {Transactions in GIS},
  volume = {15},
  number = {6},
  pages = {753--773},
  issn = {13611682},
  doi = {10.1111/j.1467-9671.2011.01294.x},
  abstract = {Widespread use of social media during crises has become commonplace, as shown by the volume of messages during the Haiti earthquake of 2010 and Japan tsunami of 2011. Location mentions are particularly important in disaster messages as they can show emergency responders where problems have occurred. This article explores the sorts of locations that occur in disaster-related social messages, how well off-theshelf software identifies those locations, and what is needed to improve automated location identification, called geo-parsing. To do this, we have sampled Twitter messages from the February 2011 earthquake in Christchurch, Canterbury, New Zealand. We annotated locations in messages manually to make a gold standard by which to measure locations identified by a Named Entity Recognition software. The Stanford NER software found some locations that were proper nouns, but did not identify locations that were not capitalized, local streets and buildings, or nonstandard place abbreviations and mis-spellings that are plentiful in microtext. We review how these problems might be solved in software research, and model a readable crisis map that shows crisis location clusters via enlarged place labels.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Gelernter_Mushegian_2011_Geo-parsing Messages from Microtext.pdf;/home/cjber/drive/pdf/Gelernter_Mushegian_2011_Geo-parsing Messages from Microtext2.pdf}
}

@book{gelernter2013,
  title = {An Algorithm for Local Geoparsing of Microtext. {{GeoInformatica}}},
  author = {Gelernter, Judith and Balaji, Shilpa},
  year = {2013},
  abstract = {Abstract The location of the author of a social media message is not invariably the same as the location that the author writes about in the message. In applications that mine these messages for information such as tracking news, political events or responding to disasters, it is the geographic content of the message rather than the location of the author that is important. To this end, we present a method to geo-parse the short, informal messages known as microtext. Our preliminary investigation has shown that many microtext messages contain place references that are abbreviated, misspelled, or highly localized. These references are missed by standard geo-parsers. Our geo-parser is built to find such references. It uses Natural Language Processing methods to identify references to streets and addresses, buildings and urban spaces, and toponyms, and place acronyms and abbreviations. It combines heuristics, open-source Named Entity Recognition software, and machine learning techniques. Our primary data consisted of Twitter messages sent immediately following the February 2011 earthquake in Christchurch, New Zealand. The algorithm identified location in the data sample, Twitter messages, giving an F statistic of 0.85 for streets, 0.86 for buildings, 0.96 for toponyms, and 0.88 for place abbreviations, with a combined average F of 0.90 for identifying places. The same data run through a geo-parsing standard, Yahoo! Placemaker, yielded an F statistic of zero for streets and buildings (because Placemaker is designed to find neither streets nor buildings), and an F of 0.67 for toponyms.},
  file = {/home/cjber/drive/pdf/Gelernter_Balaji_2013_An algorithm for local geoparsing of microtext.pdf}
}

@misc{geocomputation2019,
  title = {{{GeoComputation}}},
  author = {{GeoComputation}},
  year = {2019},
  howpublished = {http://www.geocomputation.org/what.html},
  file = {/home/cjber/drive/zotero/storage/N5EDJWK7/what.html}
}

@article{geronimus1996,
  title = {On the Validity of Using Census Geocode Characteristics to Proxy Individual Socioeconomic Characteristics},
  author = {Geronimus, Arline T and Bound, John and Neidert, Lisa J},
  year = {1996},
  journal = {Journal of the American Statistical Association},
  volume = {91},
  number = {434},
  pages = {529--537},
  issn = {0162-1459},
  doi = {10/gfztdj},
  annotation = {ZSCC: 0000282},
  file = {/home/cjber/drive/pdf/ENVS416/Geronimus et al_1996_.pdf}
}

@article{gerster2007,
  title = {Education and Second Birth Rates in {{Denmark}} 1981-1994},
  author = {Gerster, Mette and Keiding, Niels and Knudsen, Lisbeth B. and {Strandberg-Larsen}, Katrine},
  year = {2007},
  month = nov,
  journal = {Demographic Research},
  volume = {17},
  pages = {181--210},
  issn = {1435-9871},
  doi = {10/d54j8d},
  abstract = {A high educational attainment is shown to have a positive effect on second birth rates for Danish one-child mothers during the period 1981-94. We examine whether a timesqueeze is a possible explanation: due to the longer enrolment in the educational system, highly educated women have less time at their disposal in order to get the desired number of children. Also, we examine to what extent the partner\v{S}s education can explain some of the positive effect. We find no evidence that the positive effect of education is due to a time-squeeze nor to a partner effect.},
  langid = {english},
  annotation = {ZSCC: 0000000},
  file = {/home/cjber/drive/pdf/ENVS418/Gerster et al_2007_.pdf}
}

@article{gey2006,
  title = {{{GeoCLEF}} 2006: The {{CLEF}} 2006 {{Cross-Language Geographic Information Retrieval Track Overview}}},
  author = {Gey, Fredric and Larson, Ray and Sanderson, Mark and Bischoff, Kerstin and Mandl, Thomas and {Womser-Hacker}, Christa and Santos, Diana and Rocha, Paulo},
  year = {2006},
  pages = {20},
  abstract = {After being a pilot track in 2005, GeoCLEF advanced to be a regular track within CLEF 2006. The purpose of GeoCLEF is to test and evaluate cross-language geographic information retrieval (GIR): retrieval for topics with a geographic specification. For GeoCLEF 2006, twenty-five search topics were defined by the organizing groups for searching English, German, Portuguese and Spanish document collections. Topics were translated into English, German, Portuguese, Spanish and Japanese. Several topics in 2006 were significantly more geographically challenging than in 2005. Seventeen groups submitted 149 runs (up from eleven groups and 117 runs in GeoCLEF 2005). The groups used a variety of approaches, including geographic bounding boxes, named entity extraction and external knowledge bases (geographic thesauri and ontologies and gazetteers).},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Gey et al_2006_GeoCLEF 2006.pdf}
}

@article{ghafarian2020,
  title = {Identifying Crisis-Related Informative Tweets Using Learning on Distributions},
  author = {Ghafarian, Seyed Hossein and Yazdi, Hadi Sadoghi},
  year = {2020},
  month = mar,
  journal = {Information Processing \& Management},
  volume = {57},
  number = {2},
  pages = {102145},
  issn = {0306-4573},
  doi = {10/gj8br7},
  abstract = {Social networks like Twitter are good means for people to express themselves and ask for help in times of crisis. However, to provide help, authorities need to identify informative posts on the network from the vast amount of non-informative ones to better know what is actually happening. Traditional methods for identifying informative posts put emphasis on the presence or absence of certain words which has limitations for classifying these posts. In contrast, in this paper, we propose to consider the (overall) distribution of words in the post. To do this, based on the distributional hypothesis in linguistics, we assume that each tweet is a distribution from which we have drawn a sample of words. Building on recent developments in learning methods, namely learning on distributions, we propose an approach which identifies informative tweets by using distributional assumption. Extensive experiments have been performed on Twitter data from more than 20 crisis incidents of nearly all types of incidents. These experiments show the superiority of the proposed approach in a number of real crisis incidents. This implies that better modelling of the content of a tweet based on recent advances in estimating distributions and using domain-specific knowledge for various types of crisis incidents such as floods or earthquakes, may help to achieve higher accuracy in the task.},
  langid = {english},
  keywords = {Crisis incidents tweets,Crisis management,Learning on distributions},
  file = {/home/cjber/drive/pdf/Ghafarian_Yazdi_2020_Identifying crisis-related informative tweets using learning on distributions.pdf;/home/cjber/drive/zotero/storage/25LTNWYD/S030645731930322X.html}
}

@book{ghanem2017,
  title = {Handbook of Uncertainty Quantification},
  author = {Ghanem, Roger},
  year = {2017},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{New York, NY}},
  isbn = {978-3-319-12384-4 978-3-319-12386-8 978-3-319-12385-1},
  langid = {english},
  annotation = {ZSCC: 0000108},
  file = {/home/cjber/drive/pdf/ENVS492/Ghanem_2017_.pdf}
}

@article{gibbons2008,
  title = {Valuing School Quality, Better Transport, and Lower Crime: Evidence from House Prices},
  shorttitle = {Valuing School Quality, Better Transport, and Lower Crime},
  author = {Gibbons, S. and Machin, S.},
  year = {2008},
  month = mar,
  journal = {Oxford Review of Economic Policy},
  volume = {24},
  number = {1},
  pages = {99--119},
  issn = {0266-903X, 1460-2121},
  doi = {10/dth4c2},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS453/gibbons2008.pdf}
}

@inproceedings{giridhar2015,
  title = {On Quality of Event Localization from Social Network Feeds},
  booktitle = {2015 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communication Workshops}} ({{PerCom Workshops}})},
  author = {Giridhar, Prasanna and Abdelzaher, Tarek and George, Jemin and Kaplan, Lance},
  year = {2015},
  month = mar,
  pages = {75--80},
  publisher = {{IEEE}},
  address = {{St. Louis, MO}},
  doi = {10/ggwjtp},
  abstract = {Social networks, such as Twitter, carry important information on ongoing events and as such can be viewed as networks of sensors that monitor and report events in the physical world. In this paper, we concern ourselves with the challenge of event localization from Twitter feeds. We explore the quality of information that can be derived either directly or indirectly from microblog entries regarding locations of ongoing events. Contrary to prior work that used Twitter to map regions of largefootprint events, or derived coarse-grained location information, in this paper, we are interested in point-events, such as building fires or car accidents, and aim to pin-point them down to a street address. An algorithm is presented that identifies distinct event signatures in the blogosphere, clusters microblogs based on events they describe, and analyzes the resulting clusters for fine-grained location indicators. An exact event location is then derived by fusing these indicators. To evaluate the quality of derived location information, we use road-traffic-related Twitter feeds from 3 major cities in California and compare automatic event localization within our service to manually obtained ground truth data. Results show a great correspondence between our automatically determined locations and ground-truth.},
  isbn = {978-1-4799-8425-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Giridhar et al_2015_On quality of event localization from social network feeds.pdf}
}

@article{gogolla2012,
  title = {Object {{Constraint Language}}},
  author = {Gogolla, Martin},
  year = {2012},
  pages = {84},
  langid = {english},
  keywords = {⛔ No DOI found,Ontology},
  file = {/home/cjber/drive/pdf/Gogolla_2012_Object Constraint Language.pdf}
}

@article{goldstein2004,
  title = {The Emergence of {{Sub-Replacement Family Size Ideals}} in {{Europe}}},
  author = {Goldstein, Joshua and Lutz, Wolfgang and Testa, Maria Rita},
  year = {2004},
  journal = {Population Research and Policy Review},
  volume = {22},
  number = {5/6},
  pages = {479--496},
  doi = {10/fb7vck},
  abstract = {Period fertility started to drop significantly below replacement in most Western European countries during the 1970s and 1980s, while most fertility surveys, value studies and opinion polls have found that the number of children considered ideal for society or for one's own family has remained above two children per woman. These surveys have led to the expectation that, sooner or later, period fertility would recover in Europe. The most recent data from the Eurobarometer 2001 survey, however, suggest that in the German-speaking parts of Europe the average ideal family sizes given by younger men and women have fallen as low as 1.7 children. This paper examines the consistency and the credibility of these new findings, which \textendash{} if they are indeed indications of a new trend \textendash{} may alter the current discussion about future fertility trends in Europe.},
  mendeley-groups = {ENVS418},
  file = {/home/cjber/drive/pdf/ENVS418/goldstein2004.pdf}
}

@article{goodchild2006,
  title = {Summary {{Report Digital Gazetteer Research}} \& {{Practice Workshop}}},
  author = {Goodchild, Michael and Hill, Linda L},
  year = {2006},
  pages = {38},
  langid = {english},
  keywords = {⛔ No DOI found,Gazetteer},
  file = {/home/cjber/drive/pdf/Goodchild_Hill_2006_Summary Report Digital Gazetteer Research & Practice Workshop.pdf}
}

@article{goodchild2007,
  ids = {goodchild2007a},
  title = {Citizens as Sensors: The World of Volunteered Geography},
  shorttitle = {Citizens as Sensors},
  author = {Goodchild, Michael F.},
  year = {2007},
  journal = {GeoJournal},
  volume = {69},
  number = {4},
  pages = {211--221},
  publisher = {{Springer}},
  issn = {0343-2521},
  doi = {10/ctdpsc},
  abstract = {In recent months there has been an explosion of interest in using the Web to create, assemble, and disseminate geographic information provided voluntarily by individuals. Sites such as Wikimapia and OpenStreetMap are empowering citizens to create a global patchwork of geographic information, while Google Earth and other virtual globes are encouraging volunteers to develop interesting applications using their own data. I review this phenomenon, and examine associated issues: what drives people to do this, how accurate are the results, will they threaten individual privacy, and how can they augment more conventional sources? I compare this new phenomenon to more traditional citizen science and the role of the amateur in geographic observation.},
  file = {/home/cjber/drive/pdf/Goodchild_2007_Citizens as sensors.pdf}
}

@article{goodchild2008,
  title = {Introduction to Digital Gazetteer Research},
  author = {Goodchild, M. F. and Hill, L. L.},
  year = {2008},
  month = oct,
  journal = {International Journal of Geographical Information Science},
  volume = {22},
  number = {10},
  pages = {1039--1044},
  issn = {1365-8816, 1362-3087},
  doi = {10/b6wj5b},
  langid = {english},
  keywords = {Gazetteer,Key Paper},
  file = {/home/cjber/drive/pdf/Goodchild_Hill_2008_Introduction to digital gazetteer research.pdf}
}

@incollection{goodchild2011,
  ids = {goodchild2011a},
  title = {Formalizing {{Place}} in {{Geographic Information Systems}}},
  booktitle = {Communities, {{Neighborhoods}}, and {{Health}}},
  author = {Goodchild, Michael F.},
  editor = {Burton, Linda M. and Matthews, Stephen A. and Leung, ManChui and Kemp, Susan P. and Takeuchi, David T.},
  year = {2011},
  pages = {21--33},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-7482-2_2},
  isbn = {978-1-4419-7481-5 978-1-4419-7482-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Goodchild_2011_Formalizing Place in Geographic Information Systems.pdf;/home/cjber/drive/pdf/Goodchild_2011_Formalizing Place in Geographic Information Systems2.pdf}
}

@article{goos,
  title = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
  pages = {624},
  langid = {english},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{grace2020,
  title = {Hyperlocal {{Toponym Usage}} in {{Storm- Related Social Media}}},
  author = {Grace, Rob},
  year = {2020},
  pages = {12},
  abstract = {Crisis responders need to locate events reported in social media messages that typically lack geographic metadata such as geotags. Toponyms, places names referenced in messages, provide another source of geographic information, however, the availability and granularity of toponyms in crisis social media remain poorly understood. This study examines toponym usage and granularity across six categories of crisis-related information posted on Twitter during a severe storm. Findings show users often include geographic information in messages describing local and remote storm events but do so rarely when discussing other topics, more often use toponyms than geotags when describing local events, and tend to include fine-grained toponyms in reports of infrastructure damage and service disruption and course-grained toponyms in other kinds of storm-related messages. These findings present requirements for hyperlocal geoparsing techniques and suggest that social media monitoring presents more immediate affordances for course-grained damage assessment than fine-grained situational awareness during a crisis.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Grace_2020_Hyperlocal Toponym Usage in Storm- Related Social Media.pdf}
}

@article{grace2020a,
  title = {Toponym {{Usage}} in {{Social Media}} in {{Emergencies}}},
  author = {Grace, Rob},
  year = {2020},
  month = oct,
  journal = {International Journal of Disaster Risk Reduction},
  pages = {101923},
  issn = {22124209},
  doi = {10/ghjp44},
  abstract = {For emergency responders, the utility of social media hinges on available and accurate geographic information. Responders must locate events reported on social media to assess a situation, coordinate resources, and provide assistance. Unfortunately, however, social media often lacks geographic metadata and, if available, can inaccurately reflect the locations of reported events. Toponyms, place names in social media content, provide another source of geographic information yet toponym usage in emergencies remains poorly understood.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Grace_2020_Toponym Usage in Social Media in Emergencies.pdf}
}

@article{graham2015,
  title = {Towards a Study of Information Geographies: (Im)Mutable Augmentations and a Mapping of the Geographies of Information},
  shorttitle = {Towards a Study of Information Geographies},
  author = {Graham, Mark and Sabbata, Stefano De and Zook, Matthew A.},
  year = {2015},
  journal = {Geo: Geography and Environment},
  volume = {2},
  number = {1},
  pages = {88--105},
  issn = {2054-4049},
  doi = {10/ggsp4d},
  abstract = {Information has always had geography. It is from somewhere; about somewhere; it evolves and is transformed somewhere; it is mediated by networks, infrastructures, and technologies: all of which exist in physical, material places. These geographies of information about places matter because they shape how we are able to find and understand different parts of the world. Places invisible or discounted in representations are invisible in practice to many people. In other words, geographic augmentations are much more than just representations of places: they are part of the place itself; they shape it rather than simply reflect it. This fusing of the spatial and informational augmentations that are immutable means that annotations of place emerge as sites of political contestation: with different groups of people trying to impose different narratives on informational augmentations. This paper therefore explores how information geographies have their own geographic distributions: geographies of access, of participation, and of representation. The paper offers a deliberately broad survey of a range of key platforms that mediate, host, and deliver different types of geographic information. It does so using a combination of existing statistics and bespoke data not previously mapped or analysed. Through this effort, the paper demonstrates that in addition to the geographies of uneven access to contemporary modes of communication, uneven geographies of participation and representation are also evident and in some cases are being amplified rather than alleviated. In other words, the paper comprehensively shows one important facet of contemporary information geographies: that geographic information itself is characterised by a host of uneven geographies. The paper concludes that there are few signs that global informational peripheries are achieving comparable levels of participation or representation with traditional information cores, despite the hopes that the fast-paced spread of the internet to three billion people might change this pattern.},
  langid = {english},
  keywords = {augmented realities,Geoweb,immutable mobiles,information geography},
  annotation = {\_eprint: https://rgs-ibg.onlinelibrary.wiley.com/doi/pdf/10.1002/geo2.8},
  file = {/home/cjber/drive/pdf/Graham et al_2015_Towards a study of information geographies.pdf;/home/cjber/drive/zotero/storage/6SXVEPK5/geo2.html}
}

@article{gray2001,
  title = {Car Dependence in Rural {{Scotland}}: Transport Policy, Devolution and the Impact of the Fuel Duty Escalator},
  shorttitle = {Car Dependence in Rural {{Scotland}}},
  author = {Gray, David and Farrington, John and Shaw, Jon and Martin, Suzanne and Roberts, Deborah},
  year = {2001},
  month = jan,
  journal = {Journal of Rural Studies},
  volume = {17},
  number = {1},
  pages = {113--125},
  issn = {07430167},
  doi = {10/ffjtfr},
  abstract = {This paper examines dependence on the car in rural Scotland, assesses the impact of the fuel duty escalator on rural communities and discusses the role of the new Scottish Executive in shaping future rural transport policy. Questionnaires, interviews and travel diaries were used in "ve areas and revealed that households in rural Scotland enjoy high levels of car ownership, and that the car is used for over three-quarters of all journeys. Isolation and income levels are the most signi"cant predictors of car use. Those living in \&removed' areas * i.e. locations distant from main roads and/or bus routes * are more likely to own vehicles and make a higher proportion of their journeys by car. A\%uent households enjoy higher levels of car ownership, and make more journeys over greater distances by this mode than those on low-incomes. Less a\%uent households are also more likely to have disposed of a vehicle without replacing it, suggesting a more \#uctuating dependence on the car. Although those living in rural Scotland appear to count on the car, a distinction is made between those who have no alternative (structurally dependent) and those who have alternatives but rely on their vehicles. It is di\$cult to predict the exact impact of the fuel duty escalator, but it is argued that the majority of households will cope with increases in the cost of motoring, while a signi"cant minority of low income households in isolated areas will struggle to absorb the extra cost. The study highlights the need for the Scottish legislature to secure additional funding in order to sustain rural communities in the face of the rising fuel costs and suggests that an appropriate policy response might be to support isolated shops and services, i.e. subsidising alternatives to the journey as well as alternatives to the car. 2001 Elsevier Science Ltd. All rights reserved.},
  langid = {english},
  annotation = {00075},
  file = {/home/cjber/drive/pdf/ENVS492/Road Assessment/Gray et al_2001_.pdf}
}

@article{green2002,
  title = {Fear of Crime and Health in Residential Tower Blocks: {{A}} Case Study in {{Liverpool}}, {{UK}}},
  shorttitle = {Fear of Crime and Health in Residential Tower Blocks},
  author = {Green, G.},
  year = {2002},
  month = mar,
  journal = {The European Journal of Public Health},
  volume = {12},
  number = {1},
  pages = {10--15},
  issn = {1101-1262, 1464-360X},
  doi = {10/c4z46n},
  abstract = {Background: Though it is often assumed that fear of crime erodes mental health, research evidence is limited. Our study seeks to assess the relationship between these attributes in residents of the city of Liverpool. Method: Evidence is drawn from a sample survey of 407 adults living in 21 tower blocks. A number of social and psychosocial attributes linked with feelings of safety are compared with self-reported health status using logistic and multiple regression techniques. Possible reciprocal relationships were investigated using two-stage least squares. Results: Fear of crime in this sample is generally much lower in the home than in Britain as a whole and much higher out on the neighbouring streets at night, but there are sub-group variations. We find significant associations between fear of crime and health status. Feelings of safety when out alone after dark is the most consistent predictor of health status. Those feeling safe score significantly higher on all five dimensions of the SF-36 measure which cover mental and social well-being. Mental health is the strongest correlate and is probably a consequence rather than cause of feelings of safety. Conclusion: The evidence suggests elderly residents believe tower blocks provide safe accommodation. However, feelings of safety in these `fortresses' do not generally extend to walking in neighbouring streets. Fear of crime erodes quality of life and is associated with poorer health.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS456/Green_2002_.pdf}
}

@article{green2018,
  title = {Developing Openly Accessible Health Indicators for Small Areas in {{Great Britain}}: An Observational Study},
  shorttitle = {Developing Openly Accessible Health Indicators for Small Areas in {{Great Britain}}},
  author = {Green, Mark A and Daras, Konstantinos and Davies, Alec and Barr, Ben and Bayliss, David and Singleton, Alex},
  year = {2018},
  month = nov,
  journal = {The Lancet},
  volume = {392},
  pages = {S39},
  issn = {01406736},
  doi = {10/gf33rz},
  abstract = {Background Public health has long been concerned with understanding how the accessibility of individuals to certain environmental features can influence health and wellbeing. Such insights are increasingly being adopted by policy makers for designing healthy neighbourhoods. We aimed to use small area national level data to support and inform decision making about effects of environmental features on health.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Green et al_2018_.pdf}
}

@article{grenon2004,
  title = {{{SNAP}} and {{SPAN}}: {{Towards Dynamic Spatial Ontology}}},
  shorttitle = {{{SNAP}} and {{SPAN}}},
  author = {Grenon, Pierre and Smith, Barry},
  year = {2004},
  month = mar,
  journal = {Spatial Cognition \& Computation},
  volume = {4},
  number = {1},
  pages = {69--104},
  issn = {1387-5868, 1542-7633},
  doi = {10/bqtnk7},
  abstract = {We propose a modular ontology of the dynamic features of reality. This amounts, on the one hand, to a purely spatial ontology supporting snapshot views of the world at successive instants of time and, on the other hand, to a purely spatiotemporal ontology of change and process. We argue that dynamic spatial ontology must combine these two distinct types of inventory of the entities and relationships in reality, and we provide characterizations of spatiotemporal reasoning in the light of the interconnections between them.},
  langid = {english},
  keywords = {Ontology},
  file = {/home/cjber/drive/pdf/Grenon_Smith_2004_SNAP and SPAN.pdf}
}

@article{griffith2003,
  title = {Exploring {{Relationships Between}} the {{Global}} and {{Regional Measures}} of {{Spatial Autocorrelation}}},
  author = {Griffith, Daniel A. and Wong, David W. S. and Whitfield, Thomas},
  year = {2003},
  month = nov,
  journal = {Journal of Regional Science},
  volume = {43},
  number = {4},
  pages = {683--710},
  issn = {0022-4146, 1467-9787},
  doi = {10/fq2j4r},
  abstract = {The objective of this research is to investigate dimensions of geographic variation in spatial dependency contained within large multilevel data sets. We calculate 1990 population density by census block group, county, and state for the 48 coterminous states and the District of Columbia of the United States, calculations of interest to a wide variety of spatial scientists. We explore relations between these levels and their variation across the nation. The empirical findings generated by this work furnish implications concerning the Modifiable Areal Unit Problem (MAUP), spatial autocorrelation statistics, scale effects, and resolution.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Griffith et al_2003_.pdf}
}

@inproceedings{gritta2017,
  title = {Vancouver {{Welcomes You}}! {{Minimalist Location Metonymy Resolution}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for           {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gritta, Milan and Pilehvar, Mohammad Taher and Limsopatham, Nut and Collier, Nigel},
  year = {2017},
  pages = {1248--1259},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10/ggwjt7},
  abstract = {Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Gritta et al_2017_Vancouver Welcomes You.pdf}
}

@article{gritta2017a,
  title = {What's Missing in Geographical Parsing?},
  author = {Gritta, Milan and Pilehvar, Mohammad Taher and Limsopatham, Nut and Collier, Nigel},
  year = {2017},
  month = mar,
  journal = {Language Resources and Evaluation},
  volume = {52},
  number = {2},
  pages = {603--623},
  issn = {1574-020X, 1574-0218},
  doi = {10/ggwjt9},
  abstract = {Geographical data can be obtained by converting place names from freeformat text into geographical coordinates. The ability to geo-locate events in textual reports represents a valuable source of information in many real-world applications such as emergency responses, real-time social media geographical event analysis, understanding location instructions in auto-response systems and more. However, geoparsing is still widely regarded as a challenge because of domain language diversity, place name ambiguity, metonymic language and limited leveraging of context as we show in our analysis. Results to date, whilst promising, are on laboratory data and unlike in wider NLP are often not cross-compared. In this study, we evaluate and analyse the performance of a number of leading geoparsers on a number of corpora and highlight the challenges in detail. We also publish an automatically geotagged Wikipedia corpus to alleviate the dearth of (open source) corpora in this domain.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Gritta et al_2017_What's missing in geographical parsing.pdf}
}

@inproceedings{gritta2018,
  title = {Which {{Melbourne}}? {{Augmenting Geocoding}} with {{Maps}}},
  shorttitle = {Which {{Melbourne}}?},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gritta, Milan and Pilehvar, Mohammad Taher and Collier, Nigel},
  year = {2018},
  pages = {1285--1296},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10/ggwjvb},
  abstract = {The purpose of text geolocation is to associate geographic information contained in a document with a set (or sets) of coordinates, either implicitly by using linguistic features and/or explicitly by using geographic metadata combined with heuristics. We introduce a geocoder (location mention disambiguator) that achieves state-of-the-art (SOTA) results on three diverse datasets by exploiting the implicit lexical clues. Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text. To that end, we introduce the Map Vector (MapVec), a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map. We then integrate the implicit (language) and explicit (map) features to significantly improve a range of metrics. We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Gritta et al_2018_Which Melbourne.pdf}
}

@article{gritta2019,
  title = {Where Are You Talking About?},
  author = {Gritta, Milan},
  year = {2019},
  pages = {159},
  abstract = {Milan Gritta The Natural Language Processing task we focus on in this thesis is Geoparsing. Geoparsing is the process of extraction and grounding of toponyms (place names). Consider this sentence: "The victims of the Spanish earthquake off the coast of Malaga were of American and Mexican o rigin." Four toponyms will be extracted (called Geotagging) and grounded to their geographic coordinates (called Toponym Resolution). However, our research goes further than any previous work by showing how to distinguish the literal place(s) of the event (Spain, Malaga) from other linguistic types/uses such as nationalities (Mexican, American), improving downstream task accuracy. We consolidate and extend the Standard Evaluation Framework, discuss key research problems, then present concrete solutions in order to advance each stage of geoparsing. For geotagging, as well as training a SOTA neural Location-NER tagger, we simplify Metonymy Resolution with a novel minimalist feature extraction combined with an LSTM-based classifier, matching SOTA results. For toponym resolution, we deploy the latest deep learning methods to achieve SOTA performance by augmenting neural models with hitherto unused geographic features called Map Vectors. With each research project, we provide high quality datasets and system prototypes, further building resources in this field. We then show how these geoparsing advances coupled with our proposed Intra-Document Analysis can be used to associate news articles with locations in order to monitor the spread of public health threats. To this end, we evaluate our research contributions with production data from a real-time downstream application to improve geolocation of news events for disease monitoring. The data was made available to us by the Joint Research Centre (JRC), which operates one such system called MediSys that processes incoming news articles in order to monitor threats to public health and make these available to a variety of governmental, business and non-profit organisations. We also discuss steps towards an end-to-end, automated news monitoring system and make actionable recommendations for future work. In summary, the thesis aims are twofold: (1) Generate original geoparsing research aimed at advancing each stage of the pipeline by addressing pertinent challenges with concrete solutions and actionable proposals. (2) Demonstrate how this research can be applied to news event monitoring to increase the efficacy of existing biosurveillance systems, e.g. European Commission's MediSys.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Gritta_2019_Where are you talking about.pdf}
}

@article{gritta2019a,
  title = {A Pragmatic Guide to Geoparsing Evaluation: {{Toponyms}}, {{Named Entity Recognition}} and Pragmatics},
  shorttitle = {A Pragmatic Guide to Geoparsing Evaluation},
  author = {Gritta, Milan and Pilehvar, Mohammad Taher and Collier, Nigel},
  year = {2019},
  month = sep,
  journal = {Language Resources and Evaluation},
  issn = {1574-020X, 1574-0218},
  doi = {10/ggwjsn},
  abstract = {Empirical methods in geoparsing have thus far lacked a standard evaluation framework describing the task, metrics and data used to compare state-of-theart systems. Evaluation is further made inconsistent, even unrepresentative of real world usage by the lack of distinction between the different types of toponyms, which necessitates new guidelines, a consolidation of metrics and a detailed toponym taxonomy with implications for Named Entity Recognition (NER) and beyond. To address these deficiencies, our manuscript introduces a new framework in three parts. (Part 1) Task Definition: clarified via corpus linguistic analysis proposing a fine-grained Pragmatic Taxonomy of Toponyms. (Part 2) Metrics: discussed and reviewed for a rigorous evaluation including recommendations for NER/Geoparsing practitioners. (Part 3) Evaluation data: shared via a new dataset called GeoWebNews to provide test/train examples and enable immediate use of our contributions. In addition to fine-grained Geotagging and Toponym Resolution (Geocoding), this dataset is also suitable for prototyping and evaluating machine learning NLP models.},
  langid = {english},
  keywords = {TD},
  file = {/home/cjber/drive/pdf/Gritta et al_2019_A pragmatic guide to geoparsing evaluation.pdf;/home/cjber/drive/zotero/storage/P8UAQMAY/Gritta et al_2019_A pragmatic guide to geoparsing evaluation2.pdf}
}

@article{grolemund2011,
  title = {Dates and Times Made Easy with {{lubridate}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {3},
  pages = {1--25},
  doi = {10/gddk86}
}

@article{grover2010,
  title = {Use of the {{Edinburgh}} Geoparser for Georeferencing Digitized Historical Collections},
  author = {Grover, Claire and Tobin, Richard and Byrne, Kate and Woollard, Matthew and Reid, James and Dunn, Stuart and Ball, Julian},
  year = {2010},
  month = aug,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {368},
  number = {1925},
  pages = {3875--3889},
  issn = {1364-503X, 1471-2962},
  doi = {10/dq82rk},
  langid = {english},
  file = {/home/cjber/drive/pdf/Grover et al_2010_Use of the Edinburgh geoparser for georeferencing digitized historical.pdf}
}

@article{gruen,
  title = {Semi-{{Automatic Linear Feature Extraction}} by {{Dynamic Programming}} and {{LSB-Snakes}}},
  author = {Gruen, Armin and Li, Haihong},
  pages = {12},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000347},
  file = {/home/cjber/drive/pdf/ENVS492/Gruen_Li_.pdf}
}

@article{guan2013,
  title = {Partially Supervised Hierarchical Classification for Urban Features from Lidar Data with Aerial Imagery},
  author = {Guan, Haiyan and Ji, Zheng and Zhong, Liang and Li, Jonathan and Ren, Que},
  year = {2013},
  month = jan,
  journal = {International Journal of Remote Sensing},
  volume = {34},
  number = {1},
  pages = {190--210},
  issn = {0143-1161, 1366-5901},
  doi = {10/gf6d37},
  langid = {english},
  annotation = {ZSCC: 0000020},
  file = {/home/cjber/drive/pdf/ENVS492/Guan et al_2013_.pdf}
}

@book{guerrero2010,
  title = {Excel {{Data Analysis}}},
  author = {Guerrero, Hector},
  year = {2010},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-10835-8},
  isbn = {978-3-642-10834-1 978-3-642-10835-8},
  langid = {english},
  file = {/home/cjber/drive/pdf/Guerrero_2010_Excel Data Analysis.pdf}
}

@article{gusmano2018,
  title = {Population {{Aging}} and the {{Sustainability}} of the {{Welfare State}}},
  author = {Gusmano, Michael K. and Okma, Kieke G. H.},
  year = {2018},
  month = sep,
  journal = {Hastings Center Report},
  volume = {48},
  pages = {S57-S61},
  issn = {00930334},
  doi = {10/gfhhsh},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS418/gusmano2018.pdf}
}

@article{gutierrez2009,
  title = {Transport and Accessibility},
  author = {Guti{\'e}rrez, Javier},
  year = {2009},
  doi = {10/dpgjkx},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000038}
}

@inproceedings{habib2012,
  title = {Improving {{Toponym Disambiguation}} by {{Iteratively Enhancing Certainty}} of {{Extraction}}:},
  shorttitle = {Improving {{Toponym Disambiguation}} by {{Iteratively Enhancing Certainty}} of {{Extraction}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Knowledge Discovery}} and {{Information Retrieval}}},
  author = {Habib, Mena and {van Keulen}, Maurice},
  year = {2012},
  pages = {399--410},
  publisher = {{SciTePress - Science and and Technology Publications}},
  address = {{Barcelona, Spain}},
  doi = {10/ggwjtg},
  isbn = {978-989-8565-29-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Habib_van Keulen_2012_Improving Toponym Disambiguation by Iteratively Enhancing Certainty of.pdf}
}

@article{hall2011,
  title = {Interpreting Spatial Language in Image Captions},
  author = {Hall, Mark and Smart, Philip D. and Jones, Christopher B.},
  year = {2011},
  month = feb,
  journal = {Cognitive Processing},
  volume = {12},
  number = {1},
  pages = {67--94},
  issn = {1612-4782, 1612-4790},
  doi = {10.1007/s10339-010-0385-5},
  abstract = {The map as a tool for accessing data has become very popular in recent years, but a lot of data does not have the necessary spatial meta-data to allow for that. Some data such as photographs however have spatial information in their captions and if this could be extracted, then they could be made available via map-based interfaces. Towards this goal we introduce a model and spatio-linguistic reasoner for interpreting the spatial information in image captions that is based upon quantitative data about spatial language use acquired directly from people. Spatial language is inherently vague and both the model and reasoner have been designed to incorporate this vagueness at the quantitative level and not only qualitatively.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hall et al_2011_Interpreting spatial language in image captions.pdf}
}

@incollection{hall2015,
  ids = {hall2015a,hall2015b},
  title = {Spatial {{Natural Language Generation}} for {{Location Description}} in {{Photo Captions}}},
  booktitle = {Spatial {{Information Theory}}},
  author = {Hall, Mark M. and Jones, Christopher B. and Smart, Philip},
  editor = {Fabrikant, Sara Irina and Raubal, Martin and Bertolotto, Michela and Davies, Clare and Freundschuh, Scott and Bell, Scott},
  year = {2015},
  volume = {9368},
  pages = {196--223},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23374-1_10},
  abstract = {We present a spatial natural language generation system to create captions that describe the geographical context of geo-referenced photos. An analysis of existing photo captions was used to design templates representing typical caption language patterns, while the results of human subject experiments were used to create field-based spatial models of the applicability of some commonly used spatial prepositions. The language templates are instantiated with geo-data retrieved from the vicinity of the photo locations. A human subject evaluation was used to validate and to improve the spatial language generation procedure, examples of the results of which are presented in the paper.},
  isbn = {978-3-319-23373-4 978-3-319-23374-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hall et al_2015_Spatial Natural Language Generation for Location Description in Photo Captions.pdf;/home/cjber/drive/pdf/Hall et al_2015_Spatial Natural Language Generation for Location Description in Photo Captions2.pdf;/home/cjber/drive/pdf/Hall et al_2015_Spatial Natural Language Generation for Location Description in Photo Captions3.pdf}
}

@article{halterman2017,
  title = {Mordecai: {{Full}} Text Geoparsing and Event Geocoding},
  author = {Halterman, Andrew},
  year = {2017},
  journal = {The Journal of Open Source Software},
  volume = {2},
  number = {9},
  doi = {10/gf95xf}
}

@article{hamnett2003,
  title = {Gentrification and the {{Middle-class Remaking}} of {{Inner London}}, 1961-2001},
  author = {Hamnett, Chris},
  year = {2003},
  month = nov,
  journal = {Urban Studies},
  volume = {40},
  number = {12},
  pages = {2401--2426},
  issn = {0042-0980, 1360-063X},
  doi = {10/c3ww57},
  abstract = {This paper reviews the debates over the explanation of gentrification and argues that gentrification is best explained as the social and spatial manifestation of the transition from an industrial to a post-industrial economy based on financial, business and creative services, with associated changes in the nature and location of work, in the occupational class structure, earnings and incomes and the structure of the housing market. The paper sets out the links between these changes in the London context. It also examines the evidence for gentrification-induced displacement in London, arguing that it may be more appropriate to view the process partly as one of replacement.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Hamnett_2003_.pdf}
}

@article{han2012,
  title = {Geolocation {{Prediction}} in {{Social Media Data}} by {{Finding Location Indicative Words}}},
  author = {Han, Bo and Cook, Paul and Baldwin, Timothy},
  year = {2012},
  pages = {18},
  abstract = {Geolocation prediction is vital to geospatial applications like localised search and local event detection. Predominately, social media geolocation models are based on full text data, including common words with no geospatial dimension (e.g. today) and noisy strings (tmrw), potentially hampering prediction and leading to slower/more memory-intensive models. In this paper, we focus on finding location indicative words (LIWs) via feature selection, and establishing whether the reduced feature set boosts geolocation accuracy. Our results show that an information gain ratiobased approach surpasses other methods at LIW selection, outperforming state-of-the-art geolocation prediction methods by 10.6\% in accuracy and reducing the mean and median of prediction error distance by 45km and 209km, respectively, on a public dataset. We further formulate notions of prediction confidence, and demonstrate that performance is even higher in cases where our model is more confident, striking a trade-off between accuracy and coverage. Finally, the identified LIWs reveal regional language differences, which could be potentially useful for lexicographers.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Han et al_2012_Geolocation Prediction in Social Media Data by Finding Location Indicative Words.pdf}
}

@article{han2014,
  title = {Text-{{Based Twitter User Geolocation Prediction}}},
  author = {Han, Bo and Cook, Paul and Baldwin, Timothy},
  year = {2014},
  pages = {50},
  doi = {10/ggwjvf},
  abstract = {Geographical location is vital to geospatial applications like local search and event detection. In this paper, we investigate and improve on the task of text-based geolocation prediction of Twitter users. Previous studies on this topic have typically assumed that geographical references (e.g., gazetteer terms, dialectal words) in a text are indicative of its author's location. However, these references are often buried in informal, ungrammatical, and multilingual data, and are therefore non-trivial to identify and exploit. We present an integrated geolocation prediction framework and investigate what factors impact on prediction accuracy. First, we evaluate a range of feature selection methods to obtain ``location indicative words''. We then evaluate the impact of nongeotagged tweets, language, and user-declared metadata on geolocation prediction. In addition, we evaluate the impact of temporal variance on model generalisation, and discuss how users differ in terms of their geolocatability.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Han et al_2014_Text-Based Twitter User Geolocation Prediction.pdf}
}

@article{han2017,
  title = {Road Detection Based on the Fusion of {{Lidar}} and Image Data},
  author = {Han, Xiaofeng and Wang, Huan and Lu, Jianfeng and Zhao, Chunxia},
  year = {2017},
  month = nov,
  journal = {International Journal of Advanced Robotic Systems},
  volume = {14},
  number = {6},
  pages = {172988141773810},
  issn = {1729-8814, 1729-8814},
  doi = {10/gf3tdk},
  abstract = {In this article, we propose a road detection method based on the fusion of Lidar and image data under the framework of conditional random field. Firstly, Lidar point clouds are projected into the monocular images by cross calibration to get the sparse height images, and then we get high-resolution height images via a joint bilateral filter. Then, for all the training image pixels which have corresponding Lidar points, we extract their features from color image and Lidar point clouds, respectively, and use these features together with the location features to train an Adaboost classifier. After that, all the testing pixels are classified into road or non-road under a conditional random field framework. In this conditional random field framework, we use the scores computed from the Adaboost classifier as the unary potential and take the height value of each pixel and its color information into consideration together for the pairwise potential. Finally, experimental tests have been carried out on the KITTI Road data set, and the results show that our method performs well on this data set.},
  langid = {english},
  annotation = {ZSCC: 0000013},
  file = {/home/cjber/drive/pdf/ENVS492/Han et al_2017_.pdf}
}

@article{hanck2019,
  title = {Introduction to {{Econometrics}} with {{R}}},
  author = {Hanck, Christoph and Arnold, Martin and Gerber, Alexander and Schmelzer, Martin},
  year = {2019},
  pages = {392},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Hanck et al_2019_.pdf}
}

@book{harrell2015,
  title = {Regression {{Modeling Strategies}}: {{With Applications}} to {{Linear Models}}, {{Logistic}} and {{Ordinal Regression}}, and {{Survival Analysis}}},
  shorttitle = {Regression {{Modeling Strategies}}},
  author = {Harrell ,, Frank E.},
  year = {2015},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-19425-7},
  isbn = {978-3-319-19424-0 978-3-319-19425-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Harrell ,_2015_Regression Modeling Strategies.pdf}
}

@book{harron2015,
  title = {Methodological Developments in Data Linkage},
  author = {Harron, Katie and Goldstein, Harvey and Dibben, Chris},
  year = {2015},
  publisher = {{John Wiley \& Sons}},
  isbn = {1-119-07246-8}
}

@book{harvey1989,
  title = {The Urban Experience},
  author = {Harvey, David and Harvey, David},
  year = {1989},
  publisher = {{Johns Hopkins University Press Baltimore}},
  isbn = {0-8018-3849-5},
  keywords = {\#nosource}
}

@article{hasan2018,
  title = {A Survey on Real-Time Event Detection from the {{Twitter}} Data Stream},
  author = {Hasan, Mahmud and Orgun, Mehmet A and Schwitter, Rolf},
  year = {2018},
  month = aug,
  journal = {Journal of Information Science},
  volume = {44},
  number = {4},
  pages = {443--463},
  issn = {0165-5515, 1741-6485},
  doi = {10/ggnxd6},
  abstract = {The proliferation of social networking services has resulted in a rapid growth of their user base, spanning across the world. The collective information generated from these online platforms is overwhelming, in terms of both the amount of content produced every moment and the diversity of topics discussed. The real-time nature of the information produced by users has prompted researchers to analyse this content, in order to gain timely insight into the current state of affairs. Specifically, the microblogging service Twitter has been a recent focus of researchers to gather information on events occurring in real time. This article presents a survey of a wide variety of event detection methods applied to streaming Twitter data, classifying them according to shared common traits, and then discusses different aspects of the subtasks and challenges involved in event detection. We believe this survey will act as a guide and starting point for aspiring researchers to gain a structured view on state-of-the-art real-time event detection and spur further research in this direction.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hasan et al_2018_A survey on real-time event detection from the Twitter data stream.pdf}
}

@book{hassler2016,
  title = {Stochastic {{Processes}} and {{Calculus}}},
  author = {Hassler, Uwe},
  year = {2016},
  series = {Springer {{Texts}} in {{Business}} and {{Economics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23428-1},
  isbn = {978-3-319-23427-4 978-3-319-23428-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hassler_2016_Stochastic Processes and Calculus.pdf}
}

@inproceedings{hata2014,
  title = {Road Marking Detection Using {{LIDAR}} Reflective Intensity Data and Its Application to Vehicle Localization},
  booktitle = {17th {{International IEEE Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Hata, Alberto and Wolf, Denis},
  year = {2014},
  month = oct,
  pages = {584--589},
  publisher = {{IEEE}},
  address = {{Qingdao, China}},
  doi = {10/gf3tdc},
  abstract = {A correct perception of road signalizations is required for autonomous cars to follow the traffic codes. Road marking is a signalization present on road surfaces and commonly used to inform the correct lane cars must keep. Cameras have been widely used for road marking detection, however they are sensible to environment illumination. Some LIDAR sensors return infrared reflective intensity information which is insensible to illumination condition. Existing road marking detectors that analyzes reflective intensity data focus only on lane markings and ignores other types of signalization. We propose a road marking detector based on Otsu thresholding method that make possible segment LIDAR point clouds into asphalt and road marking. The results show the possibility of detecting any road marking (crosswalks, continuous lines, dashed lines). The road marking detector has also been integrated with Monte Carlo localization method so that its performance could be validated. According to the results, adding road markings onto curb maps lead to a lateral localization error of 0.3119 m.},
  isbn = {978-1-4799-6078-1},
  langid = {english},
  annotation = {ZSCC: 0000074},
  file = {/home/cjber/drive/pdf/ENVS492/Hata_Wolf_2014_.pdf}
}

@article{hatger2002,
  title = {On the Use of {{Airborne Laser Scanning Data}} to {{Verify}} and {{Enrich Road Netwirk Features}}},
  author = {Hatger, Carsten},
  year = {2002},
  pages = {6},
  abstract = {High-resolution digital terrain models (DTM) provide accurate descriptions of our surrounding with respect to its 2.5D shape. Besides that existing cartographic databases provide detailed 2D representations of topographic objects. Integrated products, which combine both planimetric and height information are rare. This paper proposes an algorithm to elaborate the outline of a road. It is founded on a well-known line simplification algorithm. Prior information is used to guide the interpretation process. Hypothesize and test methods based on support functions are used for decision-making. Robust estimation techniques are used to derive the outline of roads. Results and a discussion, which covers qualitative aspects, are given for different types of roads. The paper ends with a conclusion and a glimpse on future work.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000013},
  file = {/home/cjber/drive/pdf/ENVS492/Hatger_2002_.pdf}
}

@article{hatger2003,
  title = {Extraction of {{Road Geometry Parameters}} from {{Laser Scanning}} and {{Existing Databases}}},
  author = {Hatger, Carsten and Brenner, Claus},
  year = {2003},
  pages = {6},
  abstract = {Today's car navigation systems have reached a high level of maturity, using huge map databases with a high coverage and up-to-dateness. However, as additional applications gain importance, such as advanced driver information and warning systems, more detailed and accurate information on the true road geometry has to be incorporated into those databases. Properties like height, longitudinal and transversal slope, curvature, and width which are currently not present, have to be acquired and integrated. This article shows how existing databases either from public authorities or from private map providers can be used in combination with aerial laser scan data to derive such properties. Apart from a general discussion of the problem and our approach, first results are presented and discussed.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000065},
  file = {/home/cjber/drive/pdf/ENVS492/Hatger_Brenner_2003_.pdf}
}

@article{hatger2005,
  title = {Road Extraction by Use of Airborne Laser Scanner Data},
  author = {Hatger, Carsten},
  year = {2005},
  pages = {19},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000001},
  file = {/home/cjber/drive/pdf/ENVS492/Hatger_2005_.pdf}
}

@article{hawe2015,
  title = {Agent-Based Simulation of Emergency Response to Plan the Allocation of Resources for a Hypothetical Two-Site Major Incident},
  author = {Hawe, Glenn I. and Coates, Graham and Wilson, Duncan T. and Crouch, Roger S.},
  year = {2015},
  month = nov,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {46},
  pages = {336--345},
  issn = {09521976},
  doi = {10/f7zvmp},
  abstract = {During a major incident, the emergency services work together to ensure that those casualties who are critically injured are identified and transported to an appropriate hospital as fast as possible. If the incident is multi-site and resources are limited, the efficiency of this process is compromised as the finite resources must be shared among the multiple sites. In this paper, agent-based simulation is used to determine the allocation of resources for a two-site incident which minimizes the latest hospital arrival times for critically injured casualties. Further, how the optimal resource allocation depends on the distribution of casualties across the two sites is investigated. Such application supports the use of agentbased simulation as a tool to aid emergency response.},
  langid = {english},
  keywords = {ABI},
  file = {/home/cjber/drive/bib/pdfs/hawe_2015.pdf}
}

@article{hawkins2004,
  title = {The {{Problem}} of {{Overfitting}}},
  author = {Hawkins, Douglas M.},
  year = {2004},
  month = jan,
  journal = {Journal of Chemical Information and Computer Sciences},
  volume = {44},
  number = {1},
  pages = {1--12},
  issn = {0095-2338},
  doi = {10/b3z4jk},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hawkins_2004_The Problem of Overfitting.pdf}
}

@article{haynes2000,
  title = {Deprivation and Poor Health in Rural Areas: Inequalities Hidden by Averages},
  shorttitle = {Deprivation and Poor Health in Rural Areas},
  author = {Haynes, Robin and Gale, Susan},
  year = {2000},
  month = dec,
  journal = {Health \& Place},
  volume = {6},
  number = {4},
  pages = {275--285},
  issn = {13538292},
  doi = {10/bp3fx9},
  abstract = {Poor health and social deprivation scores in 570 wards in East Anglia, UK, were much less associated in rural than in urban areas. The deprivation measure most closely related to poor health in the least accessible rural wards was male unemployment, but use of this measure did not remove the urban{$\pm$}rural gradient of association strength. Neither did replacing wards by smaller enumeration districts as the units of analysis. The differences between urban and rural correlations were removed by restricting the comparison to wards with the same unemployment range and combining pairs of rural wards with similar deprivation values. Apparent differences between rural and urban associations are therefore not due to the choice of deprivation indices or census areas but are artifacts of the greater internal variability, smaller average deprivation range and smaller population size of rural small areas. Deprived people with poor health in rural areas are hidden by favourable averages of health and deprivation measures and do not bene\textregistered t from resource allocations based on area values. 7 2000 Elsevier Science Ltd. All rights reserved.},
  langid = {english},
  annotation = {ZSCC: 0000138},
  file = {/home/cjber/drive/pdf/ENVS492/Haynes_Gale_2000_.pdf}
}

@article{healey1992,
  title = {Rebuilding the {{City}}: {{Property-led Urban Regeneration}} ({{E}} \& {{FN Spon}}, {{London}})},
  author = {Healey, P and Davoudi, S and O'Toole, M and Tavsanoglu, S and Usher, D},
  year = {1992},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{hecht2014,
  title = {A {{Tale}} of {{Cities}}: {{Urban Biases}} in {{Volunteered Geographic Information}}},
  author = {Hecht, Brent and Stephens, Monica},
  year = {2014},
  pages = {9},
  abstract = {Geotagged tweets, Foursquare check-ins and other forms of volunteered geographic information (VGI) play a critical role in numerous studies and a large range of intelligent technologies. We show that three of the most commonly used sources of VGI \textendash{} Twitter, Flickr, and Foursquare \textendash{} are biased towards urban perspectives at the expense of rural ones. Utilizing a geostatistics-based approach, we demonstrate that, on a per capita basis, these important VGI datasets have more users, more information, and higher quality information within metropolitan areas than outside of them. VGI is a subset of user-generated content (UGC) and we discuss how our results suggest that urban biases might exist in non-geographically referenced UGC as well. Finally, because Foursquare is exclusively made up of VGI, we argue that Foursquare (and possibly other location-based social networks) has fundamentally failed to appeal to rural populations.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Hecht_Stephens_2014_A Tale of Cities.pdf}
}

@book{heiberger2015,
  title = {Statistical {{Analysis}} and {{Data Display}}},
  author = {Heiberger, Richard M. and Holland, Burt},
  year = {2015},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-2122-5},
  isbn = {978-1-4939-2121-8 978-1-4939-2122-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Heiberger_Holland_2015_Statistical Analysis and Data Display.pdf}
}

@misc{herman2019,
  title = {Different Ways of Doing {{Relation Extraction}} from Text},
  author = {Herman, Andreas},
  year = {2019},
  month = may,
  journal = {Medium},
  abstract = {Relation Extraction (RE) is the task of extracting semantic relationships from text, which usually occur between two or more entities\ldots},
  howpublished = {https://medium.com/@andreasherman/different-ways-of-doing-relation-extraction-from-text-7362b4c3169e},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/6DZSJQMA/different-ways-of-doing-relation-extraction-from-text-7362b4c3169e.html}
}

@book{heumann2016,
  title = {Introduction to {{Statistics}} and {{Data Analysis}}},
  author = {Heumann, Christian and Schomaker, Michael and {Shalabh}},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46162-5},
  isbn = {978-3-319-46160-1 978-3-319-46162-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Heumann et al_2016_Introduction to Statistics and Data Analysis.pdf}
}

@article{higgs1998,
  title = {Spatial and Temporal Variation of Mortality and Deprivation 1: Widening Health Inequalities},
  author = {Higgs, Gary and Senior, Martyn L and Williams, Huw CWL},
  year = {1998},
  journal = {Environment and planning A},
  volume = {30},
  number = {9},
  pages = {1661--1682},
  issn = {0308-518X},
  doi = {10/dqj848},
  keywords = {\#nosource}
}

@article{higgs2004,
  title = {A {{Literature Review}} of the {{Use}} of {{GIS-Based Measures}} of {{Access}} to {{Health Care Services}}},
  author = {Higgs, Gary},
  year = {2004},
  month = jun,
  journal = {Health Services and Outcomes Research Methodology},
  volume = {5},
  number = {2},
  pages = {119--139},
  issn = {1387-3741, 1572-9400},
  doi = {10/dmwm8v},
  abstract = {The increasing availability of Geographical Information Systems (GIS) in health organisations, together with the proliferation of spatially disaggregate data, has led to a number of studies that have been concerned with developing measures of access to health care services. The main aim of this paper is to review the use of GIS-based measures in exploring the relationship between geographic access, utilisation, quality and health outcomes. The varieties of approaches taken by researchers concerned with teasing out the relative importance of geographical factors that may influence access are examined. To date, in the absence of detailed data on health utilisation patterns, much of this research has focused on developing measures of potential accessibility. This paper then critically evaluates the situation with regard to the use of such measures in a broad range of accessibility studies. In particular, there has been less research to date that examines the relationship between such measures and health outcomes. In the final sections of the paper, I draw on the review to outline areas where a broader research agenda is needed, particularly in relation to more recent innovations in health care delivery.},
  langid = {english},
  annotation = {00236},
  file = {/home/cjber/drive/pdf/ENVS492/Road Assessment/Higgs_2004_.pdf}
}

@misc{highwaysengland2016,
  title = {Letter in Response to Road Width Restrictions. {{FOI}}: 734,857},
  author = {{Highways England}},
  year = {2016},
  file = {/home/cjber/drive/pdf/ENVS492/Highways England_2016_.docx}
}

@misc{highwaysengland2019,
  title = {Network Management},
  author = {{Highways England}},
  year = {2019},
  langid = {english},
  annotation = {ZSCC: 0000000},
  file = {/home/cjber/drive/pdf/ENVS492/2019_.pdf}
}

@incollection{hill2000,
  title = {Core {{Elements}} of {{Digital Gazetteers}}: {{Placenames}}, {{Categories}}, and {{Footprints}}},
  shorttitle = {Core {{Elements}} of {{Digital Gazetteers}}},
  booktitle = {Research and {{Advanced Technology}} for {{Digital Libraries}}},
  author = {Hill, Linda L.},
  editor = {Borbinha, Jos{\'e} and Baker, Thomas},
  year = {2000},
  volume = {1923},
  pages = {280--290},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45268-0_26},
  abstract = {The core elements of a digital gazetteer are the placename itself, the type of place it labels, and a geographic footprint representing its location and possibly its extent. Such gazetteer data is an important component of indirect geographic referencing through placenames. Based on the gazetteer development work of the Alexandria Digital Library, this paper presents the nature of placenames, and the process of assigning categories to places based on the words in the placenames and other information, and discusses the nature of georeferencing places with geographic footprints.},
  isbn = {978-3-540-41023-2 978-3-540-45268-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hill_2000_Core Elements of Digital Gazetteers.pdf}
}

@article{hinz2003,
  title = {Automatic Extraction of Urban Road Networks from Multi-View Aerial Imagery},
  author = {Hinz, Stefan and Baumgartner, Albert},
  year = {2003},
  month = jun,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {58},
  number = {1-2},
  pages = {83--98},
  issn = {09242716},
  doi = {10/b6gwrd},
  abstract = {In this paper, we present work on automatic road extraction from high resolution aerial imagery taken over urban areas. In order to deal with the high complexity of this type of scenes, we integrate detailed knowledge about roads and their context using explicitly formulated scale-dependent models. The knowledge about how and when certain parts of the road and context model are optimally exploited is expressed by an extraction strategy.},
  langid = {english},
  annotation = {ZSCC: 0000255},
  file = {/home/cjber/drive/pdf/ENVS492/Hinz_Baumgartner_2003_.pdf}
}

@book{ho2017,
  title = {Evaluating {{British}} Urban Policy: {{Ideology}}, Conflict and Compromise},
  author = {Ho, Suet Ying},
  year = {2017},
  publisher = {{Routledge}},
  isbn = {1-351-93815-0},
  keywords = {\#nosource}
}

@article{hodge2004,
  title = {The Economic Diversity of Rural {{England}}: Stylised Fallacies and Uncertain Evidence},
  shorttitle = {The Economic Diversity of Rural {{England}}},
  author = {Hodge, Ian and Monk, Sarah},
  year = {2004},
  month = jul,
  journal = {Journal of Rural Studies},
  volume = {20},
  number = {3},
  pages = {263--272},
  issn = {07430167},
  doi = {10/dk8svs},
  abstract = {Debate about rural policy is often based on persistent presumptions about conditions in `rural England', generally associated with economic decline, low incomes, and a lack of services. Such generalisations are rarely justified for rural areas as a whole and we term them as `stylised fallacies'. The impression of their relevance is perpetuated by the selective comparison of statistics for `urban' and `rural' areas. The paper reviews the evidence on a number of such fallacies: the economic impact of agriculture, depopulation, low incomes, rural labour markets, house prices and service provision. In each case, the position is far more complex than is commonly recognised in policy debate. The rural character of an area does not in itself offer a rationale for policy intervention. Rather, discussion could be supported through the characterisation of different types of local area. This might be approached either through statistical analysis or through qualitative analysis of emerging social and economic patterns of differentiation. In practice, each approach needs to be supported through the other.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS492/Hodge_Monk_2004_.pdf}
}

@article{hoek2008,
  title = {A Review of Land-Use Regression Models to Assess Spatial Variation of Outdoor Air Pollution},
  author = {Hoek, Gerard and Beelen, Rob and {de Hoogh}, Kees and Vienneau, Danielle and Gulliver, John and Fischer, Paul and Briggs, David},
  year = {2008},
  month = oct,
  journal = {Atmospheric Environment},
  volume = {42},
  number = {33},
  pages = {7561--7578},
  issn = {13522310},
  doi = {10/d7j2g8},
  abstract = {Studies on the health effects of long-term average exposure to outdoor air pollution have played an important role in recent health impact assessments. Exposure assessment for epidemiological studies of long-term exposure to ambient air pollution remains a difficult challenge because of substantial small-scale spatial variation. Current approaches for assessing intra-urban air pollution contrasts include the use of exposure indicator variables, interpolation methods, dispersion models and land-use regression (LUR) models. LUR models have been increasingly used in the past few years. This paper provides a critical review of the different components of LUR models.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Hoek et al_2008_.pdf}
}

@article{hoem1990,
  title = {Social {{Policy}} and {{Recent Fertility Change}} in {{Sweden}}},
  author = {Hoem, Jan M.},
  year = {1990},
  month = dec,
  journal = {Population and Development Review},
  volume = {16},
  number = {4},
  pages = {735},
  issn = {00987921},
  doi = {10/d8ctkf},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS418/hoem1990.pdf}
}

@article{hollenstein2010,
  ids = {hollenstein2010a},
  title = {Exploring Place through User-Generated Content: {{Using Flickr}} to Describe City Cores},
  shorttitle = {Exploring Place through User-Generated Content},
  author = {Hollenstein, Livia and Purves, Ross},
  year = {2010},
  month = jul,
  journal = {Journal of Spatial Information Science},
  number = {1},
  pages = {21--48},
  issn = {1948-660X},
  doi = {10/bczm5h},
  abstract = {Terms used to describe city centers, such as Downtown, are key concepts in everyday or vernacular language. Here, we explore such language by harvesting georeferenced and tagged metadata associated with 8 million Flickr images and thus consider how large numbers of people name city core areas. The nature of errors and imprecision in tagging and georeferencing are quantified, and automatically generated precision measures appear to mirror errors in the positioning of images. Users seek to ascribe appropriate semantics to images, though bulk-uploading and bulk-tagging may introduce bias. Between 0.5\textendash 2\% of tags associated with georeferenced images analyzed describe city core areas generically, while 70\% of all georeferenced images analyzed include specific place name tags, with place names at the granularity of city names being by far the most common. Using Flickr metadata, it is possible not only to describe the use of the term Downtown across the USA, but also to explore the borders of city center neighborhoods at the level of individual cities, whilst accounting for bias by the use of tag profiles.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hollenstein_Purves_2010_Exploring place through user-generated content.pdf;/home/cjber/drive/pdf/Hollenstein_Purves_2010_Exploring place through user-generated content2.pdf}
}

@article{holt2004,
  title = {Forecasting Seasonals and Trends by Exponentially Weighted Moving Averages},
  author = {Holt, Charles C.},
  year = {2004},
  month = jan,
  journal = {International Journal of Forecasting},
  volume = {20},
  number = {1},
  pages = {5--10},
  issn = {01692070},
  doi = {10/fg269b},
  abstract = {The paper provides a systematic development of the forecasting expressions for exponential weighted moving averages. Methods for series with no trend, or additive or multiplicative trend are examined. Similarly, the methods cover non-seasonal, and seasonal series with additive or multiplicative error structures. The paper is a reprinted version of the 1957 report to the Office of Naval Research (ONR 52) and is being published here to provide greater accessibility.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Holt_2004_Forecasting seasonals and trends by exponentially weighted moving averages.pdf}
}

@misc{homeoffice2016,
  title = {Future {{Control Room Improvment}}},
  author = {{Home Office}},
  year = {2016},
  howpublished = {https://assets.publishing.service.gov.uk},
  file = {/home/cjber/drive/pdf/Home Office_2016_Future Control Room Improvment.pdf}
}

@inproceedings{honnibal2015,
  title = {An {{Improved Non-monotonic Transition System}} for {{Dependency Parsing}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Honnibal, Matthew and Johnson, Mark},
  year = {2015},
  pages = {1373--1378},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10/gf3gvw},
  abstract = {Transition-based dependency parsers usually use transition systems that monotonically extend partial parse states until they identify a complete parse tree. Honnibal et al. (2013) showed that greedy onebest parsing accuracy can be improved by adding additional non-monotonic transitions that permit the parser to ``repair'' earlier parsing mistakes by ``over-writing'' earlier parsing decisions. This increases the size of the set of complete parse trees that each partial parse state can derive, enabling such a parser to escape the ``garden paths'' that can trap monotonic greedy transition-based dependency parsers.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Honnibal_Johnson_2015_An Improved Non-monotonic Transition System for Dependency Parsing.pdf}
}

@article{honnibal2017,
  title = {Spacy 2: {{Natural}} Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  journal = {To appear},
  volume = {7},
  number = {1},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{hormese2016,
  title = {Automated {{Road Extraction From High Resolution Satellite Images}}},
  author = {Hormese, Jose and Saravanan, C.},
  year = {2016},
  journal = {Procedia Technology},
  volume = {24},
  pages = {1460--1467},
  issn = {22120173},
  doi = {10/gf3tdn},
  langid = {english},
  annotation = {ZSCC: 0000004},
  file = {/home/cjber/drive/pdf/ENVS492/Hormese_Saravanan_2016_.pdf}
}

@article{hu,
  title = {Harvesting {{Big Geospatial Data}} from {{Natural Language Texts}}},
  author = {Hu, Yingjie and Adams, Benjamin},
  pages = {24},
  abstract = {A vast amount of geospatial data exists in natural language texts, such as newspapers, Wikipedia articles, social media posts, travel blogs, online reviews, and historical archives. Compared with more traditional and structured geospatial data, such as those collected by the US Geological Survey and the national statistics offices, geospatial data harvested from these unstructured texts have unique merits. They capture valuable human experiences toward places, reflect near real-time situations in different geographic areas, or record important historical information that is otherwise not available. In addition, geospatial data from these unstructured texts are often big, in terms of their volume, velocity, and variety. This chapter presents the motivations of harvesting big geospatial data from natural language texts, describes typical methods and tools for doing so, summarizes a number of existing applications, and discusses challenges and future directions.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Hu_Adams_Harvesting Big Geospatial Data from Natural Language Texts.pdf}
}

@article{hu2004,
  title = {Automatic {{Road Extraction}} from {{Dense Urban Area}} by {{Integrated Processing}} of {{Height Resolution Imagery}} and {{LiDAR Data}}},
  author = {Hu, Xiangyun and Tao, C Vincent and Hu, Yong},
  year = {2004},
  pages = {5},
  abstract = {Automated and reliable 3D city model acquisition is an increasing demand. Automatic road extraction from dense urban areas is a challenging issue due to the high complex image scene. From imagery, the obstacles of the extraction stem mainly from the difficulty of finding clues of the roads and complexity of the contextual environments. One of the promising methods to deal with this is to use data sources from multi-sensors, by which the multiple clues and constraints can be obtained so that the uncertainty can be minimized significantly. This paper focuses on the integrated processing of high resolution imagery and LIDAR (LIght Detection And Ranging) data for automatic extraction of grid structured urban road network. Under the guidance of an explicit model of the urban roads in a grid structure, the method firstly detects the primitives or clues of the roads and the contextual targets (i.e., parking lots, grasslands) both from the color image and lidar data by segmentation and image analysis. Evidences of road existing are contained in the primitives. The candidate road stripes are detected by an iterative Hough transform algorithm. This is followed by an procedure of evidence finding and validation by taking advantage of high resolution imagery and direct height information of the scene derived from lidar data. Finally the road network is formed by topology analysis. In this paper, the strategy and corresponding algorithms are described. The test data set is color ortho-imagery with 0.5 m resolution and lidar data of Toronto downtown area. The experimental results in the typical dense urban scene indicate it is able to extract the roads much more reliable and accurate by the integrated processing than by using imagery or lidar separately. It saliently exhibits advantages of the integrated processing of the multiple data sources for the road extraction from the complicated scenes.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000149},
  file = {/home/cjber/drive/pdf/ENVS492/Hu et al_.pdf}
}

@article{hu2004a,
  title = {Mining and {{Summarizing Customer Reviews}}},
  author = {Hu, Minqing and Liu, Bing},
  year = {2004},
  pages = {10},
  doi = {10/bgmfdb},
  abstract = {Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hu_Liu_2004_Mining and Summarizing Customer Reviews.pdf;/home/cjber/drive/pdf/Hu_Liu_2004_Mining and Summarizing Customer Reviews2.pdf}
}

@article{hu2019,
  title = {A Natural Language Processing and Geospatial Clustering Framework for Harvesting Local Place Names from Geotagged Housing Advertisements},
  author = {Hu, Yingjie and Mao, Huina and McKenzie, Grant},
  year = {2019},
  month = apr,
  journal = {International Journal of Geographical Information Science},
  volume = {33},
  number = {4},
  pages = {714--738},
  issn = {1365-8816, 1362-3087},
  doi = {10/gjwbfz},
  abstract = {Local place names are frequently used by residents living in a geographic region. Such place names may not be recorded in existing gazetteers, due to their vernacular nature, relative insignificance to a gazetteer covering a large area (e.g. the entire world), recent establishment (e.g. the name of a newly-opened shopping center) or other reasons. While not always recorded, local place names play important roles in many applications, from supporting public participation in urban planning to locating victims in disaster response. In this paper, we propose a computational framework for harvesting local place names from geotagged housing advertisements. We make use of those advertisements posted on local-oriented websites, such as Craigslist, where local place names are often mentioned. The proposed framework consists of two stages: natural language processing (NLP) and geospatial clustering. The NLP stage examines the textual content of housing advertisements and extracts place name candidates. The geospatial stage focuses on the coordinates associated with the extracted place name candidates and performs multiscale geospatial clustering to filter out the non-place names. We evaluate our framework by comparing its performance with those of six baselines. We also compare our result with four existing gazetteers to demonstrate the not-yet-recorded local place names discovered by our framework.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hu et al_2019_A natural language processing and geospatial clustering framework for.pdf}
}

@article{hu2019a,
  title = {A Natural Language Processing and Geospatial Clustering Framework for Harvesting Local Place Names from Geotagged Housing Advertisements},
  author = {Hu, Yingjie and Mao, Huina and McKenzie, Grant},
  year = {2019},
  month = apr,
  journal = {International Journal of Geographical Information Science},
  volume = {33},
  number = {4},
  pages = {714--738},
  issn = {1365-8816, 1362-3087},
  doi = {10/gjwbfz},
  abstract = {Local place names are frequently used by residents living in a geographic region. Such place names may not be recorded in existing gazetteers, due to their vernacular nature, relative insignificance to a gazetteer covering a large area (e.g. the entire world), recent establishment (e.g. the name of a newly-opened shopping center) or other reasons. While not always recorded, local place names play important roles in many applications, from supporting public participation in urban planning to locating victims in disaster response. In this paper, we propose a computational framework for harvesting local place names from geotagged housing advertisements. We make use of those advertisements posted on local-oriented websites, such as Craigslist, where local place names are often mentioned. The proposed framework consists of two stages: natural language processing (NLP) and geospatial clustering. The NLP stage examines the textual content of housing advertisements and extracts place name candidates. The geospatial stage focuses on the coordinates associated with the extracted place name candidates and performs multiscale geospatial clustering to filter out the non-place names. We evaluate our framework by comparing its performance with those of six baselines. We also compare our result with four existing gazetteers to demonstrate the not-yet-recorded local place names discovered by our framework.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hu et al_2019_A natural language processing and geospatial clustering framework for2.pdf}
}

@article{hu2022,
  title = {{{GazPNE}}: Annotation-Free Deep Learning for Place Name Extraction from Microblogs Leveraging Gazetteer and Synthetic Data by Rules},
  shorttitle = {{{GazPNE}}},
  author = {Hu, Xuke and {Al-Olimat}, Hussein S. and Kersten, Jens and Wiegmann, Matti and Klan, Friederike and Sun, Yeran and Fan, Hongchao},
  year = {2022},
  month = feb,
  journal = {International Journal of Geographical Information Science},
  volume = {36},
  number = {2},
  pages = {310--337},
  issn = {1365-8816, 1362-3087},
  doi = {10.1080/13658816.2021.1947507},
  abstract = {Extracting precise location information from microblogs is a crucial task in many applications, particularly in disaster response, reveal\- ing where damages are, where people need assistance, and where help can be found. A crucial prerequisite to location extrac\- tion is place name extraction. In this paper, we present GazPNE: a hybrid approach to place name extraction which fuses rules, gazetteers, and deep learning techniques without requiring any manually annotated data. The core of the approach is to learn the intrinsic characteristics of multi-word place names with deep learn\- ing from gazetteers. Specifically, GazPNE consists of a rule-based system to select n-grams from the microblogs that potentially contain place names, and a C-LSTM model that decides if the selected n-gram is a place name or not. The C-LSTM is trained on 388.1 million examples containing 6.8 million positive examples with US and Indian place names extracted from OpenStreetMap and 381.3 million negative examples synthesized by rules. We evaluate GazPNE against the SoTA on a manually annotated 4,500 tweet dataset which contains 9,026 place names from three foods: 2016 in Louisiana (US), 2016 in Houston (US), and 2015 in Chennai (India). GazPNE achieves SotA performance on the test data with an F1 of 0.84.},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/XDEWGXLU/Hu et al. - 2022 - GazPNE annotation-free deep learning for place na.pdf}
}

@article{hudson-smith2009,
  title = {{{NeoGeography}} and {{Web}} 2.0: Concepts, Tools and Applications},
  shorttitle = {{{NeoGeography}} and {{Web}} 2.0},
  author = {{Hudson-Smith}, Andrew and Crooks, Andrew and Gibin, Maurizio and Milton, Richard and Batty, Michael},
  year = {2009},
  month = jun,
  journal = {Journal of Location Based Services},
  volume = {3},
  number = {2},
  pages = {118--145},
  issn = {1748-9725, 1748-9733},
  doi = {10/fqgsfh},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hudson-Smith et al_2009_NeoGeography and Web 2.pdf}
}

@inproceedings{hughes2014,
  title = {Online Public Communications by Police \& Fire Services during the 2012 {{Hurricane Sandy}}},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hughes, Amanda L. and St. Denis, Lise A. A. and Palen, Leysia and Anderson, Kenneth M.},
  year = {2014},
  month = apr,
  pages = {1505--1514},
  publisher = {{ACM}},
  address = {{Toronto Ontario Canada}},
  doi = {10/gmf7qx},
  abstract = {Social media and other online communication tools are a subject of great interest in mass emergency response. Members of the public are turning to these solutions to seek and offer emergency information. Emergency responders are working to determine what social media policies should be in terms of their ``public information'' functions. We report on the online communications from all the coastal fire and police departments within a 100 mile radius of Hurricane Sandy's US landfall. Across four types of online communication media, we collected data from 840 fire and police departments. Findings indicate that few departments used these online channels in their Sandy response efforts, and that communications differed between fire and police departments and across media type. However, among the highly engaged departments, there is evidence that they bend and adapt policies about what constitutes appropriate public communication in the face of emergency demands; therefore, we propose that flexibility is important in considering future emergency online communication policy. We conclude with design recommendations for making online communication media more ``listenable'' for both emergency managers and members of the public.},
  isbn = {978-1-4503-2473-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hughes et al_2014_Online public communications by police & fire services during the 2012.pdf}
}

@inproceedings{hui2008,
  title = {Laser {{Intensity Used}} in {{Classification}} of {{Lidar Point Cloud Data}}},
  booktitle = {{{IGARSS}} 2008 - 2008 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Hui, Li and Di, Liping and Xianfeng, Huang and Deren, Li},
  year = {2008},
  pages = {II-1140-II-1143},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10/cmb4tq},
  abstract = {LIDAR (LIght Detection And Ranging) is a powerful remote sensing technology for the acquisition of terrain surface. The LIDAR system not only generates the 3D points cloud with irregular spacing, but also detects the laser impulse reflection data. The algorithms used for the LIDAR data are mostly used to deal with the 3D points cloud, produce the digital terrain model (DTM) and detect the objects, such as buildings. Usually, these objects must be classified as part of the extraction. In order to classify, other information besides the height information of points cloud is required, such as laser intensity information. However, few classification algorithms using intensity data have been deeply investigated. The laser intensity is different from material to material. The intensity of reflection on the same material is similar, while pulsed on different material is differ. Based on this theory, this paper provides a classification algorithm of airborne laser scanning altimetry data combined with the intensity of laser in detail.},
  isbn = {978-1-4244-2807-6},
  langid = {english},
  annotation = {ZSCC: 0000016},
  file = {/home/cjber/drive/pdf/ENVS492/Hui et al_2008_.pdf}
}

@book{hunt2019,
  title = {Advanced {{Guide}} to {{Python}} 3 {{Programming}}},
  author = {Hunt, John},
  year = {2019},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-25943-3},
  isbn = {978-3-030-25942-6 978-3-030-25943-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Hunt_2019_Advanced Guide to Python 3 Programming.pdf}
}

@incollection{hutchison2010,
  title = {Learning to {{Detect Roads}} in {{High-Resolution Aerial Images}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2010},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Mnih, Volodymyr and Hinton, Geoffrey E.},
  editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  year = {2010},
  volume = {6316},
  pages = {210--223},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15567-3_16},
  abstract = {Reliably extracting information from aerial imagery is a difficult problem with many practical applications. One specific case of this problem is the task of automatically detecting roads. This task is a difficult vision problem because of occlusions, shadows, and a wide variety of non-road objects. Despite 30 years of work on automatic road detection, no automatic or semi-automatic road detection system is currently on the market and no published method has been shown to work reliably on large datasets of urban imagery. We propose detecting roads using a neural network with millions of trainable weights which looks at a much larger context than was used in previous attempts at learning the task. The network is trained on massive amounts of data using a consumer GPU. We demonstrate that predictive performance can be substantially improved by initializing the feature detectors using recently developed unsupervised learning methods as well as by taking advantage of the local spatial coherence of the output labels. We show that our method works reliably on two challenging urban datasets that are an order of magnitude larger than what was used to evaluate previous approaches.},
  isbn = {978-3-642-15566-6 978-3-642-15567-3},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Hutchison et al_2010_.pdf}
}

@article{hutter2014,
  title = {An {{Efficient Approach}} for {{Assessing Hyperparameter Importance}}},
  author = {Hutter, Frank and Hoos, Holger and {Leyton-Brown}, Kevin},
  year = {2014},
  pages = {9},
  abstract = {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that\textemdash even in very highdimensional cases\textemdash most performance variation is attributable to just a few hyperparameters.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Hutter et al_2014_An Efficient Approach for Assessing Hyperparameter Importance.pdf}
}

@book{hylleberg1992,
  title = {Modelling Seasonality},
  author = {Hylleberg, Svend},
  year = {1992},
  publisher = {{Oxford University Press}},
  isbn = {0-19-877318-8}
}

@book{igual2017,
  title = {Introduction to {{Data Science}}},
  author = {Igual, Laura and Segu{\'i}, Santi},
  year = {2017},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-50017-1},
  isbn = {978-3-319-50016-4 978-3-319-50017-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Igual_Seguí_2017_Introduction to Data Science.pdf}
}

@article{ihaka2011,
  title = {Rnoweb: {{Literate Programming}} with and for {{R}}},
  author = {Ihaka, Ross},
  year = {2011},
  pages = {5},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000000},
  file = {/home/cjber/drive/pdf/ENVS492/Ihaka_2011_.pdf}
}

@article{ilbery2004,
  title = {Forecasting Food Supply Chain Developments in Lagging Rural Regions: Evidence from the {{UK}}},
  shorttitle = {Forecasting Food Supply Chain Developments in Lagging Rural Regions},
  author = {Ilbery, Brian and Maye, Damian and Kneafsey, Moya and Jenkins, Tim and Walkley, Catherine},
  year = {2004},
  month = jul,
  journal = {Journal of Rural Studies},
  volume = {20},
  number = {3},
  pages = {331--344},
  issn = {07430167},
  doi = {10/d5r24k},
  abstract = {Endemic problems in EU `lagging rural regions' (LRRs) are well documented and various support mechanisms have long been in place to help overcome structural difficulties. Nevertheless, new rural development architectures are now being sought and some scholars have posited that LRRs may benefit from the `quality (re)turn' in food and a relative shift from long to short food supply chains. The ways in which this `new agriculture' relates to rural development in lagging regions sound fine in theory. However, in practice it is far from clear what will actually happen, where and how. This paper attempts to answer some of these questions and, using a Delphi technique, to forecast those factors likely to influence supply chain development and performance in two LRRs in the UK: West Wales and the Scottish\textendash English Borders. The findings suggest that while most experts willingly accept the socio-economic values that can be gained by localising, shortening and synergising the food chain in LRRs, there are also important barriers that question the emergence of such an agrarian based rural development dynamic. These include the small number and size of `alternative' producers in both locales, with most still locked into industrial forms of production; the restrictive influence of bureaucracy; the shortfall of key intermediaries in both regions' food chains; and the poor provision of key physical infrastructures (e.g. roads, railway and telecommunications). The Delphi method also reveals how expert opinions about rural development in LRRs are contingent and contested, with contradictions emerging within, as well as between, rounds.},
  langid = {english},
  annotation = {ZSCC: 0000148},
  file = {/home/cjber/drive/pdf/ENVS492/Ilbery et al_2004_.pdf}
}

@article{immergluck2018,
  title = {Sustainable for Whom? {{Green}} Urban Development, Environmental Gentrification, and the {{Atlanta Beltline}}},
  shorttitle = {Sustainable for Whom?},
  author = {Immergluck, Dan and Balan, Tharunya},
  year = {2018},
  month = apr,
  journal = {Urban Geography},
  volume = {39},
  number = {4},
  pages = {546--562},
  issn = {0272-3638, 1938-2847},
  doi = {10/gfzmb4},
  abstract = {Large-scale, sustainable urban development projects can transform surrounding neighborhoods. Without precautionary policies, environmental amenities produced by these projects, such as parks, trails, walkability, and higher-density development, tend to result in higher land and housing costs. This will make it harder for a low- and moderate-income households to live near the projects, and neighborhoods are likely to become increasingly affluent. The Atlanta Beltline will ultimately connect 45 Atlanta neighborhoods via a 22-mile loop of trails, parks, and eventually a streetcar, all of which follow abandoned railroad tracks. This paper examines the effect of the Beltline on housing values within one half mile. From 2011 to 2015, depending on the segment of the Beltline, values rose between 17.9 percent and 26.6 percent more for homes within a half-mile of the Beltline than elsewhere. The implications for housing affordability and neighborhood change of projects like the Beltline, and associated policy questions, are addressed.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/immergluck2018.pdf}
}

@article{imran2013,
  title = {Extracting {{Information Nuggets}} from {{Disaster- Related Messages}} in {{Social Media}}},
  author = {Imran, Muhammad and Elbassuoni, Shady and Castillo, Carlos and Diaz, Fernando and Meier, Patrick},
  year = {2013},
  pages = {10},
  abstract = {Microblogging sites such as Twitter can play a vital role in spreading information during ``natural'' or man-made disasters. But the volume and velocity of tweets posted during crises today tend to be extremely high, making it hard for disaster-affected communities and professional emergency responders to process the information in a timely manner. Furthermore, posts tend to vary highly in terms of their subjects and usefulness; from messages that are entirely off-topic or personal in nature, to messages containing critical information that augments situational awareness. Finding actionable information can accelerate disaster response and alleviate both property and human losses. In this paper, we describe automatic methods for extracting information from microblog posts. Specifically, we focus on extracting valuable ``information nuggets'', brief, self-contained information items relevant to disaster response. Our methods leverage machine learning methods for classifying posts and information extraction. Our results, validated over one large disaster-related dataset, reveal that a careful design can yield an effective system, paving the way for more sophisticated data analysis and visualization systems.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Imran et al_2013_Extracting Information Nuggets from Disaster- Related Messages in Social Media.pdf}
}

@article{imran2015,
  title = {Processing {{Social Media Messages}} in {{Mass Emergency}}: {{A Survey}}},
  shorttitle = {Processing {{Social Media Messages}} in {{Mass Emergency}}},
  author = {Imran, Muhammad and Castillo, Carlos and Diaz, Fernando and Vieweg, Sarah},
  year = {2015},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {47},
  number = {4},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10/gdxwgx},
  abstract = {Social media platforms provide active communication channels during mass convergence and emergency events such as disasters caused by natural hazards. As a result, first responders, decision makers, and the public can use this information to gain insight into the situation as it unfolds. In particular, many social media messages communicated during emergencies convey timely, actionable information. Processing social media messages to obtain such information, however, involves solving multiple challenges including: parsing brief and informal messages, handling information overload, and prioritizing different types of information found in messages. These challenges can be mapped to classical information processing operations such as filtering, classifying, ranking, aggregating, extracting, and summarizing. We survey the state of the art regarding computational methods to process social media messages and highlight both their contributions and shortcomings. In addition, we examine their particularities, and methodically examine a series of key subproblems ranging from the detection of events to the creation of actionable and useful summaries. Research thus far has, to a large extent, produced methods to extract situational awareness information from social media. In this survey, we cover these various approaches, and highlight their benefits and shortcomings. We conclude with research challenges that go beyond situational awareness, and begin to look at supporting decision making and coordinating emergency-response actions.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Imran et al_2015_Processing Social Media Messages in Mass Emergency.pdf}
}

@inproceedings{imran2016,
  title = {Twitter as a {{Lifeline}}: {{Human-annotated Twitter Corpora}} for {{NLP}} of {{Crisis-related Messages}}},
  shorttitle = {Twitter as a {{Lifeline}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {Imran, Muhammad and Mitra, Prasenjit and Castillo, Carlos},
  year = {2016},
  month = may,
  pages = {1638--1643},
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Portoro\v{z}, Slovenia}},
  abstract = {Microblogging platforms such as Twitter provide active communication channels during mass convergence and emergency events such as earthquakes, typhoons. During the sudden onset of a crisis situation, affected people post useful information on Twitter that can be used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. Processing social media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from the incoming stream of messages and classifying them into different classes among others. One of the basic necessities of many of these tasks is the availability of data, in particular human-annotated data. In this paper, we present human-annotated Twitter corpora collected during 19 different crises that took place between 2013 and 2015. To demonstrate the utility of the annotations, we train machine learning classifiers. Moreover, we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets. To deal with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations.},
  file = {/home/cjber/drive/pdf/Imran et al_2016_Twitter as a Lifeline.pdf}
}

@article{imran2020,
  title = {Using {{AI}} and {{Social Media Multimodal Content}} for {{Disaster Response}} and {{Management}}: {{Opportunities}}, {{Challenges}}, and {{Future Directions}}},
  shorttitle = {Using {{AI}} and {{Social Media Multimodal Content}} for {{Disaster Response}} and {{Management}}},
  author = {Imran, Muhammad and Ofli, Ferda and Caragea, Doina and Torralba, Antonio},
  year = {2020},
  month = sep,
  journal = {Information Processing \& Management},
  volume = {57},
  number = {5},
  pages = {102261},
  issn = {03064573},
  doi = {10/gmf7s4},
  abstract = {People increasingly use Social Media (SM) platforms such as Twitter and Facebook during disasters and emergencies to post situational updates including reports of injured or dead people, infrastructure damage, requests of urgent needs, and the like. Information on SM comes in many forms, such as textual messages, images, and videos. Several studies have shown the utility of SM information for disaster response and management, which encouraged humanitarian organizations to start incorporating SM data sources into their workflows. However, several challenges prevent these organizations from using SM data for response efforts. These challenges include near-real-time information processing, information overload, information extraction, summarization, and verification of both textual and visual content. We highlight various applications and opportunities of SM multimodal data, latest advancements, current challenges, and future directions for the crisis informatics and other related research fields.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Imran et al_2020_Using AI and Social Media Multimodal Content for Disaster Response and.pdf;/home/cjber/drive/pdf/Imran et al_2020_Using AI and Social Media Multimodal Content for Disaster Response and2.pdf}
}

@misc{inrix2019,
  title = {{{INRIX}}},
  author = {INRIX},
  year = {2019},
  journal = {Inrix},
  abstract = {INRIX helps cities and businesses use big data to identify and solve transportation problems, making the world safer, happier and greener.},
  howpublished = {http://inrix.com/},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/zotero/storage/AAX5D9Y9/inrix.com.html}
}

@incollection{iooss2017,
  title = {Introduction to {{Sensitivity Analysis}}},
  booktitle = {Handbook of {{Uncertainty Quantification}}},
  author = {Iooss, Bertrand and Saltelli, Andrea},
  editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
  year = {2017},
  pages = {1103--1122},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-12385-1_31},
  isbn = {978-3-319-12384-4 978-3-319-12385-1},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Iooss_Saltelli_2017_.pdf}
}

@misc{iso2019,
  title = {{{ISO}} 8601-2:2019(En), {{Date}} and Time \textemdash{} {{Representations}} for Information Interchange \textemdash{} {{Part}} 2: {{Extensions}}},
  author = {{ISO}},
  year = {2019},
  howpublished = {https://www.iso.org/obp/ui/\#iso:std:iso:8601:-2:ed-1:v1:en},
  file = {/home/cjber/drive/zotero/storage/JYDRVFVF/ui.html}
}

@inproceedings{itoh2016,
  title = {Spatio-Temporal {{Event Visualization}} from a {{Geo-parsed Microblog Stream}}},
  booktitle = {Companion {{Publication}} of the 21st {{International Conference}} on {{Intelligent User Interfaces}} - {{IUI}} '16 {{Companion}}},
  author = {Itoh, Masahiko and Yoshinaga, Naoki and Toyoda, Masashi},
  year = {2016},
  pages = {58--61},
  publisher = {{ACM Press}},
  address = {{Sonoma, California, USA}},
  doi = {10/ggwjtz},
  abstract = {We devised a method of visualizing spatio-temporal events extracted from a geo-parsed microblog stream by using a multi-layered geo-locational word-cloud representation. In our method, real-time geo-parsing geo-locates posts in the stream, in order to recognize words appearing on a userspecified location and time grid as temporal local events. The recognized temporal local events (e.g., sports games) are then displayed on a map as multi-layered word-clouds and are then used for finding global events (e.g., earthquakes), in order to avoid occlusions among the local and global events. We showed the effectiveness of our method by testing it on real events extracted from our archive of five years worth of Twitter posts.},
  isbn = {978-1-4503-4140-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Itoh et al_2016_Spatio-temporal Event Visualization from a Geo-parsed Microblog Stream.pdf}
}

@inproceedings{iyengar2011,
  title = {Content-{{Based Prediction}} of {{Temporal Boundaries}} for {{Events}} in {{Twitter}}},
  booktitle = {2011 {{IEEE Third Int}}'l {{Conference}} on {{Privacy}}, {{Security}}, {{Risk}} and {{Trust}} and 2011 {{IEEE Third Int}}'l {{Conference}} on {{Social Computing}}},
  author = {Iyengar, Akshaya and Finin, Tim and Joshi, Anupam},
  year = {2011},
  month = oct,
  pages = {186--191},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10/fzz5ns},
  abstract = {Social media services like Twitter, Flickr and YouTube publish high volumes of user generated content as a major event occurs, making them a potential data source for event analysis. The large volume and noisy content of social media makes automatic preprocessing essential. Intuitively, the eventrelated data falls into three major phases: the buildup to the event, the event itself, and the post-event effects and repercussions. We describe an approach to automatically determine when an anticipated event started and ended by analyzing the content of tweets using an SVM classifier and hidden Markov model. We evaluate our performance by predicting event boundaries on Twitter data for a set of events in the domains of sports, weather and social activities.},
  isbn = {978-1-4577-1931-8 978-0-7695-4578-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Iyengar et al_2011_Content-Based Prediction of Temporal Boundaries for Events in Twitter.pdf}
}

@article{jaakkola2008,
  title = {Retrieval {{Algorithms}} for {{Road Surface Modelling Using Laser-Based Mobile Mapping}}},
  author = {Jaakkola, Anttoni and Hyypp{\"a}, Juha and Hyypp{\"a}, Hannu and Kukko, Antero},
  year = {2008},
  month = sep,
  journal = {Sensors},
  volume = {8},
  number = {9},
  pages = {5238--5249},
  issn = {1424-8220},
  doi = {10/d2p8gr},
  abstract = {Automated processing of the data provided by a laser-based mobile mapping system will be a necessity due to the huge amount of data produced. In the future, vehiclebased laser scanning, here called mobile mapping, should see considerable use for road environment modelling. Since the geometry of the scanning and point density is different from airborne laser scanning, new algorithms are needed for information extraction. In this paper, we propose automatic methods for classifying the road marking and kerbstone points and modelling the road surface as a triangulated irregular network. On the basis of experimental tests, the mean classification accuracies obtained using automatic method for lines, zebra crossings and kerbstones were 80.6\%, 92.3\% and 79.7\%, respectively.},
  langid = {english},
  annotation = {ZSCC: 0000162},
  file = {/home/cjber/drive/pdf/ENVS492/Jaakkola et al_2008_.pdf}
}

@article{jackson2013,
  title = {Assessing {{Completeness}} and {{Spatial Error}} of {{Features}} in {{Volunteered Geographic Information}}},
  author = {Jackson, Steven and Mullen, William and Agouris, Peggy and Crooks, Andrew and Croitoru, Arie and Stefanidis, Anthony},
  year = {2013},
  month = jun,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {2},
  number = {2},
  pages = {507--530},
  issn = {2220-9964},
  doi = {10/gchq83},
  abstract = {The assessment of the quality and accuracy of Volunteered Geographic Information (VGI) contributions, and by extension the ultimate utility of VGI data has fostered much debate within the geographic community. The limited research to date has been focused on VGI data of linear features and has shown that the error in the data is heterogeneously distributed. Some have argued that data produced by numerous contributors will produce a more accurate product than an individual and some research on crowd-sourced initiatives has shown that to be true, although research on VGI is more infrequent. This paper proposes a method for quantifying the completeness and accuracy of a select subset of infrastructure-associated point datasets of volunteered geographic data within a major metropolitan area using a national geospatial dataset as the reference benchmark with two datasets from volunteers used as test datasets. The results of this study illustrate the benefits of including quality control in the collection process for volunteered data.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Jackson et al_2013_Assessing Completeness and Spatial Error of Features in Volunteered Geographic.pdf}
}

@book{james2013,
  ids = {james2013a},
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  series = {Springer {{Texts}} in {{Statistics}}},
  volume = {103},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-7138-7},
  isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/James et al_2013_An Introduction to Statistical Learning.pdf;/home/cjber/drive/pdf/James et al_2013_An Introduction to Statistical Learning2.pdf}
}

@article{janowicz2011,
  title = {The Semantics of Similarity in Geographic Information Retrieval},
  author = {Janowicz, Krzysztof and Raubal, Martin and Kuhn, Werner},
  year = {2011},
  month = may,
  journal = {Journal of Spatial Information Science},
  number = {2},
  pages = {29--57},
  issn = {1948-660X},
  doi = {10/dhpht3},
  abstract = {Similarity measures have a long tradition in fields such as information retrieval, artificial intelligence, and cognitive science. Within the last years, these measures have been extended and reused to measure semantic similarity; i.e., for comparing meanings rather than syntactic differences. Various measures for spatial applications have been developed, but a solid foundation for answering what they measure, how they are best applied in information retrieval, which role contextual information plays, and how similarity values or rankings should be interpreted is still missing. It is therefore difficult to decide which measure should be used for a particular application or to compare results from different similarity theories. Based on a review of existing similarity measures, we introduce a framework to specify the semantics of similarity. We discuss similarity-based information retrieval paradigms as well as their implementation in web-based user interfaces for geographic information retrieval to demonstrate the applicability of the framework. Finally, we formulate open challenges for similarity research.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Janowicz et al_2011_The semantics of similarity in geographic information retrieval.pdf}
}

@article{janowicz2020,
  title = {{{GeoAI}}: Spatially Explicit Artificial Intelligence Techniques for Geographic Knowledge Discovery and Beyond},
  shorttitle = {{{GeoAI}}},
  author = {Janowicz, Krzysztof and Gao, Song and McKenzie, Grant and Hu, Yingjie and Bhaduri, Budhendra},
  year = {2020},
  month = apr,
  journal = {International Journal of Geographical Information Science},
  volume = {34},
  number = {4},
  pages = {625--636},
  issn = {1365-8816, 1362-3087},
  doi = {10/ggckz2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Janowicz et al_2020_GeoAI.pdf}
}

@article{jencks1990,
  title = {The Social Consequences of Growing up in a Poor Neighborhood},
  author = {Jencks, Christopher and Mayer, Susan E},
  year = {1990},
  journal = {Inner-city poverty in the United States},
  volume = {111},
  pages = {186},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{jessop2008,
  title = {Theorizing Sociospatial Relations},
  author = {Jessop, Bob and Brenner, Neil and Jones, Martin},
  year = {2008},
  journal = {Environment and Planning D: Society and Space},
  volume = {26},
  number = {3},
  pages = {389--401},
  issn = {0263-7758, 1472-3433},
  doi = {10/bfhc2j},
  abstract = {This essay seeks to reframe recent debates on sociospatial theory through the introduction of an approach that can grasp the inherently polymorphic, multidimensional character of sociospatial relations. As previous advocates of a scalar turn, we now question the privileging, in any form, of a single dimension of sociospatial processes, scalar or otherwise. We consider several recent sophisticated `turns' within critical social science; explore their methodological limitations; and highlight several important strands of sociospatial theory that seek to transcend the latter. On this basis, we argue for a more systematic recognition of polymorphy\"othe organization of sociospatial relations in multiple forms\"owithin sociospatial theory. Specifically, we suggest that territories (T), places (P), scales (S), and networks (N) must be viewed as mutually constitutive and relationally intertwined dimensions of sociospatial relations. We present this proposition as an extension of recent contributions to the spatialization of the strategic-relational approach (SRA), and we explore some of its methodological implications. We conclude by briefly illustrating the applicability of the `TPSN framework' to several realms of inquiry into sociospatial processes under contemporary capitalism.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/jessop2008.pdf}
}

@inproceedings{jin2019,
  ids = {jin2019a},
  title = {Geographic {{Entity Relationship Extraction Model Based}} on {{Piecewise Convolution}} of {{Residual Network}}},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Machine Learning}} and {{Soft Computing}}  - {{ICMLSC}} 2019},
  author = {Jin, Ying and Zhao, Shuai and Wu, Yudong},
  year = {2019},
  pages = {160--165},
  publisher = {{ACM Press}},
  address = {{Da Lat, Viet Nam}},
  doi = {10/gjwbfw},
  abstract = {Nowadays, geographic entity relationship extraction systems generally rely on artificial feature extraction. These features either require complex and complete data sets, or cannot describe deep features such as semantics. And data sets that can be used for geographic relationship extraction are scarce. To tackle these problems, this paper uses distant supervision to map existing knowledge bases into rich unstructured data which contributes to a large amount of training data. In training, this paper uses the deep residual network to extract more abstract and deeper features. Then the piecewise max pooling and selective attention mechanisms are used to further improve the accuracy of the model. Finally, the experimental results show that the deeper network and the piecewise max pooling significantly improve the extraction results.},
  isbn = {978-1-4503-6612-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Jin et al_2019_Geographic Entity Relationship Extraction Model Based on Piecewise Convolution.pdf;/home/cjber/drive/pdf/Jin et al_2019_Geographic Entity Relationship Extraction Model Based on Piecewise Convolution2.pdf;/home/cjber/drive/pdf/Jin et al_2019_Geographic Entity Relationship Extraction Model Based on Piecewise Convolution3.pdf;/home/cjber/drive/pdf/Jin et al_2019_Geographic Entity Relationship Extraction Model Based on Piecewise Convolution4.pdf;/home/cjber/drive/pdf/Jin et al_2019_Geographic Entity Relationship Extraction Model Based on Piecewise Convolution5.pdf}
}

@article{johansson2005,
  title = {Automatic {{Text-to-Scene Conversion}} in the {{Traffic Accident Domain}}},
  author = {Johansson, Richard and Berglund, Anders and Danielsson, Magnus and Nugues, Pierre},
  year = {2005},
  pages = {6},
  abstract = {In this paper, we describe a system that automatically converts narratives into 3D scenes. The texts, written in Swedish, describe road accidents. One of the program's key features is that it animates the generated scene using temporal relations between the events. We believe that this system is the first text-to-scene converter that is not restricted to invented narratives.},
  langid = {english},
  keywords = {⛔ No DOI found,TTS},
  file = {/home/cjber/drive/pdf/Johansson et al_2005_Automatic Text-to-Scene Conversion in the Traffic Accident Domain.pdf}
}

@inproceedings{johnson2009,
  title = {How the Statistical Revolution Changes (Computational) Linguistics},
  booktitle = {Proceedings of the {{EACL}} 2009 {{Workshop}} on the {{Interaction}} between {{Linguistics}} and {{Computational Linguistics Virtuous}}, {{Vicious}} or {{Vacuous}}? - {{ILCL}} '09},
  author = {Johnson, Mark},
  year = {2009},
  pages = {3--11},
  publisher = {{Association for Computational Linguistics}},
  address = {{Athens, Greece}},
  doi = {10/c5w5m9},
  abstract = {This paper discusses some of the ways that the ``statistical revolution'' has changed and continues to change the relationship between linguistics and computational linguistics. I claim that it is more useful in parsing to make an open world assumption about possible linguistic structures, rather than the closed world assumption usually made in grammar-based approaches to parsing, and I sketch two different ways in which grammar-based approaches might be modified to achieve this. I also describe some of the ways in which probabilistic models are starting to have a significant impact on psycholinguistics and language acquisition. In language acquisition Bayesian techniques may let us empirically evaluate the role of putative universals in universal grammar.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Johnson_2009_How the statistical revolution changes (computational) linguistics.pdf;/home/cjber/drive/pdf/Johnson_2009_How the statistical revolution changes (computational) linguistics2.pdf}
}

@book{johnston1983,
  title = {Classification Using Information Statistics},
  author = {Johnston, R. J. and Semple, R. K.},
  year = {1983},
  series = {Concepts and Techniques in Modern Geography},
  number = {no. 37},
  publisher = {{Geo Books}},
  address = {{Norwich [Norfolk]}},
  isbn = {978-0-86094-134-7},
  langid = {english},
  lccn = {G70.3 .J632 1983},
  file = {/home/cjber/drive/pdf/ENVS416/Johnston_Semple_1983_.pdf}
}

@article{jones1991,
  title = {Specifying and {{Estimating Multi-Level Models}} for {{Geographical Research}}},
  author = {Jones, Kelvyn},
  year = {1991},
  journal = {Transactions of the Institute of British Geographers},
  volume = {16},
  number = {2},
  pages = {148},
  issn = {00202754},
  doi = {10/djfct9},
  abstract = {Itisarguedthatmulti-levmelodelsbasedonshrinkagesetimatorespresenatconsiderabilme provemeonvtersingle-level modelsestimatebdyordinary-lesaqsutaresI.nsubstantivtermst,heML modelsallowrelationshitposvaryintimeand spaceaccordintgocontextS.hrinkageestimatomrsakeveryefficieunsteoftheinformaticoonntaineidnthehierprchical datasetsthatareestimatebdyMLmodelsA. numbeorfMLmodelfs orhouse-privceariatioanrespecifieidntermosffixed andrandoma,llowed-to-vareyff, ectEs.mpiricialllustratioonfssomeoftheseML modelsaregivenforhouse-price variatioinnSouthampton.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/jones1991.pdf}
}

@article{jones1993,
  title = {A {{Multi-level Analysis}} of the {{Variations}} in {{Domestic Property Prices}}: {{Southern England}}, 1980-87},
  shorttitle = {A {{Multi-level Analysis}} of the {{Variations}} in {{Domestic Property Prices}}},
  author = {Jones, Kelvyn and Bullen, Nina},
  year = {1993},
  month = oct,
  journal = {Urban Studies},
  volume = {30},
  number = {8},
  pages = {1409--1426},
  issn = {0042-0980, 1360-063X},
  doi = {10/bbgf3p},
  abstract = {It is argued that previous research on spatial variations in domestic property prices has failed to consider the multi-level nature of the problem . In describing underlying temporal trends and spatial patterns at the macro level, it is necessary to take account of the attributes of individual properties at the micro level. As an empirical illustration of the general approach, a number of models are calibrated for Southern England using sample data derived from building-society mortgages.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS453/Jones_Bullen_1993_.pdf}
}

@article{jones1995,
  title = {Individuals and Their Ecologies: Analysing the Geography of Chronic Illness within a Multilevel Modelling Framework},
  shorttitle = {Individuals and Their Ecologies},
  author = {Jones, Kelvyn and Duncan, Craig},
  year = {1995},
  month = mar,
  journal = {Health \& Place},
  volume = {1},
  number = {1},
  pages = {27--40},
  issn = {13538292},
  doi = {10/dkf89d},
  abstract = {This paper argues for the importance of place differences in understanding chronic illness. A conceptual distinction is drawn between individual and ecological effects and it is argued that aggregate analysis provides an inappropriate methodology for studying place differences. Multilevel modelling, in contrast, allows For the simultaneous analysis of individuals and their ecologies. This approach is applied to data derived from a nationally representative sample of over 9 000 United Kingdom individuals in nearly 400 places.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Jones_Duncan_1995_.pdf}
}

@article{jones1995a,
  title = {Emergency Medical Service Accessibility and Outcomefrom Road Traffic Accidents},
  author = {Jones, A.P. and Bentham, G.},
  year = {1995},
  month = may,
  journal = {Public Health},
  volume = {109},
  number = {3},
  pages = {169--177},
  issn = {00333506},
  doi = {10/fkf36f},
  langid = {english},
  annotation = {00108},
  file = {/home/cjber/drive/pdf/ENVS492/Road Assessment/Jones_Bentham_1995_.pdf}
}

@incollection{jones2001,
  title = {Geographical {{Information Retrieval}} with {{Ontologies}} of {{Place}}},
  booktitle = {Spatial {{Information Theory}}},
  author = {Jones, Christopher B. and Alani, Harith and Tudhope, Douglas},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Montello, Daniel R.},
  year = {2001},
  volume = {2205},
  pages = {322--335},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45424-1_22},
  abstract = {Geographical context is required of many information retrieval tasks in which the target of the search may be documents, images or records which are referenced to geographical space only by means of place names. Often there may be an imprecise match between the query name and the names associated with candidate sources of information. There is a need therefore for geographical information retrieval facilities that can rank the relevance of candidate information with respect to geographical closeness as well as semantic closeness with respect to the topic of interest. Here we present an ontology of place that combines limited coordinate data with qualitative spatial relationships between places. This parsimonious model of place is intended to support information retrieval tasks that may be global in scope. The ontology has been implemented with a semantic modelling system linking non-spatial conceptual hierarchies with the place ontology. An hierarchical distance measure is combined with Euclidean distance between place centroids to create a hybrid spatial distance measure. This can be combined with thematic distance, based on classification semantics, to create an integrated semantic closeness measure that can be used for a relevance ranking of retrieved objects.},
  isbn = {978-3-540-42613-4 978-3-540-45424-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Jones et al_2001_Geographical Information Retrieval with Ontologies of Place.pdf}
}

@article{jones2008,
  ids = {jones2008b},
  title = {Modelling Vague Places with Knowledge from the {{Web}}},
  author = {Jones, C. B. and Purves, R. S. and Clough, P. D. and Joho, H.},
  year = {2008},
  month = oct,
  journal = {International Journal of Geographical Information Science},
  volume = {22},
  number = {10},
  pages = {1045--1065},
  issn = {1365-8816, 1362-3087},
  doi = {10/cspf97},
  langid = {english},
  file = {/home/cjber/drive/pdf/Jones et al_2008_Modelling vague places with knowledge from the Web.pdf;/home/cjber/drive/pdf/Jones et al_2008_Modelling vague places with knowledge from the Web2.pdf}
}

@article{jones2008a,
  title = {Geographical Information Retrieval},
  author = {Jones, Christopher B. and Purves, Ross S.},
  year = {2008},
  month = mar,
  journal = {International Journal of Geographical Information Science},
  volume = {22},
  number = {3},
  pages = {219--228},
  issn = {1365-8816, 1362-3087},
  doi = {10/dmfgtb},
  langid = {english},
  file = {/home/cjber/drive/pdf/Jones_Purves_2008_Geographical information retrieval.pdf}
}

@incollection{jones2009,
  title = {Geographical {{Information Retrieval}}},
  booktitle = {Encyclopedia of {{Database Systems}}},
  author = {Jones, Christopher B. and Purves, Ross S.},
  editor = {LIU, LING and {\"O}ZSU, M. TAMER},
  year = {2009},
  pages = {1227--1231},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-39940-9_177},
  isbn = {978-0-387-39940-9},
  file = {/home/cjber/drive/pdf/Jones_Purves_2009_Geographical Information Retrieval.pdf}
}

@article{jones2013,
  title = {The {{Identification}} of {{Acute Stroke}}: {{An Analysis}} of {{Emergency Calls}}},
  shorttitle = {The {{Identification}} of {{Acute Stroke}}},
  author = {Jones, Stephanie P. and Carter, Bernie and Ford, Gary A. and Gibson, Josephine M. E. and Leathley, Michael J. and McAdam, Joanna J. and O'Donnell, Mark and Punekar, Shuja and Quinn, Tom and Watkins, Caroline L.},
  year = {2013},
  month = aug,
  journal = {International Journal of Stroke},
  volume = {8},
  number = {6},
  pages = {408--412},
  issn = {1747-4930, 1747-4949},
  doi = {10/f45d32},
  langid = {english},
  keywords = {ES},
  file = {/home/cjber/drive/pdf/Jones et al_2013_The Identification of Acute Stroke.pdf}
}

@article{joshi2018,
  title = {Twitter {{Sentiment Analysis System}}},
  author = {Joshi, Shaunak and Deshpande, Deepali},
  year = {2018},
  month = jun,
  journal = {International Journal of Computer Applications},
  volume = {180},
  number = {47},
  pages = {35--39},
  issn = {09758887},
  doi = {10/gdvjxg},
  abstract = {Social media is increasingly used by humans to express their feelings and opinions in the form of short text messages. Detecting sentiments in text has a wide range of applications including identifying anxiety or depression of individuals and measuring well-being or mood of a community. Sentiments can be expressed in many ways that can be seen such as facial expression and gestures, speech and by written text. Sentiment Analysis in text documents is essentially a content \textendash{} based classification problem involving concepts from the domains of Natural Language Processing as well as Machine Learning. In this paper, sentiment recognition based on textual data and the techniques used in sentiment analysis are discussed.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Joshi_Deshpande_2018_Twitter Sentiment Analysis System.pdf}
}

@article{joshi2020,
  title = {{{SpanBERT}}: {{Improving Pre-training}} by {{Representing}} and {{Predicting Spans}}},
  shorttitle = {{{SpanBERT}}},
  author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
  year = {2020},
  month = jan,
  journal = {arXiv:1907.10529 [cs]},
  eprint = {1907.10529},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-large, our single model obtains 94.6\% and 88.7\% F1 on SQuAD 1.1 and 2.0, respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6\textbackslash\% F1), strong performance on the TACRED relation extraction benchmark, and even show gains on GLUE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/Joshi et al_2020_SpanBERT.pdf;/home/cjber/drive/pdf/Joshi et al_2020_SpanBERT2.pdf}
}

@inproceedings{ju2016,
  title = {Things and Strings: Improving Place Name Disambiguation from Short Texts by Combining Entity Co-Occurrence with Topic Modeling},
  booktitle = {European {{Knowledge Acquisition Workshop}}},
  author = {Ju, Yiting and Adams, Benjamin and Janowicz, Krzysztof and Hu, Yingjie and Yan, Bo and McKenzie, Grant},
  year = {2016},
  pages = {353--367},
  publisher = {{Springer}},
  keywords = {\#nosource}
}

@article{jung2018,
  title = {Real-{{Time Road Lane Detection}} in {{Urban Areas Using LiDAR Data}}},
  author = {Jung, Jiyoung and Bae, Sung-Ho},
  year = {2018},
  month = oct,
  journal = {Electronics},
  volume = {7},
  number = {11},
  pages = {276},
  issn = {2079-9292},
  doi = {10/gf3tdm},
  abstract = {The generation of digital maps with lane-level resolution is rapidly becoming a necessity, as semi- or fully-autonomous driving vehicles are now commercially available. In this paper, we present a practical real-time working prototype for road lane detection using LiDAR data, which can be further extended to automatic lane-level map generation. Conventional lane detection methods are limited to simple road conditions and are not suitable for complex urban roads with various road signs on the ground. Given a 3D point cloud scanned by a 3D LiDAR sensor, we categorized the points of the drivable region and distinguished the points of the road signs on the ground. Then, we developed an expectation-maximization method to detect parallel lines and update the 3D line parameters in real time, as the probe vehicle equipped with the LiDAR sensor moved forward. The detected and recorded line parameters were integrated to build a lane-level digital map with the help of a GPS/INS sensor. The proposed system was tested to generate accurate lane-level maps of two complex urban routes. The experimental results showed that the proposed system was fast and practical in terms of effectively detecting road lines and generating lane-level maps.},
  langid = {english},
  annotation = {ZSCC: 0000003},
  file = {/home/cjber/drive/pdf/ENVS492/Jung_Bae_2018_.pdf}
}

@article{kahle2013,
  title = {Ggmap: {{Spatial Visualization}} with Ggplot2},
  author = {Kahle, David and Wickham, Hadley},
  year = {2013},
  journal = {The R Journal},
  volume = {5},
  number = {1},
  pages = {144--161},
  doi = {10/gf7b8x}
}

@article{kahraman2015,
  title = {Road {{Detection}} from {{High Satellite Images Using Neural Networks}}},
  author = {Kahraman, Idris and Kamil Turan, Muhammed and Rakip Karas, Ismail},
  year = {2015},
  journal = {International Journal of Modeling and Optimization},
  volume = {5},
  number = {4},
  pages = {304--307},
  issn = {20103697},
  doi = {10/gf3tdh},
  abstract = {In this paper, we propose a road detection model approach based on neural networks from satellite images. The model is based on Multilayer Perceptron (MLP) which is one of the most preferred artificial neural network architecture in classification and prediction problems. According the neural network, the RGB values are used for deciding the pixel belongs to road or not. The found road pixels are marked in the output image.},
  langid = {english},
  annotation = {ZSCC: 0000005},
  file = {/home/cjber/drive/pdf/ENVS492/Kahraman et al_2015_.pdf}
}

@article{kar2012,
  title = {A Process Oriented Areal Interpolation Technique: A Coastal County Example},
  author = {Kar, Bandana and Hodgson, Michael E},
  year = {2012},
  journal = {Cartography and Geographic Information Science},
  volume = {39},
  number = {1},
  pages = {3--16},
  issn = {1523-0406},
  doi = {10/f3zt27},
  keywords = {\#nosource}
}

@article{karimzadeh2019,
  title = {{{GeoTxt}}: {{A}} Scalable Geoparsing System for Unstructured Text Geolocation},
  shorttitle = {{{GeoTxt}}},
  author = {Karimzadeh, Morteza and Pezanowski, Scott and MacEachren, Alan M. and Wallgr{\"u}n, Jan O.},
  year = {2019},
  month = feb,
  journal = {Transactions in GIS},
  volume = {23},
  number = {1},
  pages = {118--136},
  issn = {1361-1682, 1467-9671},
  doi = {10/gf952n},
  abstract = {In this article we present GeoTxt, a scalable geoparsing system for the recognition and geolocation of place names in unstructured text. GeoTxt offers six named entity recognition (NER) algorithms for place name recognition, and utilizes an enterprise search engine for the indexing, ranking, and retrieval of toponyms, enabling scalable geoparsing for streaming text. GeoTxt offers a flexible application programming interface (API), allowing for customized attribute and/or spatial ranking of retrieved toponyms. We evaluate the system on a corpus of manually geo-annotated tweets. First, we benchmark the performance of the six NERs that GeoTxt provides access to. Second, we assess GeoTxt toponym resolution accuracy incrementally, demonstrating improvements in toponym resolution achieved (or not achieved) by adding specific heuristics and disambiguation methods. Compared to using the GeoNames web service, GeoTxt's toponym resolution demonstrates a 20\% accuracy gain. Our results show that places mentioned in the same tweet do not tend to be geographically proximate.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Karimzadeh et al_2019_GeoTxt.pdf}
}

@article{karthikeyan2001,
  title = {Tutorial on Cloth Modelling},
  author = {Karthikeyan, PS and Rnaganatan, PS},
  year = {2001},
  journal = {ACM Student Tutorial Contest, India},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000004}
}

@article{ke2009,
  title = {An Agenda for the next Generation Gazetteer: Geographic Information Contribution and Retrieval},
  author = {Ke, Carsten},
  year = {2009},
  pages = {10},
  abstract = {Gazetteers are key components of georeferenced information systems, including applications such as Web-based mapping services. Existing gazetteers lack the capabilities to fully integrate user-contributed and vernacular geographic information, as well as to support complex queries. To address these issues, a next generation gazetteer should leverage formal semantics, harvesting of implicit geographic information \textendash{} such as geotagged photos \textendash{} as well as models of trust for contributors. In this paper, we discuss these requirements in detail. We elucidate how existing standards can be integrated to realize a gazetteer infrastructure allowing for bottom-up contribution as well as information exchange between different gazetteers. We show how to ensure the quality of user-contributed information and demonstrate how to improve querying and navigation using semantics-based information retrieval.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Ke_2009_An agenda for the next generation gazetteer.pdf}
}

@incollection{kessler2009,
  title = {Bottom-{{Up Gazetteers}}: {{Learning}} from the {{Implicit Semantics}} of {{Geotags}}},
  shorttitle = {Bottom-{{Up Gazetteers}}},
  booktitle = {{{GeoSpatial Semantics}}},
  author = {Ke{\ss}ler, Carsten and Mau{\'e}, Patrick and Heuer, Jan Torben and Bartoschek, Thomas},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Janowicz, Krzysztof and Raubal, Martin and Levashkin, Sergei},
  year = {2009},
  volume = {5892},
  pages = {83--102},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-10436-7_6},
  abstract = {As directories of named places, gazetteers link the names to geographic footprints and place types. Most existing gazetteers are managed strictly top-down: entries can only be added or changed by the responsible toponymic authority. The covered vocabulary is therefore often limited to an administrative view on places, using only official place names. In this paper, we propose a bottom-up approach for gazetteer building based on geotagged photos harvested from the web. We discuss the building blocks of a geotag and how they relate to each other to formally define the notion of a geotag. Based on this formalization, we introduce an extraction process for gazetteer entries that captures the emergent semantics of collections of geotagged photos and provides a group-cognitive perspective on named places. Using an experimental setup based on clustering and filtering algorithms, we demonstrate how to identify place names and assign adequate geographic footprints. The results for three different place names (Soho, Camino de Santiago and Kilimanjaro), representing different geographic feature types, are evaluated and compared to the results obtained from traditional gazetteers. Finally, we sketch how our approach can be combined with other (for example, linguistic) approaches and discuss how such a bottom-up gazetteer can complement existing gazetteers.},
  isbn = {978-3-642-10435-0 978-3-642-10436-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Keßler et al_2009_Bottom-Up Gazetteers.pdf}
}

@article{khan2013,
  title = {Extracting {{Spatial Information From Place Descriptions}}},
  author = {Khan, Arbaz and Vasardani, Maria and Winter, Stephan},
  year = {2013},
  pages = {8},
  abstract = {A computational model of understanding place descriptions is a cardinal issue in multiple disciplines and provides critical applications especially in dialog-driven geolocation services. This research targets the automated extraction of spatial triplets to represent qualitative spatial relations between recognized places from natural language place descriptions via a simple class of locative expressions. We attempt to produce triplets, informative and convenient enough as a medium to convert verbal descriptions to graph representations of places and their relationships. We present a reasoning approach devoid of any external resources (such as maps, path geometries or robotic vision) for understanding place descriptions. We then apply our methodologies to situated place descriptions and study the results, its errors and implied future research.},
  langid = {english},
  keywords = {⛔ No DOI found,Key Paper},
  file = {/home/cjber/drive/pdf/Khan et al_2013_Extracting Spatial Information From Place Descriptions.pdf}
}

@article{kim2015,
  title = {Harvesting Large Corpora for Generating Place Graphs},
  author = {Kim, Junchul and Vasardani, Maria and Winter, Stephan},
  year = {2015},
  pages = {6},
  abstract = {This paper proposes a novel approach of harvesting large corpora from the Web for generating place graphs. The approach consists of three main phases: an efficient strategy for harvesting relevant web pages that include place descriptions related to a particular environment, extracting triplets from the descriptions, and generating place graphs from these triplets. The paper also discusses the characteristics of the generated place graphs, and identifies further challenges given the wellknown flexibility of natural language.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Kim et al_2015_Harvesting large corpora for generating place graphs.pdf}
}

@article{kim2016,
  title = {From Descriptions to Depictions: {{A}} Dynamic Sketch Map Drawing Strategy},
  shorttitle = {From Descriptions to Depictions},
  author = {Kim, Junchul and Vasardani, Maria and Winter, Stephan},
  year = {2016},
  month = jan,
  journal = {Spatial Cognition \& Computation},
  volume = {16},
  number = {1},
  pages = {29--53},
  issn = {1387-5868, 1542-7633},
  doi = {10/ggwjs5},
  abstract = {People use verbal descriptions to communicate spatial information, externalising relevant parts of their mental spatial representations. In many situations, such as emergency response, the ability of a machine to interpret these verbal descriptions could assist in human-machine interaction. As a first step in such an endeavor, this paper presents an automatic approach that translates spatial objects and their spatial relations extracted from verbal descriptions via natural language processing into a plausible sketch map. The proposed methodology applies a hierarchical and dynamic sketch map drawing strategy that is inspired by heuristics people apply in their interpretation of place descriptions, in order to accommodate underspecifying, flexible and conflicting common language. The methodology is implemented and tested. This article ends with some insights for further research towards automatic interpretation of verbal place descriptions.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kim et al_2016_From descriptions to depictions.pdf}
}

@article{kim2017,
  title = {Landmark {{Extraction}} from {{Web-Harvested Place Descriptions}}},
  author = {Kim, Junchul and Vasardani, Maria and Winter, Stephan},
  year = {2017},
  month = may,
  journal = {KI - K\"unstliche Intelligenz},
  volume = {31},
  number = {2},
  pages = {151--159},
  issn = {0933-1875, 1610-1987},
  doi = {10/ggwjth},
  abstract = {Large corpora of place descriptions provide abundant human spatial knowledge, different from the geometry-based information stored in current GIS. These place descriptions, used in everyday communication, frequently refer to landmarks. This paper suggests a model for extracting landmarks from web-harvested place descriptions, considering the landmark's cognitive significance. The model allows landmarks to be extracted according to different contexts via web harvesting and text classification methods. In this work, an implementation of our approach is used to extract context-based landmarks for a target area\textemdash Melbourne in Australia.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kim et al_2017_Landmark Extraction from Web-Harvested Place Descriptions.pdf}
}

@article{kim2018,
  title = {Social Network Analysis: {{Characteristics}} of Online Social Networks after a Disaster},
  shorttitle = {Social Network Analysis},
  author = {Kim, Jooho and Hastak, Makarand},
  year = {2018},
  month = feb,
  journal = {International Journal of Information Management},
  volume = {38},
  number = {1},
  pages = {86--96},
  issn = {02684012},
  doi = {10/gcqdv5},
  abstract = {Social media, such as Twitter and Facebook, plays a critical role in disaster management by propagating emergency information to a disaster-affected community. It ranks as the fourth most popular source for accessing emergency information. Many studies have explored social media data to understand the networks and extract critical information to develop a pre- and post-disaster mitigation plan.},
  langid = {english},
  keywords = {Disaster communication,Disaster response,Emergency information,Social media,Social network analysis (SNA)},
  file = {/home/cjber/drive/pdf/Kim_Hastak_2018_Social network analysis.pdf;/home/cjber/drive/pdf/Kim_Hastak_2018_Social network analysis2.pdf;/home/cjber/drive/zotero/storage/5GQPVPME/S026840121730525X.html}
}

@article{kingma2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/Kingma_Ba_2017_Adam.pdf}
}

@article{knuth1984,
  title = {Literate Programming},
  author = {Knuth, Donald Ervin},
  year = {1984},
  journal = {The Computer Journal},
  volume = {27},
  number = {2},
  pages = {97--111},
  issn = {1460-2067},
  doi = {10/c73jjw},
  annotation = {ZSCC: 0002155},
  file = {/home/cjber/drive/pdf/ENVS492/Knuth_1984_.pdf}
}

@inproceedings{kolomiyets2013,
  title = {{{SemEval-2013 Task}} 3: {{Spatial Role Labeling}}},
  shorttitle = {{{SemEval-2013 Task}} 3},
  booktitle = {Second {{Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (*{{SEM}}), {{Volume}} 2: {{Proceedings}} of the {{Seventh International Workshop}} on {{Semantic Evaluation}} ({{SemEval}} 2013)},
  author = {Kolomiyets, Oleksandr and Kordjamshidi, Parisa and Moens, Marie-Francine and Bethard, Steven},
  year = {2013},
  month = jun,
  pages = {255--262},
  publisher = {{Association for Computational Linguistics}},
  address = {{Atlanta, Georgia, USA}},
  file = {/home/cjber/drive/pdf/Kolomiyets et al_2013_SemEval-2013 Task 3.pdf}
}

@article{kordjamshidi2010,
  title = {Spatial {{Role Labeling}}: {{Task Definition}} and {{Annotation Scheme}}},
  author = {Kordjamshidi, Parisa and Otterlo, Martijn Van and Moens, Marie-Francine},
  year = {2010},
  pages = {8},
  abstract = {One of the essential functions of natural language is to talk about spatial relationships between objects. Linguistic constructs can express highly complex, relational structures of objects, spatial relations between them, and patterns of motion through spaces relative to some reference point. Learning how to map this information onto a formal representation from a text is a challenging problem. At present no well-defined framework for automatic spatial information extraction exists that can handle all of these issues. In this paper we introduce the task of spatial role labeling and propose an annotation scheme that is language-independent and facilitates the application of machine learning techniques. Our framework consists of a set of spatial roles based on the theory of holistic spatial semantics with the intent of covering all aspects of spatial concepts, including both static and dynamic spatial relations. We illustrate our annotation scheme with many examples throughout the paper, and in addition we highlight how to connect to spatial calculi such as region connection calculus and also how our approach fits into related work.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Kordjamshidi et al_2010_Spatial Role Labeling.pdf;/home/cjber/drive/pdf/Kordjamshidi et al_2010_Spatial Role Labeling2.pdf}
}

@article{kordjamshidi2011,
  title = {Spatial Role Labeling: {{Towards}} Extraction of Spatial Relations from Natural Language},
  shorttitle = {Spatial Role Labeling},
  author = {Kordjamshidi, Parisa and Van Otterlo, Martijn and Moens, Marie-Francine},
  year = {2011},
  month = dec,
  journal = {ACM Transactions on Speech and Language Processing},
  volume = {8},
  number = {3},
  pages = {1--36},
  issn = {15504875},
  doi = {10/b6q99m},
  langid = {english},
  keywords = {Key Paper},
  file = {/home/cjber/drive/pdf/Kordjamshidi et al_2011_Spatial role labeling.pdf}
}

@article{kordjamshidi2012,
  title = {{{SemEval-2012 Task}} 3: {{Spatial Role Labeling}}},
  author = {Kordjamshidi, Parisa and Bethard, Steven and Moens, Marie-Francine},
  year = {2012},
  pages = {9},
  abstract = {This SemEval2012 shared task is based on a recently introduced spatial annotation scheme called Spatial Role Labeling. The Spatial Role Labeling task concerns the extraction of main components of the spatial semantics from natural language: trajectors, landmarks and spatial indicators. In addition to these major components, the links between them and the general-type of spatial relationships including region, direction and distance are targeted. The annotated dataset contains about 1213 sentences which describe 612 images of the CLEF IAPR TC-12 Image Benchmark. We have one participant system with two runs. The participant's runs are compared to the system in (Kordjamshidi et al., 2011c) which is provided by task organizers.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Kordjamshidi et al_2012_SemEval-2012 Task 3.pdf}
}

@article{kordjamshidi2015,
  ids = {kordjamshidi2014},
  title = {Global Machine Learning for Spatial Ontology Population},
  author = {Kordjamshidi, Parisa and Moens, Marie-Francine},
  year = {2015},
  month = jan,
  journal = {Journal of Web Semantics},
  volume = {30},
  pages = {3--21},
  issn = {15708268},
  doi = {10/f7b2pp},
  abstract = {Understanding spatial language is important in many applications such as geographical information systems, human computer interaction or text-to-scene conversion. Due to the challenges of designing spatial ontologies, the extraction of spatial information from natural language still has to be placed in a welldefined framework. In this work, we propose an ontology which bridges between cognitive\textendash linguistic spatial concepts in natural language and multiple qualitative spatial representation and reasoning models. To make a mapping between natural language and the spatial ontology, we propose a novel global machine learning framework for ontology population. In this framework we consider relational features and background knowledge which originate from both ontological relationships between the concepts and the structure of the spatial language. The advantage of the proposed global learning model is the scalability of the inference, and the flexibility for automatically describing text with arbitrary semantic labels that form a structured ontological representation of its content. The machine learning framework is evaluated with SemEval-2012 and SemEval-2013 data from the spatial role labeling task.},
  langid = {english},
  keywords = {Key Paper,Ontology},
  file = {/home/cjber/drive/pdf/Kordjamshidi_Moens_2015_Global machine learning for spatial ontology population.pdf;/home/cjber/drive/pdf/Kordjamshidi_Moens_2015_Global machine learning for spatial ontology population2.pdf}
}

@inproceedings{kordjamshidi2017,
  title = {Spatial {{Language Understanding}} with {{Multimodal Graphs}} Using {{Declarative Learning}} Based {{Programming}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Structured Prediction}} for {{Natural}}           {{Language Processing}}},
  author = {Kordjamshidi, Parisa and Rahgooy, Taher and Manzoor, Umar},
  year = {2017},
  pages = {33--43},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10/ggwjtw},
  abstract = {This work is on a previously formalized semantic evaluation task of spatial role labeling (SpRL) that aims at extraction of formal spatial meaning from text. Here, we report the results of initial efforts towards exploiting visual information in the form of images to help spatial language understanding. We discuss the way of designing new models in the framework of declarative learning-based programming (DeLBP). The DeLBP framework facilitates combining modalities and representing various data in a unified graph. The learning and inference models exploit the structure of the unified graph as well as the global first order domain constraints beyond the data to predict the semantics which forms a structured meaning representation of the spatial context. Continuous representations are used to relate the various elements of the graph originating from different modalities. We improved over the state-of-the-art results on SpRL.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kordjamshidi et al_2017_Spatial Language Understanding with Multimodal Graphs using Declarative.pdf}
}

@incollection{kordjamshidi2017a,
  ids = {kordjamshidi2017b},
  title = {Spatial {{Role Labeling Annotation Scheme}}},
  booktitle = {Handbook of {{Linguistic Annotation}}},
  author = {Kordjamshidi, Parisa and {van Otterlo}, Martijn and Moens, Marie-Francine},
  editor = {Ide, Nancy and Pustejovsky, James},
  year = {2017},
  pages = {1025--1052},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-024-0881-2_38},
  isbn = {978-94-024-0879-9 978-94-024-0881-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kordjamshidi et al_2017_Spatial Role Labeling Annotation Scheme.pdf;/home/cjber/drive/pdf/Kordjamshidi et al_2017_Spatial Role Labeling Annotation Scheme2.pdf}
}

@article{kormilitzin2020,
  title = {Med7: A Transferable Clinical Natural Language Processing Model for Electronic Health Records},
  shorttitle = {Med7},
  author = {Kormilitzin, Andrey and Vaci, Nemanja and Liu, Qiang and {Nevado-Holgado}, Alejo},
  year = {2020},
  month = apr,
  journal = {arXiv:2003.01271 [cs]},
  eprint = {2003.01271},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The field of clinical natural language processing has been advanced significantly since the introduction of deep learning models. The self-supervised representation learning and the transfer learning paradigm became the methods of choice in many natural language processing application, in particular in the settings with the dearth of high quality manually annotated data. Electronic health record systems are ubiquitous and the majority of patients' data are now being collected electronically and in particular in the form of free text. Identification of medical concepts and information extraction is a challenging task, yet important ingredient for parsing unstructured data into structured and tabulated format for downstream analytical tasks. In this work we introduced a named-entity recognition model for clinical natural language processing. The model is trained to recognise seven categories: drug names, route, frequency, dosage, strength, form, duration. The model was first self-supervisedly pre-trained by predicting the next word, using a collection of 2 million free-text patients' records from MIMIC-III corpora and then fine-tuned on the named-entity recognition task. The model achieved a lenient (strict) micro-averaged F1 score of 0.957 (0.893) across all seven categories. Additionally, we evaluated the transferability of the developed model using the data from the Intensive Care Unit in the US to secondary care mental health records (CRIS) in the UK. A direct application of the trained NER model to CRIS data resulted in reduced performance of F1=0.762, however after fine-tuning on a small sample from CRIS, the model achieved a reasonable performance of F1=0.944. This demonstrated that despite a close similarity between the data sets and the NER tasks, it is essential to fine-tune on the target domain data in order to achieve more accurate results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/Kormilitzin et al_2020_Med7.pdf}
}

@article{kracht2008,
  title = {The Fine Structure of Spatial Expressions},
  author = {Kracht, Marcus},
  year = {2008},
  journal = {Syntax and semantics of spatial P},
  pages = {35--62},
  publisher = {{John Benjamins Amsterdam}},
  doi = {10/ggwjvg},
  keywords = {\#nosource}
}

@article{kramer2015,
  title = {Are Area-Based Initiatives Able to Improve Area Safety in Deprived Areas? {{A}} Quasi-Experimental Evaluation of the {{Dutch District Approach}}},
  shorttitle = {Are Area-Based Initiatives Able to Improve Area Safety in Deprived Areas?},
  author = {Kramer, Dani{\"e}lle and {Jongeneel-Grimen}, Birthe and Stronks, Karien and Droomers, Mari{\"e}l and Kunst, Anton E.},
  year = {2015},
  month = dec,
  journal = {BMC Public Health},
  volume = {15},
  number = {1},
  pages = {711},
  issn = {1471-2458},
  doi = {10/gb3kq7},
  abstract = {Background: Numerous area-based initiatives have been implemented in deprived areas across Western-Europe with the aim to improve the socio-economic and environmental conditions in these areas. Only few of these initiatives have been scientifically evaluated for their impact on key social determinants of health, like perceived area safety. Therefore, this study aimed to assess the impact of a Dutch area-based initiative called the District Approach on trends in perceived area safety and underlying problems in deprived target districts. Methods: A quasi-experimental design was used. Repeated cross-sectional data on perceived area safety and underlying problems were obtained from the National Safety Monitor (2005\textendash 2008) and its successor the Integrated Safety Monitor (2008\textendash 2011). Study population consisted of 133,522 Dutch adults, including 3,595 adults from target districts. Multilevel logistic regression analyses were performed to assess trends in self-reported general safety, physical order, social order, and non-victimization before and after the start of the District Approach mid-2008. Trends in target districts were compared with trends in various control groups. Results: Residents of target districts felt less safe, perceived less physical and social order, and were victimized more often than adults elsewhere in the Netherlands. For non-victimization, target districts showed a somewhat more positive change in trend after the start of the District Approach than the rest of the Netherlands or other deprived districts. Differences were only statistically significant in women, older adults, and lower educated adults. For general safety, physical order, and social order, there were no differences in trend change between target districts and control groups. Conclusions: Results suggest that the District Approach has been unable to improve perceptions of area safety and disorder in deprived areas, but that it did result in declining victimization rates.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Kramer et al_2015_.pdf}
}

@article{kraus1998,
  title = {Determination of Terrain Models in Wooded Areas with Airborne Laser Scanner Data},
  author = {Kraus, K. and Pfeifer, N.},
  year = {1998},
  month = aug,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {53},
  number = {4},
  pages = {193--203},
  issn = {09242716},
  doi = {10/c37dsh},
  abstract = {Large-scale terrain measurement in wooded areas was an unsolved problem up to now. Laser scanning solves this problem to a large extent. In this article, the characteristics of laser scanning will be compared to photogrammetry with reference to a big pilot project. Laser scanning supplies data with a skew distribution of errors because a portion of the supplied points is not on the terrain but on the treetops. Thus, the usual interpolation and filtering has to be adapted to this new data type. We will report on the implementation of this new method. The results are in accordance with the expectations. The geomorphologic quality of the contours, computed from a terrain model derived from laser scanning, needs to be improved. Solutions are still to be found. \textcopyright{} 1998 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  annotation = {ZSCC: 0001405},
  file = {/home/cjber/drive/pdf/ENVS492/Kraus_Pfeifer_1998_.pdf;/home/cjber/drive/pdf/ENVS492/Kraus_Pfeifer_1998_2.pdf}
}

@article{kravdal2001,
  title = {The {{High Fertility}} of {{College Educated Women}} in {{Norway}}: {{An Artefact}} of the {{Separate Modelling}} of {{Each Parity Transition}}},
  shorttitle = {The {{High Fertility}} of {{College Educated Women}} in {{Norway}}},
  author = {Kravdal, {\O}ystein},
  year = {2001},
  month = dec,
  journal = {Demographic Research},
  volume = {5},
  pages = {187--216},
  issn = {1435-9871},
  doi = {10/b9j2zk},
  abstract = {College education has a positive impact on birth rates, net of age and duration since previous birth, according to models estimated separately for second and third births. There are also indications of such effects on first-birth rates, in the upper 20s and 30s. Whereas a high fertility among the better-educated perhaps could be explained by socioeconomic or ideational factors, it might just as well be a result of selection. When all three parity transitions are modelled jointly, with a common unobserved factor included, negative effects of educational level appear. On the whole, the effects are less clearly negative for women born in the 1950s than for those born in the 1940s or late 1930s. The cohorts from the 1950s show educational differentials in completed fertility that are quite small and to a large extent stem from a higher proportion of childlessness among the better-educated. Second-birth progression ratios are just as high for the college educated as for women with only compulsory education, and the third-birth progression ratios differ very little. This reflects weakly negative net effects of education after first birth and spill-over effects from the higher age at first birth, counterbalanced by differential selectivity of earlier parity transitions.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS418/kravdal2001.pdf}
}

@article{kron2019,
  title = {Changes in Risk of Extreme Weather Events in {{Europe}}},
  author = {Kron, Wolfgang and L{\"o}w, Petra and Kundzewicz, Zbigniew W.},
  year = {2019},
  month = oct,
  journal = {Environmental Science \& Policy},
  volume = {100},
  pages = {74--83},
  issn = {14629011},
  doi = {10/gmhcpm},
  abstract = {Over the last decades, the damage caused by weather events has increased dramatically and ubiquitously. In Europe, weather catastrophes constitute a growing burden on national economies and insurance companies, not least because of the costs of precautionary measures. For a long time, the insurance sector has flagged that weather disasters are on the rise, both in terms of the number of occurrences and material damage caused. The main reasons for this are: increase in the number and area of settlements in exposed areas, the accumulation of ever more valuable and vulnerable assets in these areas, as well as the climate and environmental changes that have already taken place. This paper examines observed changes in risk of various categories of weather disasters in Europe, backed by statistical analyses of relevant, updated information originating from a valuable and quite unique source, Munich Re's NatCatSERVICE database, that is of considerable interest and value to the scientific community and beyond (e.g. in the reinsurance and insurance industries). The paper also calls for partnership in the reduction of risk of weather extremes and discusses the role of the insurance industry.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kron et al_2019_Changes in risk of extreme weather events in Europe.pdf}
}

@article{kropczynski2018,
  title = {Identifying {{Actionable Information}} on {{Social Media}} for {{Emergency Dispatch}}},
  author = {Kropczynski, Jess and Coche, Julien and Obeysekare, Eric and B{\'e}naben, Frederick and Grace, Rob and Halse, Shane and Montarnal, Aur{\'e}lie and Tapia, Andrea},
  year = {2018},
  pages = {11},
  abstract = {Crisis informatics researchers have taken great interest in methods to identify information relevant to crisis events posted by digital bystanders on social media. This work codifies the information needs of emergency dispatchers and first responders as a method to identify actionable information on social media. Through a design workshop with public safety professionals at a Public-Safety Answering Point (PSAP) in the United States, we develop a set of information requirements that must be satisfied to dispatch first responders and meet their immediate situational awareness needs. We then present a manual coding scheme to identify information satisfying these requirements in social media posts and apply this scheme to fictitious tweets professionals propose as actionable information to better assess ways that this information may be communicated. Finally, we propose automated methods from previous literature in the field that can be used to implement these methods in the future.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Kropczynski et al_2018_Identifying Actionable Information on Social Media for Emergency Dispatch.pdf}
}

@article{kryvasheyeu2016,
  title = {Rapid Assessment of Disaster Damage Using Social Media Activity},
  author = {Kryvasheyeu, Yury and Chen, Haohui and Obradovich, Nick and Moro, Esteban and Van Hentenryck, Pascal and Fowler, James and Cebrian, Manuel},
  year = {2016},
  month = mar,
  journal = {Science Advances},
  volume = {2},
  number = {3},
  pages = {e1500779},
  issn = {2375-2548},
  doi = {10/gc5tfp},
  abstract = {Could social media data aid in disaster response and damage assessment? Countries face both an increasing frequency and an increasing intensity of natural disasters resulting from climate change. During such events, citizens turn to social media platforms for disaster-related communication and information. Social media improves situational awareness, facilitates dissemination of emergency information, enables early warning systems, and helps coordinate relief efforts. In addition, the spatiotemporal distribution of disaster-related messages helps with the real-time monitoring and assessment of the disaster itself. We present a multiscale analysis of Twitter activity before, during, and after Hurricane Sandy. We examine the online response of 50 metropolitan areas of the United States and find a strong relationship between proximity to Sandy's path and hurricane-related social media activity. We show that real and perceived threats, together with physical disaster effects, are directly observable through the intensity and composition of Twitter's message stream. We demonstrate that per-capita Twitter activity strongly correlates with the per-capita economic damage inflicted by the hurricane. We verify our findings for a wide range of disasters and suggest that massive online social networks can be used for rapid assessment of damage caused by a large-scale disaster.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kryvasheyeu et al_2016_Rapid assessment of disaster damage using social media activity.pdf}
}

@book{kubat2017,
  title = {An {{Introduction}} to {{Machine Learning}}},
  author = {Kubat, Miroslav},
  year = {2017},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-63913-0},
  isbn = {978-3-319-63912-3 978-3-319-63913-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kubat_2017_An Introduction to Machine Learning.pdf}
}

@book{kuhn2013,
  title = {Applied {{Predictive Modeling}}},
  author = {Kuhn, Max and Johnson, Kjell},
  year = {2013},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-6849-3},
  isbn = {978-1-4614-6848-6 978-1-4614-6849-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kuhn_Johnson_2013_Applied Predictive Modeling.pdf}
}

@article{kumar2013,
  title = {An Automated Algorithm for Extracting Road Edges from Terrestrial Mobile {{LiDAR}} Data},
  author = {Kumar, Pankaj and McElhinney, Conor P. and Lewis, Paul and McCarthy, Timothy},
  year = {2013},
  month = nov,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {85},
  pages = {44--55},
  issn = {09242716},
  doi = {10/f5gjhk},
  abstract = {Terrestrial mobile laser scanning systems provide rapid and cost effective 3D point cloud data which can be used for extracting features such as the road edge along a route corridor. This information can assist road authorities in carrying out safety risk assessment studies along road networks. The knowledge of the road edge is also a prerequisite for the automatic estimation of most other road features. In this paper, we present an algorithm which has been developed for extracting left and right road edges from terrestrial mobile LiDAR data. The algorithm is based on a novel combination of two modified versions of the parametric active contour or snake model. The parameters involved in the algorithm are selected empirically and are fixed for all the road sections. We have developed a novel way of initialising the snake model based on the navigation information obtained from the mobile mapping vehicle. We tested our algorithm on different types of road sections representing rural, urban and national primary road sections. The successful extraction of road edges from these multiple road section environments validates our algorithm. These findings and knowledge provide valuable insights as well as a prototype road edge extraction toolset, for both national road authorities and survey companies.},
  langid = {english},
  annotation = {ZSCC: 0000088},
  file = {/home/cjber/drive/pdf/ENVS492/Kumar et al_2013_.pdf}
}

@article{kumar2019,
  title = {Location Reference Identification from Tweets during Emergencies: {{A}} Deep Learning Approach},
  shorttitle = {Location Reference Identification from Tweets during Emergencies},
  author = {Kumar, Abhinav and Singh, Jyoti Prakash},
  year = {2019},
  month = feb,
  journal = {International Journal of Disaster Risk Reduction},
  volume = {33},
  pages = {365--375},
  issn = {22124209},
  doi = {10.1016/j.ijdrr.2018.10.021},
  abstract = {Twitter is recently being used during crises to communicate with officials and provide rescue and relief operation in real time. The geographical location information of the event, as well as users, are vitally important in such scenarios. The identification of geographic location is one of the challenging tasks as the location information fields, such as user location and place name of tweets are not reliable. The extraction of location information from tweet text is difficult as it contains a lot of non-standard English, grammatical errors, spelling mistakes, non-standard abbreviations, and so on. This research aims to extract location words used in the tweet using a Convolutional Neural Network (CNN) based model. We achieved the exact matching score of 0.929, Hamming loss of 0.002, and F1-score of 0.96 for the tweets related to the earthquake. Our model was able to extract even three- to four-word long location references which is also evident from the exact matching score of over 92\%. The findings of this paper can help in early event localization, emergency situations, real-time road traffic management, localized advertisement, and in various location-based services.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kumar_Singh_2019_Location reference identification from tweets during emergencies.pdf}
}

@article{kumardash2020,
  title = {Multimodal {{Learning Based Spatial Relation Identification}}},
  author = {Kumar Dash, Sandeep and Sureshchandra, Y. V. and Mishra, Yatharth and Pakray, Partha and Das, Ranjita and Gelbukh, Alexander},
  year = {2020},
  month = sep,
  journal = {Computaci\'on y Sistemas},
  volume = {24},
  number = {3},
  issn = {2007-9737, 1405-5546},
  doi = {10/gjwbfx},
  abstract = {Spatial Relation identification is one of the integral parts of Spatial Information Retrieval. It deals with identifying the spatially related objects in view of their physical orientation or placement with respect to each other. The concept is widely used in many fields such as Robotics, Image Caption Generation and many more such areas. In this work the focus is to gather information from multiple modalities such as Image and its corresponding Text so as to strengthen the learning process for the identification of Spatial Relation pairs from a given text. Two different multimodal approaches are proposed in this work. In the first approach, information is explored as a sequential learning process where the individual Spatial Roles are identified as connected entities, which makes the Spatial Relation retrieval easy and efficient enough. To counter the small size of the dataset along with necessity to avoid overfitting, an efficient backward propagation based Neural Network was used to classify the candidate roles and the relations. The feature selection was different for all the classification tasks. Building on the selected feature from the first approach, the second approach uses a transfer learning method that utilizes an existing image caption generation model to retrieve the vital topic based information from image which is then used for the task. Thereby both approaches used information from two modalities which are further used to train the system in the respective approach. The model achieves state-of-the-art performance in terms of Precision for two of the Spatial Roles identification. This validates the advantage of using multimodal learning when compared with other partial-multimodal processes.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Kumar Dash et al_2020_Multimodal Learning Based Spatial Relation Identification.pdf;/home/cjber/drive/pdf/Kumar Dash et al_2020_Multimodal Learning Based Spatial Relation Identification2.pdf}
}

@book{lake2013,
  ids = {lake2013a},
  title = {Concise {{Guide}} to {{Databases}}},
  author = {Lake, Peter and Crowther, Paul},
  year = {2013},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-5601-7},
  isbn = {978-1-4471-5600-0 978-1-4471-5601-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lake_Crowther_2013_Concise Guide to Databases.pdf;/home/cjber/drive/pdf/Lake_Crowther_2013_Concise Guide to Databases2.pdf}
}

@article{lakoff1980,
  title = {The {{Metaphorical Structure}} of the {{Human Conceptual System}}},
  author = {Lakoff, George and Johnson, Mark},
  year = {1980},
  month = apr,
  journal = {Cognitive Science},
  volume = {4},
  number = {2},
  pages = {195--208},
  issn = {03640213},
  doi = {10/cxpmcn},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lakoff_Johnson_1980_The Metaphorical Structure of the Human Conceptual System.pdf}
}

@article{laliberte2012,
  title = {A Comparison of Three Feature Selection Methods for Object-Based Classification of Sub-Decimeter Resolution {{UltraCam-L}} Imagery},
  author = {Laliberte, A.S. and Browning, D.M. and Rango, A.},
  year = {2012},
  month = apr,
  journal = {International Journal of Applied Earth Observation and Geoinformation},
  volume = {15},
  pages = {70--78},
  issn = {03032434},
  doi = {10/fm5z7r},
  abstract = {The availability of numerous spectral, spatial, and contextual features with object-based image analysis (OBIA) renders the selection of optimal features a time consuming and subjective process. While several feature selection methods have been used in conjunction with OBIA, a robust comparison of the utility and efficiency of approaches would facilitate broader and more effective implementation. In this study, we evaluated three feature selection methods, (1) Jeffreys\textendash Matusita distance (JM), (2) classification tree analysis (CTA), and (3) feature space optimization (FSO) for object-based vegetation classifications with sub-decimeter digital aerial imagery in arid rangelands of the southwestern U.S. We assessed strengths, weaknesses, and best uses for each method using the criteria of ease of use, ability to rank and/or reduce input features, and classification accuracies. For the five sites tested, JM resulted in the highest overall classification accuracies for three sites, while CTA yielded highest accuracies for two sites. FSO resulted in the lowest accuracies. CTA offered ease of use and ability to rank and reduce features, while JM had the advantage of assessing class separation distances. FSO allowed for determining features relatively quickly, because it operates within the OBIA software used in this analysis (eCognition). However, the feature ranking in FSO is not transparent and accuracies were relatively low. While all methods offered an objective approach for determining suitable features for classifications of sub-decimeter resolution aerial imagery, we concluded that CTA was best suited for this particular application. We explore the limitations, assumptions, and appropriate uses for this and other datasets.},
  langid = {english},
  annotation = {ZSCC: 0000110},
  file = {/home/cjber/drive/pdf/ENVS492/Laliberte et al_2012_.pdf;/home/cjber/drive/pdf/ENVS492/Laliberte et al_2012_2.pdf}
}

@article{lample2016,
  ids = {lample2016a},
  title = {Neural {{Architectures}} for {{Named Entity Recognition}}},
  author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
  year = {2016},
  month = apr,
  journal = {arXiv:1603.01360 [cs]},
  eprint = {1603.01360},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Lample et al_2016_Neural Architectures for Named Entity Recognition.pdf;/home/cjber/drive/pdf/Lample et al_2016_Neural Architectures for Named Entity Recognition2.pdf;/home/cjber/drive/zotero/storage/9XEQA8SE/1603.html}
}

@article{lampoltshammer2012,
  title = {Natural {{Language Processing}} in {{Geographic Information Systems}}: {{Some Trends}} and {{Open Issues}}},
  author = {Lampoltshammer, Thomas J and Heistracher, Thomas},
  year = {2012},
  volume = {3},
  number = {3},
  pages = {9},
  abstract = {The increasing ubiquity of information technology motivates situative interaction mechanisms for the benefit of standard and power users. Classical interaction paradigms in geographic information systems are currently subject to amendment or even to substitution by context-aware and technology-agnostic natural-language-based interfaces. This work compiles a recent selection of radically innovative concepts found in three selected domains of the open GIS literature including related research demands. The added value herein lies in the coherence of future options and challenges in that field.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lampoltshammer_Heistracher_2012_Natural Language Processing in Geographic Information Systems.pdf}
}

@article{lan2019,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2019},
  month = oct,
  journal = {arXiv:1909.11942 [cs]},
  eprint = {1909.11942},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a selfsupervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/ google-research/google-research/tree/master/albert.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Lan et al_2019_ALBERT.pdf}
}

@book{landi2018,
  title = {Linear {{Algebra}} and {{Analytic Geometry}} for {{Physical Sciences}}},
  author = {Landi, Giovanni and Zampini, Alessandro},
  year = {2018},
  series = {Undergraduate {{Lecture Notes}} in {{Physics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-78361-1},
  isbn = {978-3-319-78360-4 978-3-319-78361-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Landi_Zampini_2018_Linear Algebra and Analytic Geometry for Physical Sciences.pdf}
}

@article{laurini2015,
  title = {Geographic {{Ontologies}}, {{Gazetteers}} and {{Multilingualism}}},
  author = {Laurini, Robert},
  year = {2015},
  month = jan,
  journal = {Future Internet},
  volume = {7},
  number = {4},
  pages = {1--23},
  issn = {1999-5903},
  doi = {10/gm8mg3},
  abstract = {Different languages imply different visions of space, so that terminologies are different in geographic ontologies. In addition to their geometric shapes, geographic features have names, sometimes different in diverse languages. In addition, the role of gazetteers, as dictionaries of place names (toponyms), is to maintain relations between place names and location. The scope of geographic information retrieval is to search for geographic information not against a database, but against the whole Internet: but the Internet stores information in different languages, and it is of paramount importance not to remain stuck to a unique language. In this paper, our first step is to clarify the links between geographic objects as computer representations of geographic features, ontologies and gazetteers designed in various languages. Then, we propose some inference rules for matching not only types, but also relations in geographic ontologies with the assistance of gazetteers.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Laurini_2015_Geographic Ontologies, Gazetteers and Multilingualism.pdf}
}

@article{lawless2004,
  title = {Locating and {{Explaining Area-Based Urban Initiatives}}: {{New Deal}} for {{Communities}} in {{England}}},
  shorttitle = {Locating and {{Explaining Area-Based Urban Initiatives}}},
  author = {Lawless, Paul},
  year = {2004},
  month = jun,
  journal = {Environment and Planning C: Government and Policy},
  volume = {22},
  number = {3},
  pages = {383--399},
  issn = {0263-774X, 1472-3425},
  doi = {10/fktprt},
  abstract = {It is now more than thirty years since the first area-based initiative (ABI) was launched in England. New Deal for Communities announced in 1998 is one of the most ambitious of English ABIs in that it aims over a period of ten years to close gaps between these thirty-nine areas and national standards in five outcome areas of crime, education, health, worklessness, and housing. Evidence gleaned from the national evaluation of 2002/03 helps illuminate trends and tensions within three themes which have proved central to the wider urban debate: community engagement, partnership working, and the complexity of ABIs. On the broad canvas, evidence from the evaluation suggests that institutional factors continue to impinge strongly on the programme, that the original assumption that partnerships should be given a strong degree of local flexibility and freedoms has been steadily eroded, and that the initiative as a whole sits within that raft of essentially reformist policy interventions effected by the Labour government since 1997.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Lawless_2004_.pdf}
}

@article{lawless2006,
  title = {Area-Based Urban Interventions: Rationale and Outcomes: The {{New Deal}} for {{Communities}} Programme in {{England}}},
  author = {Lawless, Paul},
  year = {2006},
  journal = {Urban studies},
  volume = {43},
  number = {11},
  pages = {1991--2011},
  issn = {0042-0980},
  doi = {10/bn99x9},
  keywords = {\#nosource}
}

@article{laylavi2016,
  title = {A {{Multi-Element Approach}} to {{Location Inference}} of {{Twitter}}: {{A Case}} for {{Emergency Response}}},
  shorttitle = {A {{Multi-Element Approach}} to {{Location Inference}} of {{Twitter}}},
  author = {Laylavi, Farhad and Rajabifard, Abbas and Kalantari, Mohsen},
  year = {2016},
  month = may,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {5},
  number = {5},
  pages = {56},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10/f8v96g},
  abstract = {Since its inception, Twitter has played a major role in real-world events\textemdash especially in the aftermath of disasters and catastrophic incidents, and has been increasingly becoming the first point of contact for users wishing to provide or seek information about such situations. The use of Twitter in emergency response and disaster management opens up avenues of research concerning different aspects of Twitter data quality, usefulness and credibility. A real challenge that has attracted substantial attention in the Twitter research community exists in the location inference of twitter data. Considering that less than 2\% of tweets are geotagged, finding location inference methods that can go beyond the geotagging capability is undoubtedly the priority research area. This is especially true in terms of emergency response, where spatial aspects of information play an important role. This paper introduces a multi-elemental location inference method that puts the geotagging aside and tries to predict the location of tweets by exploiting the other inherently attached data elements. In this regard, textual content, users' profile location and place labelling, as the main location-related elements, are taken into account. Location-name classes in three granularity levels are defined and employed to look up the location references from the location-associated elements. The inferred location of the finest granular level is assigned to a tweet, based on a novel location assignment rule. The location assigned by the location inference process is considered to be the inferred location of a tweet, and is compared with the geotagged coordinates as the ground truth of the study. The results show that this method is able to successfully infer the location of 87\% of the tweets at the average distance error of 12.2 km and the median distance error of 4.5 km, which is a significant improvement compared with that of the current methods that can predict the location with much larger distance errors or at a city-level resolution at best.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {emergency response,location inference,social media,twitter},
  file = {/home/cjber/drive/pdf/Laylavi et al_2016_A Multi-Element Approach to Location Inference of Twitter.pdf;/home/cjber/drive/zotero/storage/CIYJPY4Y/56.html}
}

@article{layton2012,
  title = {Stopping Sight Distance},
  author = {Layton, Robert and Dixon, Karen},
  year = {2012},
  journal = {Kiewit Center for Infrastructure and Transportation, Oregon Department of Transportation},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000037},
  file = {/home/cjber/drive/pdf/ENVS492/Layton_Dixon_2012_.pdf}
}

@book{lee2015,
  title = {Data {{Structures}} and {{Algorithms}} with {{Python}}},
  author = {Lee, Kent D. and Hubbard, Steve},
  year = {2015},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-13072-9},
  isbn = {978-3-319-13071-2 978-3-319-13072-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lee_Hubbard_2015_Data Structures and Algorithms with Python.pdf}
}

@book{lee2017,
  title = {Foundations of {{Programming Languages}}},
  author = {Lee, Kent D.},
  year = {2017},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-70790-7},
  isbn = {978-3-319-70789-1 978-3-319-70790-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lee_2017_Foundations of Programming Languages.pdf}
}

@inproceedings{lee2018,
  title = {Higher-{{Order Coreference Resolution}} with {{Coarse-to-Fine Inference}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  author = {Lee, Kenton and He, Luheng and Zettlemoyer, Luke},
  year = {2018},
  pages = {687--692},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10/gf6gbr},
  abstract = {We introduce a fully differentiable approximation to higher-order inference for coreference resolution. Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting accuracy. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lee et al_2018_Higher-Order Coreference Resolution with Coarse-to-Fine Inference.pdf;/home/cjber/drive/pdf/Lee et al_2018_Higher-Order Coreference Resolution with Coarse-to-Fine Inference2.pdf}
}

@book{lees2008,
  title = {Gentrification},
  author = {Lees, Loretta and Slater, Tom and Wyly, Elvin K.},
  year = {2008},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  isbn = {978-0-415-95036-7 978-0-415-95037-4},
  langid = {english},
  annotation = {OCLC: 255362224},
  file = {/home/cjber/drive/pdf/ENVS416/Lees et al_2008_.pdf}
}

@article{leetaru2013,
  title = {Mapping the Global {{Twitter}} Heartbeat: {{The}} Geography of {{Twitter}}},
  author = {Leetaru, Kalev and Wang, Shaowen and Cao, Guofeng and Padmanabhan, Anand and Shook, Eric},
  year = {2013},
  journal = {First Monday},
  issn = {1396-0466},
  doi = {10.5210/fm.v18i5.4366}
}

@article{leidner,
  title = {Towards a {{Reference Corpus}} for {{Automatic Toponym Resolution Evaluation}}},
  author = {Leidner, Jochen L},
  pages = {7},
  abstract = {Spatial named entities ground events in space, and this relationship is essential for advanced text processing applications such as question answering and event tracking. Toponym resolution is the task of mapping from an entity to a spatial representation (an extensional coordinate model), given the context. Whereas work on the temporal dimension is ongoing [17], to date no reference corpus exists to evaluate competing algorithms for toponym resolution.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Leidner_Towards a Reference Corpus for Automatic Toponym Resolution Evaluation.pdf}
}

@article{leidner2007,
  ids = {leidner2007a},
  title = {Toponym Resolution in Text: Annotation, Evaluation and Applications of Spatial Grounding},
  shorttitle = {Toponym Resolution in Text},
  author = {Leidner, Jochen L.},
  year = {2007},
  month = dec,
  journal = {ACM SIGIR Forum},
  volume = {41},
  number = {2},
  pages = {124--126},
  issn = {0163-5840},
  doi = {10/dd8dv2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Leidner_2007_Toponym resolution in text.pdf;/home/cjber/drive/pdf/Leidner_2007_Toponym resolution in text2.pdf}
}

@article{leidner2011,
  ids = {leidner2011a},
  title = {Detecting Geographical References in the Form of Place Names and Associated Spatial Natural Language},
  author = {Leidner, Jochen L and Lieberman, Michael D},
  year = {2011},
  journal = {SIGSPATIAL Special},
  volume = {3},
  number = {2},
  pages = {5--11},
  doi = {10/cwvm2v},
  keywords = {Geoparsing,Key Paper,NLP},
  file = {/home/cjber/drive/pdf/Leidner_Lieberman_2011_Detecting geographical references in the form of place names and associated.pdf;/home/cjber/drive/pdf/Leidner_Lieberman_2011_Detecting geographical references in the form of place names and associated2.pdf;/home/cjber/drive/pdf/Leidner_Lieberman_2011_Detecting geographical references in the form of place names and associated3.pdf;/home/cjber/drive/pdf/Leidner_Lieberman_2011_Detecting geographical references in the form of place names and associated4.pdf}
}

@article{leon1997,
  title = {Huge Variation in {{Russian}} Mortality Rates 1984\textendash 94: Artefact, Alcohol, or What?},
  author = {Leon, David A and Chenet, Laurent and Shkolnikov, Vladimir M and Zakharov, Sergei and Shapiro, Judith and Rakhmanova, Galina and Vassin, Sergei and McKee, Martin},
  year = {1997},
  journal = {The lancet},
  volume = {350},
  number = {9075},
  pages = {383--388},
  issn = {0140-6736},
  doi = {10/dpnsk3},
  keywords = {\#nosource}
}

@article{leon1998,
  title = {Social Stress and the {{Russian}} Mortality Crisis},
  author = {Leon, David A and Shkolnikov, Vladimir M},
  year = {1998},
  journal = {JAMA},
  volume = {279},
  number = {10},
  pages = {790--791},
  issn = {0098-7484},
  doi = {10/bh3c49},
  keywords = {\#nosource}
}

@article{leveling2008,
  ids = {leveling2008a},
  title = {On Metonymy Recognition for Geographic Information Retrieval},
  author = {Leveling, Johannes and Hartrumpf, Sven},
  year = {2008},
  month = mar,
  journal = {International Journal of Geographical Information Science},
  volume = {22},
  number = {3},
  pages = {289--299},
  issn = {1365-8816, 1362-3087},
  doi = {10/c8cdt3},
  langid = {english},
  keywords = {Ontology},
  file = {/home/cjber/drive/pdf/Leveling_Hartrumpf_2008_On metonymy recognition for geographic information retrieval.pdf}
}

@article{li,
  title = {Geospatial {{Information Technology For Emergency Response}}},
  author = {Li, Siyka Zlatanova Jonathan},
  pages = {2},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Li_Geospatial Information Technology For Emergency Response.pdf;/home/cjber/drive/pdf/Li_Geospatial Information Technology For Emergency Response2.pdf}
}

@article{li2006,
  title = {Exploring {{Probabilistic Toponym Resolution}} for {{Geographical Information Retrieval}}},
  author = {Li, Yi and Moffat, Alistair and Stokes, Nicola and Cavedon, Lawrence},
  year = {2006},
  pages = {7},
  abstract = {A key problem that arises when unstructured text is being queried is that of properly recognizing and exploiting geographical terms and entities. Here we describe a mechanism for probabilistic toponym resolution, and our experiments with the new method in the setting of the 2005 GeoCLEF queries and judgments. The new method gives improved retrieval effectiveness on a subset of the topics.},
  langid = {english},
  keywords = {⛔ No DOI found,TD},
  file = {/home/cjber/drive/pdf/Li et al_2006_Exploring Probabilistic Toponym Resolution for Geographical Information.pdf}
}

@article{li2012,
  title = {Are {{Linear Regression Techniques Appropriate}} for {{Analysis When}} the {{Dependent}} ({{Outcome}}) {{Variable Is Not Normally Distributed}}?},
  author = {Li, Xiang and Wong, Wanling and Lamoureux, Ecosse L. and Wong, Tien Y},
  year = {2012},
  month = may,
  journal = {Investigative Opthalmology \& Visual Science},
  volume = {53},
  number = {6},
  pages = {3082},
  issn = {1552-5783},
  doi = {10/gf33rw},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS453/Li et al_2012_.pdf}
}

@inproceedings{li2014,
  title = {Fine-Grained Location Extraction from Tweets with Temporal Awareness},
  booktitle = {Proceedings of the 37th International {{ACM SIGIR}} Conference on {{Research}} \& Development in Information Retrieval - {{SIGIR}} '14},
  author = {Li, Chenliang and Sun, Aixin},
  year = {2014},
  pages = {43--52},
  publisher = {{ACM Press}},
  address = {{Gold Coast, Queensland, Australia}},
  doi = {10/ggwjs4},
  abstract = {Twitter is a popular platform for sharing activities, plans, and opinions. Through tweets, users often reveal their location information and short term visiting plans. In this paper, we are interested in extracting fine-grained locations mentioned in tweets with temporal awareness. More specifically, we like to extract each point-ofinterest (POI) mention in a tweet and predict whether the user has visited, is currently at, or will soon visit this POI. Our proposed solution, named Petar, consists of two main components: a POI inventory and a time-aware POI tagger. The POI inventory is built by exploiting the crowd wisdom of Foursquare community. It contains not only the formal names of POIs but also the informal abbreviations. The POI tagger, based on Conditional Random Field (CRF) model, is designed to simultaneously identify the POIs and resolve their associated temporal awareness. In our experiments, we investigated four types of features (i.e., lexical, grammatical, geographical, and BILOU schema features) for time-aware POI extraction. With the four types of features, Petar achieves promising extraction accuracy and outperforms all baseline methods.},
  isbn = {978-1-4503-2257-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Li_Sun_2014_Fine-grained location extraction from tweets with temporal awareness.pdf}
}

@article{li2016,
  title = {An {{Efficient Method}} for {{Automatic Road Extraction Based}} on {{Multiple Features}} from {{LiDAR Data}}},
  author = {Li, Y. and Hu, X. and Guan, H. and Liu, P.},
  year = {2016},
  month = jun,
  journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {XLI-B3},
  pages = {289--293},
  issn = {2194-9034},
  doi = {10/gf3tdf},
  abstract = {The road extraction in urban areas is difficult task due to the complicated patterns and many contextual objects. LiDAR data directly provides three dimensional (3D) points with less occlusions and smaller shadows. The elevation information and surface roughness are distinguishing features to separate roads. However, LiDAR data has some disadvantages are not beneficial to object extraction, such as the irregular distribution of point clouds and lack of clear edges of roads. For these problems, this paper proposes an automatic road centerlines extraction method which has three major steps: (1) road center point detection based on multiple feature spatial clustering for separating road points from ground points, (2) local principal component analysis with least squares fitting for extracting the primitives of road centerlines, and (3) hierarchical grouping for connecting primitives into complete roads network. Compared with MTH (consist of Mean shift algorithm, Tensor voting, and Hough transform) proposed in our previous article, this method greatly reduced the computational cost. To evaluate the proposed method, the Vaihingen data set, a benchmark testing data provided by ISPRS for ``Urban Classification and 3D Building Reconstruction'' project, was selected. The experimental results show that our method achieve the same performance by less time in road extraction using LiDAR data.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS492/Li et al_2016_.pdf}
}

@article{li2018,
  title = {Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter Optimization}}},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  pages = {52},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Li et al_2018_Hyperband.pdf}
}

@article{li2018a,
  title = {A {{Galerkin}} Energy-Preserving Method for Two Dimensional Nonlinear {{Schr\"odinger}} Equation},
  author = {Li, Haochen and Jiang, Chaolong and Lv, Zhongquan},
  year = {2018},
  month = may,
  journal = {Applied Mathematics and Computation},
  volume = {324},
  pages = {16--27},
  issn = {0096-3003},
  doi = {10/gmjwxm},
  abstract = {In this paper, a Galerkin energy-preserving scheme is proposed for solving nonlinear Schr\"odinger equation in two dimensions. The nonlinear Schr\"odinger equation is first rewritten as an infinite-dimensional Hamiltonian system. Following the method of lines, the spatial derivatives of the nonlinear Schr\"odinger equation are approximated with the aid of the Galerkin methods. The resulting ordinary differential equations can be cast into a canonical Hamiltonian system. A fully-discretized scheme is then devised by considering an average vector field method in time. Moreover, based on the fast Fourier transform and the matrix diagonalization method, a fast solver is developed to solving the resulting algebraic equations. Finally, the proposed scheme is employed to capture the blow-up phenomena of the nonlinear Schr\"odinger equation.},
  langid = {english},
  keywords = {AVF method,Energy-preserving,Galerkin method,Hamiltonian system,NLS equation},
  file = {/home/cjber/drive/zotero/storage/IWBHZHDE/S0096300317308457.html}
}

@article{li2018b,
  title = {Disaster Response Aided by Tweet Classification with a Domain Adaptation Approach},
  author = {Li, Hongmin and Caragea, Doina and Caragea, Cornelia and Herndon, Nic},
  year = {2018},
  journal = {Journal of Contingencies and Crisis Management},
  volume = {26},
  number = {1},
  pages = {16--27},
  issn = {1468-5973},
  doi = {10/gc35fc},
  abstract = {Social media platforms such as Twitter provide valuable information for aiding disaster response during emergency events. Machine learning could be used to identify such information. However, supervised learning algorithms rely on labelled data, which is not readily available for an emerging target disaster. While labelled data might be available for a prior source disaster, supervised classifiers learned only from the source disaster may not perform well on the target disaster, as each event has unique characteristics (e.g., type, location, and culture) and may cause different social media responses. To address this limitation, we propose to use a domain adaptation approach, which learns classifiers from unlabelled target data, in addition to source labelled data. Our approach uses the Na\"ive Bayes classifier, together with an iterative Self-Training strategy. Experimental results on the task of identifying tweets relevant to a disaster of interest show that the domain adaptation classifiers are better as compared to the supervised classifiers learned only from labelled source data.},
  langid = {english},
  keywords = {classification,disaster response,domain adaptation,Twitter},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-5973.12194},
  file = {/home/cjber/drive/pdf/Li et al_2018_Disaster response aided by tweet classification with a domain adaptation.pdf;/home/cjber/drive/zotero/storage/IWSN8FME/1468-5973.html}
}

@article{liang2020,
  title = {{{BOND}}: {{BERT-Assisted Open-Domain Named Entity Recognition}} with {{Distant Supervision}}},
  shorttitle = {{{BOND}}},
  author = {Liang, Chen and Yu, Yue and Jiang, Haoming and Er, Siawpeng and Wang, Ruijia and Zhao, Tuo and Zhang, Chao},
  year = {2020},
  month = aug,
  journal = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  eprint = {2006.15509},
  eprinttype = {arxiv},
  pages = {1054--1064},
  doi = {10/gg9w87},
  abstract = {We study the open-domain named entity recognition (NER) problem under distant supervision. The distant supervision, though does not require large amounts of manual annotations, yields highly incomplete and noisy distant labels via external knowledge bases. To address this challenge, we propose a new computational framework -- BOND, which leverages the power of pre-trained language models (e.g., BERT and RoBERTa) to improve the prediction performance of NER models. Specifically, we propose a two-stage training algorithm: In the first stage, we adapt the pre-trained language model to the NER tasks using the distant labels, which can significantly improve the recall and precision; In the second stage, we drop the distant labels, and propose a self-training approach to further improve the model performance. Thorough experiments on 5 benchmark datasets demonstrate the superiority of BOND over existing distantly supervised NER methods. The code and distantly labeled data have been released in https://github.com/cliang1453/BOND.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/Liang et al_2020_BOND.pdf;/home/cjber/drive/zotero/storage/MX3V9NJT/2006.html}
}

@article{liaw2002,
  title = {Classification and {{Regression}} by {{randomForest}}},
  author = {Liaw, Andy and Wiener, Matthew},
  year = {2002},
  volume = {2},
  pages = {6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Liaw_Wiener_2002_Classiﬁcation and Regression by randomForest.pdf}
}

@inproceedings{lieberman2011,
  title = {Multifaceted Toponym Recognition for Streaming News},
  booktitle = {Proceedings of the 34th International {{ACM SIGIR}} Conference on {{Research}} and Development in {{Information}} - {{SIGIR}} '11},
  author = {Lieberman, Michael D. and Samet, Hanan},
  year = {2011},
  pages = {843},
  publisher = {{ACM Press}},
  address = {{Beijing, China}},
  doi = {10.1145/2009916.2010029},
  abstract = {News sources on the Web generate constant streams of information, describing many aspects of the events that shape our world. In particular, geography plays a key role in the news, and enabling geographic retrieval of news articles involves recognizing the textual references to geographic locations (called toponyms) present in the articles, which can be difficult due to ambiguity in natural language. Toponym recognition in news is often accomplished with algorithms designed and tested around small corpora of news articles, but these static collections do not reflect the streaming nature of online news, as evidenced by poor performance in tests. In contrast, a method for toponym recognition is presented that is tuned for streaming news by leveraging a wide variety of recognition components, both rule-based and statistical. An evaluation of this method shows that it outperforms two prominent toponym recognition systems when tested on large datasets of streaming news, indicating its suitability for this domain.},
  isbn = {978-1-4503-0757-4},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lieberman_Samet_2011_Multifaceted toponym recognition for streaming news.pdf}
}

@book{liesen2015,
  title = {Linear {{Algebra}}},
  author = {Liesen, J{\"o}rg and Mehrmann, Volker},
  year = {2015},
  series = {Springer {{Undergraduate Mathematics Series}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24346-7},
  isbn = {978-3-319-24344-3 978-3-319-24346-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Liesen_Mehrmann_2015_Linear Algebra.pdf}
}

@incollection{ligozat2004,
  title = {What {{Is}} a {{Qualitative Calculus}}? {{A General Framework}}},
  shorttitle = {What {{Is}} a {{Qualitative Calculus}}?},
  booktitle = {{{PRICAI}} 2004: {{Trends}} in {{Artificial Intelligence}}},
  author = {Ligozat, G{\'e}rard and Renz, Jochen},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Zhang, Chengqi and W. Guesgen, Hans and Yeap, Wai-Kiang},
  year = {2004},
  volume = {3157},
  pages = {53--64},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28633-2_8},
  abstract = {What is a qualitative calculus? Many qualitative spatial and temporal calculi arise from a set of JEPD (jointly exhaustive and pairwise disjoint) relations: a stock example is Allen's calculus, which is based on thirteen basic relations between intervals on the time line. This paper examines the construction of such a formalism from a general point of view, in order to make apparent the formal algebraic properties of all formalisms of that type. We show that the natural algebraic object governing this kind of calculus is a non-associative algebra (in the sense of Maddux), and that the notion of weak representation is the right notion for describing most basic properties. We discuss the ubiquity of weak representations in various guises, and argue that the fundamental notion of consistency itself can best be understood in terms of consistency of one weak representation with respect to another.},
  isbn = {978-3-540-22817-2 978-3-540-28633-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Ligozat_Renz_2004_What Is a Qualitative Calculus.pdf}
}

@article{lin2016,
  title = {Crisis Communication, Learning and Responding: {{Best}} Practices in Social Media},
  shorttitle = {Crisis Communication, Learning and Responding},
  author = {Lin, Xialing and Spence, Patric R. and Sellnow, Timothy L. and Lachlan, Kenneth A.},
  year = {2016},
  month = dec,
  journal = {Computers in Human Behavior},
  volume = {65},
  pages = {601--605},
  issn = {0747-5632},
  doi = {10/ggff7z},
  abstract = {As noted by Seeger (2006) the notion of best practices is often use to improve professional practice; to create research and functional recommendations to use in a specific situation. This essay describes best practices in crisis communication specifically through the use of social media. It provides suggestions and approaches for improving the effectiveness of crisis communication and learning with and between organizations, governments and citizens. Seven best practices for effective crisis communication using social media are outlined.},
  langid = {english},
  keywords = {Best practices,Communicaiton,Crisis communication,Risk,Social media},
  file = {/home/cjber/drive/pdf/Lin et al_2016_Crisis communication, learning and responding.pdf;/home/cjber/drive/zotero/storage/UZBP5PHV/S0747563216304137.html}
}

@article{lin2020,
  ids = {lin2020a},
  title = {A Deep Learning Architecture for Semantic Address Matching},
  author = {Lin, Yue and Kang, Mengjun and Wu, Yuyang and Du, Qingyun and Liu, Tao},
  year = {2020},
  month = mar,
  journal = {International Journal of Geographical Information Science},
  volume = {34},
  number = {3},
  pages = {559--576},
  issn = {1365-8816, 1362-3087},
  doi = {10/gg2bhj},
  abstract = {Address matching is a crucial step in geocoding, which plays an important role in urban planning and management. To date, the unprecedented development of location-based services has generated a large amount of unstructured address data. Traditional address matching methods mainly focus on the literal similarity of address records and are therefore not applicable to the unstructured address data. In this study, we introduce an address matching method based on deep learning to identify the semantic similarity between address records. First, we train the word2vec model to transform the address records into their corresponding vector representations. Next, we apply the enhanced sequential inference model (ESIM), a deep text-matching model, to make local and global inferences to determine if two addresses match. To evaluate the accuracy of the proposed method, we fine-tune the model with real-world address data from the Shenzhen Address Database and compare the outputs with those of several popular address matching methods. The results indicate that the proposed method achieves a higher matching accuracy for unstructured address records, with its precision, recall, and F1 score (i.e., the harmonic mean of precision and recall) reaching 0.97 on the test set.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lin et al_2020_A deep learning architecture for semantic address matching.pdf;/home/cjber/drive/zotero/storage/3RMBWU99/Lin et al_2020_A deep learning architecture for semantic address matching2.pdf}
}

@inproceedings{lingad2013,
  title = {Location Extraction from Disaster-Related Microblogs},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{World Wide Web}} - {{WWW}} '13 {{Companion}}},
  author = {Lingad, John and Karimi, Sarvnaz and Yin, Jie},
  year = {2013},
  pages = {1017--1020},
  publisher = {{ACM Press}},
  address = {{Rio de Janeiro, Brazil}},
  doi = {10/ggwjtk},
  abstract = {Location information is critical to understanding the impact of a disaster, including where the damage is, where people need assistance and where help is available. We investigate the feasibility of applying Named Entity Recognizers to extract locations from microblogs, at the level of both geo-location and point-of-interest. Our experimental results show that such tools once retrained on microblog data have great potential to detect the where information, even at the granularity of point-of-interest.},
  isbn = {978-1-4503-2038-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lingad et al_2013_Location extraction from disaster-related microblogs.pdf;/home/cjber/drive/pdf/Lingad et al_2013_Location extraction from disaster-related microblogs2.pdf}
}

@article{liu,
  title = {Road {{Extraction}} from {{Airborne Lidar Data}} and {{Integrated Remote Sensing Data}}},
  author = {Liu, Li},
  pages = {89},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Liu_.pdf}
}

@article{liu2009,
  title = {Positioning Localities Based on Spatial Assertions},
  author = {Liu, Y. and Guo, Q. H. and Wieczorek, J. and Goodchild, M. F.},
  year = {2009},
  month = nov,
  journal = {International Journal of Geographical Information Science},
  volume = {23},
  number = {11},
  pages = {1471--1501},
  issn = {1365-8816, 1362-3087},
  doi = {10/fcq535},
  langid = {english},
  file = {/home/cjber/drive/pdf/Liu et al_2009_Positioning localities based on spatial assertions.pdf}
}

@inproceedings{liu2014,
  title = {Automatic {{Identification}} of {{Locative Expressions}} from {{Social Media Text}}: {{A Comparative Analysis}}},
  shorttitle = {Automatic {{Identification}} of {{Locative Expressions}} from {{Social Media Text}}},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Location}} and the {{Web}} - {{LocWeb}} '14},
  author = {Liu, Fei and Vasardani, Maria and Baldwin, Timothy},
  year = {2014},
  pages = {9--16},
  publisher = {{ACM Press}},
  address = {{Shanghai, China}},
  doi = {10/ggwjsr},
  abstract = {With the proliferation of smartphones and the increasing popularity of social media, people have developed habits of posting not only their thoughts and opinions, but also content concerning their whereabouts. On such highly-interactive yet informal social media platforms, people make heavy use of informal language, including when it comes to locative expressions. Such usage inhibits the ability of traditional Natural Language Processing approaches to retrieve geospatial information from social media text. In this research, we: (1) develop a medium-scale corpus of ``locative expressions'' derived from a variety of social media sources; (2) benchmark the performance of a range of geoparsers over the corpus, with the finding that even the best-performing systems are substantially lacking; and (3) carry out extensive error analysis to suggest ways of improving the accuracy and robustness of geoparsers.},
  isbn = {978-1-4503-1459-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Liu et al_2014_Automatic Identification of Locative Expressions from Social Media Text.pdf}
}

@inproceedings{liu2017,
  title = {Dense {{Semantic Labeling}} of {{Very-High-Resolution Aerial Imagery}} and {{LiDAR}} with {{Fully-Convolutional Neural Networks}} and {{Higher-Order CRFs}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Liu, Yansong and Piramanayagam, Sankaranarayanan and Monteiro, Sildomar T. and Saber, Eli},
  year = {2017},
  month = jul,
  pages = {1561--1570},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}},
  doi = {10/gf3tdb},
  abstract = {The increasing availability of very-high-resolution (VHR) aerial optical images as well as coregistered LiDAR data opens great opportunities for improving objectlevel dense semantic labeling of airborne remote sensing imagery. As a result, efficient and effective multisensor fusion techniques are needed to fully exploit these complementary data modalities. Recent researches demonstrated how to process remote sensing images using pre-trained deep convolutional neural networks (DCNNs) at the feature level. In this paper, we propose a decision-level fusion approach using a probabilistic graphical model for the task of dense semantic labeling. Our proposed method first obtains two initial probabilistic labeling predictions from a fully-convolutional neural network and a linear classifier, e.g. logistic regression, respectively. These two predictions are then combined within a higher-order conditional random field (CRF). We utilize graph cut inference to estimate the final dense semantic labeling results. Higher-order CRF modeling helps to resolve fusion ambiguities by explicitly using the spatial contextual information, which can be learned from the training data. Experiments on the ISPRS 2D semantic labeling Potsdam dataset show that our proposed approach compares favorably to the state-of-the-art baseline methods.},
  isbn = {978-1-5386-0733-6},
  langid = {english},
  annotation = {ZSCC: 0000032},
  file = {/home/cjber/drive/pdf/ENVS492/Liu et al_2017_.pdf}
}

@article{liu2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.11692 [cs]},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Liu et al_2019_RoBERTa.pdf;/home/cjber/drive/zotero/storage/PZWRCFSJ/1907.html}
}

@article{liua,
  title = {A {{Novel Algorithm}} for {{Road Extraction}} from {{Airborne Lidar Data}}},
  author = {Liu, Li and Lim, Samsung},
  pages = {10},
  abstract = {Road data in 3-dimensional forms is required for a variety of geospatial applications e.g. road maintenance, transport planning and location-based services. Although airborne lidar can produce dense point clouds from which 3-dimensional road information can be retrieved in detail, lidar data is often incomplete due to the line-of-sight requirement, and therefore a better result of information extraction from lidar data can be achieved if supplement data is used. This paper presents a novel algorithm for road extraction from lidar point clouds and associative vector data. A moving window-based classification technique using various cues determined from the lidar data is applied in a hierarchical way to separate road from other objects. In order to fill the gaps caused by the line-of-sight problem e.g. shadows of trees and buildings, the vector data is used in the refinement process to improve the results by including a buffer, deleting false positives, interpolating and fitting the road-representing points into polylines. To validate the proposed algorithm, four samples of lidar data sets are tested. The test result shows that it is a practical method for road extraction from airborne lidar data for both structured and unstructured lanes.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {/home/cjber/drive/pdf/ENVS492/LiDAR/Liu_Lim_.pdf}
}

@article{liub,
  title = {A {{Study}} of {{Colloquial Place Names}} through {{Geotagged Social Media Data}}},
  author = {Liu, Yuan},
  pages = {96},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Liu_A Study of Colloquial Place Names through Geotagged Social Media Data.pdf}
}

@book{logan2015,
  title = {Applied {{Partial Differential Equations}}},
  author = {Logan, J. David},
  year = {2015},
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-12493-3},
  isbn = {978-3-319-12492-6 978-3-319-12493-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Logan_2015_Applied Partial Differential Equations.pdf}
}

@article{lorant2001,
  title = {Deprivation and Mortality: The Implications of Spatial Autocorrelation for Health Resources Allocation},
  shorttitle = {Deprivation and Mortality},
  author = {Lorant, Vincent and Thomas, Isabelle and Deli{\`e}ge, Denise and Tonglet, Ren{\'e}},
  year = {2001},
  month = dec,
  journal = {Social Science \& Medicine},
  volume = {53},
  number = {12},
  pages = {1711--1719},
  issn = {02779536},
  doi = {10/d7rqnw},
  abstract = {This paper aims at investigating whether the relationship between mortality and socio-economic deprivation is affected by the spatial autocorrelation of ecological data. A simple model is used in which mortality (all-ages and premature) is the dependent variable, and deprivation, morbidity and other socio-economic indicators are the explanatory variables. Deprivation is measured by the Townsend index; the other socio-economic variables are the median income, unequal income distribution (Gini coefficient) and population density. Morbidity is estimated on the basis of hospital admission rates and overweight prevalence. Spatial autocorrelation is measured by the Moran's I coefficient. All mortality and morbidity variables have significant, positive, and moderate-to-high spatial autocorrelation. Two multivariate models are explored: a weighted least-squares model ignoring spatial autocorrelation and a simultaneous autoregressive model. The paper concludes that spatial autocorrelation has a significant impact on the relationship between mortality and socio-economic variables. Future ecological models intended to inform health resources allocation need to pay greater attention to the spatial dimension of the data used. \# 2001 Elsevier Science Ltd. All rights reserved.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Lorant et al_2001_.pdf}
}

@article{lorini2019,
  title = {Integrating {{Social Media}} into a {{Pan-European Flood Awareness System}}: {{A Multilingual Approach}}},
  shorttitle = {Integrating {{Social Media}} into a {{Pan-European Flood Awareness System}}},
  author = {Lorini, V. and Castillo, C. and Dottori, F. and Kalas, M. and Nappo, D. and Salamon, P.},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.10876 [cs]},
  eprint = {1904.10876},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper describes a prototype system that integrates social media analysis into the European Flood Awareness System (EFAS). This integration allows the collection of social media data to be automatically triggered by flood risk warnings determined by a hydro-meteorological model. Then, we adopt a multi-lingual approach to find flood-related messages by employing two state-of-the-art methodologies: language-agnostic word embeddings and language-aligned word embeddings. Both approaches can be used to bootstrap a classifier of social media messages for a new language with little or no labeled data. Finally, we describe a method for selecting relevant and representative messages and displaying them back in the interface of EFAS.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/cjber/drive/pdf/Lorini et al_2019_Integrating Social Media into a Pan-European Flood Awareness System.pdf}
}

@article{loshchilov2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  journal = {arXiv:1711.05101 [cs, math]},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/home/cjber/drive/pdf/Loshchilov_Hutter_2019_Decoupled Weight Decay Regularization.pdf;/home/cjber/drive/zotero/storage/QJVURUWK/1711.html}
}

@article{lu2012,
  title = {Predictability of Population Displacement after the 2010 {{Haiti}} Earthquake},
  author = {Lu, X. and Bengtsson, L. and Holme, P.},
  year = {2012},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {29},
  pages = {11576--11581},
  issn = {0027-8424, 1091-6490},
  doi = {10/f2z47x},
  langid = {english},
  file = {/home/cjber/drive/pdf/Lu et al_2012_Predictability of population displacement after the 2010 Haiti earthquake.pdf}
}

@article{lu2014,
  title = {The {{GWmodel R}} Package: Further Topics for Exploring Spatial Heterogeneity Using Geographically Weighted Models},
  author = {Lu, Binbin and Harris, Paul and Charlton, Martin and Brunsdon, Christopher},
  year = {2014},
  journal = {Geo-spatial Information Science},
  volume = {17},
  number = {2},
  pages = {85--101},
  doi = {10/gdsksj},
  keywords = {\#nosource}
}

@article{ludwig2016,
  ids = {ludwig2016a},
  title = {Deep {{Embedding}} for {{Spatial Role Labeling}}},
  author = {Ludwig, Oswaldo and Liu, Xiao and Kordjamshidi, Parisa and Moens, Marie-Francine},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.08474 [cs]},
  eprint = {1603.08474},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the \$F1\$ measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/cjber/drive/pdf/Ludwig et al_2016_Deep Embedding for Spatial Role Labeling.pdf;/home/cjber/drive/pdf/Ludwig et al_2016_Deep Embedding for Spatial Role Labeling2.pdf;/home/cjber/drive/zotero/storage/3LY8BMZG/1603.html;/home/cjber/drive/zotero/storage/CLH98DVI/1603.html}
}

@book{luenberger2016,
  title = {Linear and {{Nonlinear Programming}}},
  author = {Luenberger, David G. and Ye, Yinyu},
  year = {2016},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  volume = {228},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-18842-3},
  isbn = {978-3-319-18841-6 978-3-319-18842-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Luenberger_Ye_2016_Linear and Nonlinear Programming.pdf}
}

@article{luna2018,
  title = {Social Media Applications and Emergency Management: {{A}} Literature Review and Research Agenda},
  shorttitle = {Social Media Applications and Emergency Management},
  author = {Luna, Sergio and Pennock, Michael J.},
  year = {2018},
  month = jun,
  journal = {International Journal of Disaster Risk Reduction},
  volume = {28},
  pages = {565--577},
  issn = {22124209},
  doi = {10/gdhw62},
  abstract = {Social media applications have proven to be a dependable communication channel even when traditional methods fail. Their application to emergency management offers new benefits to the domain. For instance, analysis of information as the event unfolds may increase situational awareness, news and alerts may reach larger audiences in less time and decision makers may monitor public activities as well as coordinate with stakeholders. With such benefits, it seems the adoption of social media applications to emergency management should be automatic. However, their implementation introduces risks as well. To better understand the benefits and challenges, a review and analysis of the literature regarding the application of social media to emergency management was conducted. Identified research gaps were mapped into social and technological challenges. These challenges were then analyzed to set research directions for practitioners and researchers.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Luna_Pennock_2018_Social media applications and emergency management.pdf}
}

@book{lussault2009,
  title = {De La Lutte Des Classes \`a La Lutte Des Places},
  author = {Lussault, Michel},
  year = {2009},
  publisher = {{Grasset Paris}},
  isbn = {2-246-73391-X}
}

@book{lynch1960,
  title = {The Image of the City},
  author = {Lynch, Kevin},
  year = {1960},
  series = {Publication of the {{Joint Center}} for {{Urban Studies}}},
  edition = {Nachdr.},
  publisher = {{MIT PRESS}},
  address = {{Cambridge, Mass.}},
  isbn = {978-0-262-12004-3 978-0-262-62001-7},
  langid = {english},
  annotation = {OCLC: 255254857},
  file = {/home/cjber/drive/pdf/Lynch_1960_The image of the city.pdf}
}

@article{maas1999,
  title = {The {{Potential}} of {{Height Texture Measures}} for the {{Segmentation}} of {{Airborne Laserscanner Data}}},
  author = {Maas, Hans-Gerd},
  year = {1999},
  pages = {8},
  abstract = {Airborne laserscanning is being used for an increasing number of mapping and GIS data acquisition tasks. Besides the original purpose of digital terrain model generation, new applications arise in the automatic detection and modeling of objects such as buildings or vegetation for the generation of 3-D city models.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000134},
  file = {/home/cjber/drive/pdf/ENVS492/Maas_.pdf}
}

@article{machado2010,
  title = {An {{Ontological Gazetteer}} for {{Geographic Information Retrieval}}},
  author = {Machado, Ivre Marjorie R and {de Alencar}, Rafael Odon and {de Oliveira}, Roberto and Junior, Campos and Junior, Clodoveu A Davis},
  year = {2010},
  pages = {13},
  abstract = {The volume of spatial information on the Web grows daily, added to that, the problems to recognize references to spatial relationships and to deal with ambiguous names. This article presents a gazetteer, which has a structure different from conventional gazetteers. The ontological gazetteer will not only identify the names of places, but also record concepts and terms related to a place, as in an ontology in which concepts are the main places and features. A case study showed good results for detection of place names and inference implied by news Web sites based on content of ontological gazetteer.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Machado et al_2010_An Ontological Gazetteer for Geographic Information Retrieval.pdf}
}

@article{macintyre1993,
  title = {Area, {{Class}} and {{Health}}: {{Should}} We Be {{Focusing}} on {{Places}} or {{People}}?},
  shorttitle = {Area, {{Class}} and {{Health}}},
  author = {Macintyre, Sally and Maciver, Sheila and Sooman, Anne},
  year = {1993},
  month = apr,
  journal = {Journal of Social Policy},
  volume = {22},
  number = {02},
  pages = {213},
  issn = {0047-2794, 1469-7823},
  doi = {10/csxr4n},
  abstract = {In Britain there has been a long tradition of research into associations between area of residence and health. Rarely has this involved investigating socio-economic or cultural features of areas that might influence health; usually studies use area level data, for example about specific pathogens or about levels of deprivation, as surrogates for individual level data, rather than being interested in the areas themselves. This paper reviews the literature on the relationship between area and health. It advocates directly studying features of the local social and physical environment which might promote or inhibit health, illustrating this approach with some findings from a study in the West of Scotland, and suggests that improvements in public health might be achieved by focusing on places as well as on people.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Macintyre et al_1993_.pdf}
}

@article{mai2020,
  title = {Multi-{{Scale Representation Learning}} for {{Spatial Feature Distributions}} Using {{Grid Cells}}},
  author = {Mai, Gengchen and Janowicz, Krzysztof and Yan, Bo and Zhu, Rui and Cai, Ling and Lao, Ni},
  year = {2020},
  month = feb,
  journal = {arXiv:2003.00824 [cs, stat]},
  eprint = {2003.00824},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea is to use neural networks to convert words in texts to vector space representations based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose representation model for space is valuable for a multitude of tasks. However, no such general model exists to date beyond simply applying discretization or feed-forward nets to coordinates, and little effort has been put into jointly modeling distributions with vastly different characteristics, which commonly emerges from GIS data. Meanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in mammals provide a multi-scale periodic representation that functions as a metric for location encoding and is critical for recognizing places and for path-integration. Therefore, we propose a representation learning model called Space2Vec to encode the absolute positions and spatial relationships of places. We conduct experiments on two real-world geographic data for two different tasks: 1) predicting types of POIs given their positions and context, 2) image classification leveraging their geo-locations. Results show that because of its multi-scale representations, Space2Vec outperforms well-established ML approaches such as RBF kernels, multi-layer feed-forward nets, and tile embedding approaches for location modeling and image classification tasks. Detailed analysis shows that all baselines can at most well handle distribution at one scale but show poor performances in other scales. In contrast, Space2Vec's multi-scale representation can handle distributions at different scales.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.2.0,I.2.6,I.5.1,J.2,Statistics - Machine Learning},
  file = {/home/cjber/drive/pdf/Mai et al_2020_Multi-Scale Representation Learning for Spatial Feature Distributions using.pdf;/home/cjber/drive/zotero/storage/JT4LFMGR/2003.html}
}

@incollection{mai2020a,
  title = {Relaxing {{Unanswerable Geographic Questions Using A Spatially Explicit Knowledge Graph Embedding Model}}},
  booktitle = {Geospatial {{Technologies}} for {{Local}} and {{Regional Development}}},
  author = {Mai, Gengchen and Yan, Bo and Janowicz, Krzysztof and Zhu, Rui},
  editor = {Kyriakidis, Phaedon and Hadjimitsis, Diofantos and Skarlatos, Dimitrios and Mansourian, Ali},
  year = {2020},
  pages = {21--39},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-14745-7_2},
  abstract = {Recent years have witnessed a rapid increase in Question Answering (QA) research and products in both academic and industry. However, geographic question answering remained nearly untouched although geographic questions account for a substantial part of daily communication. Compared to general QA systems, geographic QA has its own uniqueness, one of which can be seen during the process of handling unanswerable questions. Since users typically focus on the geographic constraints when they ask questions, if the question is unanswerable based on the knowledge base used by a QA system, users should be provided with a relaxed query which takes distance decay into account during the query relaxation and rewriting process. In this work, we present a spatially explicit translational knowledge graph embedding model called TransGeo which utilizes an edge-weighted PageRank and sampling strategy to encode the distance decay into the embedding model training process. This embedding model is further applied to relax and rewrite unanswerable geographic questions. We carry out two evaluation tasks: link prediction as well as query relaxation/rewriting for an approximate answer prediction task. A geographic knowledge graph training/testing dataset, DB18, as well as an unanswerable geographic query dataset, GeoUQ, are constructed. Compared to four other baseline models, our TransGeo model shows substantial advantages in both tasks.},
  isbn = {978-3-030-14744-0 978-3-030-14745-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Mai et al_2020_Relaxing Unanswerable Geographic Questions Using A Spatially Explicit Knowledge.pdf}
}

@article{malecki2003,
  title = {Digital {{Development}} in {{Rural Areas}}: {{Potentials}} and {{Pitfalls}}},
  author = {Malecki, Edward},
  year = {2003},
  month = apr,
  journal = {Journal of Rural Studies},
  volume = {19},
  pages = {201--214},
  doi = {10/fhms4b},
  keywords = {\#nosource}
}

@incollection{malmasi2016,
  title = {Location {{Mention Detection}} in {{Tweets}} and {{Microblogs}}},
  booktitle = {Computational {{Linguistics}}},
  author = {Malmasi, Shervin and Dras, Mark},
  editor = {Hasida, K{\^o}iti and Purwarianti, Ayu},
  year = {2016},
  volume = {593},
  pages = {123--134},
  publisher = {{Springer Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-10-0515-2_9},
  abstract = {The automatic identification of location expressions in social media text is an actively researched task. We present a novel approach to detection mentions of locations in the texts of microblogs and social media. We propose an approach based on Noun Phrase extraction and n-gram based matching instead of the traditional methods using Named Entity Recognition (NER) or Conditional Random Fields (CRF), arguing that our method is better suited to noisy microblog text. Our proposed system is comprised of several individual modules to detect addresses, Points of Interest (e.g. hospitals or universities), distance and direction markers; and location names (e.g. suburbs or countries). Our system won the ALTA 2014 Twitter Location Detection shared task with an F-score of 0.792 for detecting location expressions in a test set of 1,000 tweets, demonstrating its efficacy for this task. A number of directions for future work are discussed.},
  isbn = {978-981-10-0514-5 978-981-10-0515-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Malmasi_Dras_2016_Location Mention Detection in Tweets and Microblogs.pdf}
}

@misc{manchestercitycouncil2003,
  title = {Northern {{Quarter Development Framework}}},
  author = {{Manchester City Council}},
  year = {2003},
  month = aug,
  howpublished = {https://web.archive.org/web/20080511183526/http://www.manchester.gov.uk/downloads/Northern\_Quarter\_Development\_Framework.pdf},
  file = {/home/cjber/drive/pdf/Manchester City Council_2003_Northern Quarter Development Framework.pdf}
}

@article{mangalgi2020,
  title = {Deep {{Contextual Embeddings}} for {{Address Classification}} in {{E-commerce}}},
  author = {Mangalgi, Shreyas and Kumar, Lakshya and Tallamraju, Ravindra Babu},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.03020 [cs, stat]},
  eprint = {2007.03020},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {E-commerce customers in developing nations like India tend to follow no fixed format while entering shipping addresses. Parsing such addresses is challenging because of a lack of inherent structure or hierarchy. It is imperative to understand the language of addresses, so that shipments can be routed without delays. In this paper, we propose a novel approach towards understanding customer addresses by deriving motivation from recent advances in Natural Language Processing (NLP). We also formulate different pre-processing steps for addresses using a combination of edit distance and phonetic algorithms. Then we approach the task of creating vector representations for addresses using Word2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these approaches with respect to sub-region classification task for North and South Indian cities. Through experiments, we demonstrate the effectiveness of generalized RoBERTa model, pre-trained over a large address corpus for language modelling task. Our proposed RoBERTa model achieves a classification accuracy of around 90\% with minimal text preprocessing for sub-region classification task outperforming all other approaches. Once pre-trained, the RoBERTa model can be fine-tuned for various downstream tasks in supply chain like pincode 1 suggestion and geo-coding. The model generalizes well for such tasks even with limited labelled data. To the best of our knowledge, this is the first of its kind research proposing a novel approach of understanding customer addresses in e-commerce domain by pre-training language models and fine-tuning them for different purposes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/cjber/drive/pdf/Mangalgi et al_2020_Deep Contextual Embeddings for Address Classification in E-commerce.pdf;/home/cjber/drive/pdf/Mangalgi et al_2020_Deep Contextual Embeddings for Address Classification in E-commerce2.pdf}
}

@article{mani,
  title = {Qualitative {{Modeling}} of {{Spatial Prepositions}} and {{Motion Expressions}}},
  author = {Mani, Inderjeet and Pustejovsky, James},
  pages = {249},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Mani_Pustejovsky_Qualitative Modeling of Spatial Prepositions and Motion Expressions.pdf;/home/cjber/drive/pdf/Mani_Pustejovsky_Qualitative Modeling of Spatial Prepositions and Motion Expressions2.pdf}
}

@article{mani2010,
  ids = {mani2010a},
  title = {{{SpatialML}}: Annotation Scheme, Resources, and Evaluation},
  shorttitle = {{{SpatialML}}},
  author = {Mani, Inderjeet and Doran, Christy and Harris, Dave and Hitzeman, Janet and Quimby, Rob and Richer, Justin and Wellner, Ben and Mardis, Scott and Clancy, Seamus},
  year = {2010},
  month = sep,
  journal = {Language Resources and Evaluation},
  volume = {44},
  number = {3},
  pages = {263--280},
  issn = {1574-020X, 1574-0218},
  doi = {10/cnmp8m},
  langid = {english},
  file = {/home/cjber/drive/pdf/Mani et al_2010_SpatialML.pdf;/home/cjber/drive/zotero/storage/Q3VUBRJA/Mani et al_2010_SpatialML2.pdf}
}

@book{manning1999,
  ids = {manning1999a},
  title = {Foundations of Statistical Natural Language Processing},
  author = {Manning, Christopher D and Manning, Christopher D and Sch{\"u}tze, Hinrich},
  year = {1999},
  publisher = {{MIT press}},
  isbn = {0-262-13360-1},
  file = {/home/cjber/drive/pdf/Manning et al_1999_Foundations of statistical natural language processing.pdf;/home/cjber/drive/pdf/Manning et al_1999_Foundations of statistical natural language processing2.pdf}
}

@book{manning2008,
  title = {Introduction to Information Retrieval},
  author = {Manning, Christopher D and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year = {2008},
  publisher = {{Cambridge university press}},
  isbn = {0-521-86571-9},
  file = {/home/cjber/drive/pdf/Manning et al_2008_Introduction to information retrieval.pdf}
}

@inproceedings{manning2014,
  ids = {manning2014a},
  title = {The {{Stanford CoreNLP Natural Language Processing Toolkit}}},
  booktitle = {Proceedings of 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
  year = {2014},
  pages = {55--60},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, Maryland}},
  doi = {10/gf3xhp},
  abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
  langid = {english},
  keywords = {NLP},
  file = {/home/cjber/drive/pdf/Manning et al_2014_The Stanford CoreNLP Natural Language Processing Toolkit.pdf;/home/cjber/drive/pdf/Manning et al_2014_The Stanford CoreNLP Natural Language Processing Toolkit2.pdf;/home/cjber/drive/pdf/Manning et al_2014_The Stanford CoreNLP Natural Language Processing Toolkit3.pdf}
}

@article{mansbridge2005,
  title = {Perceptions of {{Imprecise Regions}} in {{Relation}} to {{Geographical Information Retrieval}}},
  author = {Mansbridge, Linda},
  year = {2005},
  pages = {87},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Mansbridge_2005_Perceptions of Imprecise Regions in Relation to Geographical Information.pdf}
}

@article{manthorpe2008,
  title = {Elderly People's Perspectives on Health and Well-being in Rural Communities in {{England}}: Findings from the Evaluation of the {{National Service Framework}} for {{Older People}}},
  author = {Manthorpe, Jill and Iliffe, Steve and Clough, Roger and Cornes, Michelle and Bright, Les and Moriarty, Jo and {Older People Researching Social Issues}},
  year = {2008},
  journal = {Health \& social care in the community},
  volume = {16},
  number = {5},
  pages = {460--468},
  issn = {0966-0410},
  doi = {10/ct29jh},
  keywords = {\#nosource},
  annotation = {ZSCC: NoCitationData[s0]}
}

@book{marin2014,
  title = {Bayesian {{Essentials}} with {{R}}},
  author = {Marin, Jean-Michel and Robert, Christian P.},
  year = {2014},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-8687-9},
  isbn = {978-1-4614-8686-2 978-1-4614-8687-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/Marin_Robert_2014_Bayesian Essentials with R.pdf}
}

@article{marks2002,
  title = {Emergency (999) Calls to the Ambulance Service That Do Not Result in the Patient Being Transported to Hospital: An Epidemiological Study},
  shorttitle = {Emergency (999) Calls to the Ambulance Service That Do Not Result in the Patient Being Transported to Hospital},
  author = {Marks, P J},
  year = {2002},
  month = sep,
  journal = {Emergency Medicine Journal},
  volume = {19},
  number = {5},
  pages = {449--452},
  issn = {14720205, 14720213},
  doi = {10/c9pjxx},
  abstract = {Objective: To describe the demographic and clinical characteristics of patients who are not transported to hospital after an emergency (999) call to the East Midlands Ambulance Service, the reason for non-transportation, and the priority assigned when the ambulance is dispatched. Methods: The first 500 consecutive non-transported patients from 1 March 2000 were identified from the ambulance service command and control data. Epidemiological and clinical data were then obtained from the patient report form completed by the attending ambulance crew and compared with the initial priority dispatch (AMPDS) code that determined the urgency of the ambulance response. Results: Data were obtained for 498 patients. Twenty six per cent of these calls were assigned an AMPDS delta code (the most urgent category) at the time the call was received. Falls accounted for 34\% of all non-transported calls. This group of patients were predominantly elderly people (over 70 years old) and the majority (89\%) were identified as less urgent (coded AMPDS alpha or bravo) at telephone triage. The mean time that an ambulance was committed to each non-transported call was 34 minutes. Conclusions: This study shows that falls in elderly people account for a significant proportion of nontransported 999 calls and are often assigned a low priority when the call is first received. There could be major gains if some of these patients could be triaged to an alternative response, both in terms of increasing the ability of the ambulance service to respond faster to clinically more urgent calls and improving the cost effectiveness of the health service. The AMPDS priority dispatch system has been shown to be sensitive but this study suggests that its specificity may be poor, resulting in rapid responses to relatively minor problems. More research is required to determine whether AMPDS prioritisation can reliably and safely identify 999 calls where an alternative to an emergency ambulance would be a more appropriate response.},
  langid = {english},
  keywords = {ES},
  file = {/home/cjber/drive/pdf/Marks_2002_Emergency (999) calls to the ambulance service that do not result in the.pdf}
}

@article{marsden2002,
  title = {The Social Management of Rural Nature: Understanding Agrarian-Based Rural Development},
  author = {Marsden, Terry and Banks, Jo and Bristow, Gillian},
  year = {2002},
  journal = {Environment and planning A},
  volume = {34},
  number = {5},
  pages = {809--825},
  issn = {0308-518X},
  doi = {10/d8b774},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000228}
}

@article{martin2006,
  title = {Last of the Censuses? {{The}} Future of Small Area Population Data},
  author = {Martin, David},
  year = {2006},
  journal = {Transactions of the Institute of British Geographers},
  volume = {31},
  number = {1},
  pages = {6--18},
  issn = {0020-2754},
  doi = {10/b757f4},
  file = {/home/cjber/drive/pdf/ENVS416/Martin_2006_.pdf}
}

@article{martinez-rojas2018,
  title = {Twitter as a Tool for the Management and Analysis of Emergency Situations: {{A}} Systematic Literature Review},
  shorttitle = {Twitter as a Tool for the Management and Analysis of Emergency Situations},
  author = {{Mart{\'i}nez-Rojas}, Mar{\'i}a and {Pardo-Ferreira}, Mar{\'i}a del Carmen and {Rubio-Romero}, Juan Carlos},
  year = {2018},
  month = dec,
  journal = {International Journal of Information Management},
  volume = {43},
  pages = {196--208},
  issn = {0268-4012},
  doi = {10/gfmbxd},
  abstract = {The importance of timely, accurate and effective use of available information is essential to the proper management of emergency situations. In recent years, emerging technologies have provided new approaches towards the distribution and acquisition of crowdsourced information to facilitate situational awareness and management during emergencies. In this regard, internet and social networks have shown potential to be an effective tool in disseminating and obtaining up-to-date information. Among the most popular social networks, research has pointed to Twitter as a source of information that offers valuable real-time data for decision-making. The objective of this paper is to conduct a systematic literature review that provides an overview of the current state of research concerning the use of Twitter to emergencies management, as well as presents the challenges and future research directions.},
  langid = {english},
  keywords = {Data,Emergencies,Management,Review,Social network,Twitter},
  file = {/home/cjber/drive/pdf/Martínez-Rojas et al_2018_Twitter as a tool for the management and analysis of emergency situations.pdf;/home/cjber/drive/zotero/storage/TU826T2M/S0268401218303499.html}
}

@inproceedings{martins2005,
  title = {A Graph-Ranking Algorithm for Geo-Referencing Documents},
  booktitle = {Fifth {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}}'05)},
  author = {Martins, B. and Silva, M.J.},
  year = {2005},
  month = nov,
  pages = {4 pp.-},
  issn = {2374-8486},
  doi = {10/d987xq},
  abstract = {This paper presents an application of PageRank for assigning documents with a corresponding geographical scope. We describe the technique in detail, together with its theoretical formulation. Experimental results are promising, comparing favorably with previous proposals.},
  keywords = {Data mining,document handling,Frequency,geographic information systems,geographical scope,georeferencing documents,graph-ranking algorithm,Merging,Ontologies,PageRank,Statistics,Testing,Text recognition,Training data,Vocabulary},
  file = {/home/cjber/drive/pdf/Martins_Silva_2005_A graph-ranking algorithm for geo-referencing documents.pdf;/home/cjber/drive/zotero/storage/4ICJAPFG/1565771.html}
}

@article{mason1999,
  title = {The Pattern of Area Deprivation},
  author = {Mason, S},
  year = {1999},
  journal = {Persistent Poverty and Lifetime Inequality: The evidence},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{mathew2014,
  title = {Optimisation of Maintenance Strategy for Rural Road Network Using Genetic Algorithm},
  author = {Mathew, Binu Sara and Isaac, Kuncheria P.},
  year = {2014},
  month = apr,
  journal = {International Journal of Pavement Engineering},
  volume = {15},
  number = {4},
  pages = {352--360},
  issn = {1029-8436, 1477-268X},
  doi = {10/gf33rv},
  langid = {english},
  annotation = {00022},
  file = {/home/cjber/drive/pdf/ENVS492/Road Assessment/Mathew_Isaac_2014_.pdf}
}

@inproceedings{mathioudakis2010,
  title = {{{TwitterMonitor}}: Trend Detection over the Twitter Stream},
  shorttitle = {{{TwitterMonitor}}},
  booktitle = {Proceedings of the 2010 {{ACM SIGMOD International Conference}} on {{Management}} of Data},
  author = {Mathioudakis, Michael and Koudas, Nick},
  year = {2010},
  month = jun,
  pages = {1155--1158},
  publisher = {{ACM}},
  address = {{Indianapolis Indiana USA}},
  doi = {10/dr8qjr},
  abstract = {We present TwitterMonitor, a system that performs trend detection over the Twitter stream. The system identifies emerging topics (i.e. `trends') on Twitter in real time and provides meaningful analytics that synthesize an accurate description of each topic. Users interact with the system by ordering the identified trends using different criteria and submitting their own description for each trend.},
  isbn = {978-1-4503-0032-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Mathioudakis_Koudas_2010_TwitterMonitor.pdf}
}

@article{matkan2014,
  title = {Road {{Extraction}} from {{Lidar Data Using Support Vector Machine Classification}}},
  author = {Matkan, Ali Akbar and Hajeb, Mohammad and Sadeghian, Saeed},
  year = {2014},
  month = may,
  journal = {Photogrammetric Engineering \& Remote Sensing},
  volume = {80},
  number = {5},
  pages = {409--422},
  issn = {00991112},
  doi = {10/f52cdx},
  abstract = {This paper presents a method for road extraction from lidar data based on SVM classification. The lidar data are used exclusively to evaluate the potential in the road extraction process. First, the SVM algorithm is used to classify the lidar data into five classes: road, tree, building, grassland, and cement. Then, some misclassified pixels in the road class is removed using the road values in the normalized Digital Surface Model and Normalized Difference Distance features. In the postprocessing stage, a method based on Radon transform and Spline interpolation is employed to automatically locate and fill the gaps in the road network. The experimental results show that the proposed algorithm for gap filling works well on straight roads. The proposed road extraction algorithm is tested on three datasets. An accuracy assessment indicated 63.7 percent, 60.26 percent and 66.71 percent quality for three datasets. Finally, centerline of the detected roads is extracted using mathematical morphology.},
  langid = {english},
  annotation = {ZSCC: 0000022},
  file = {/home/cjber/drive/pdf/ENVS492/Matkan et al_2014_.pdf}
}

@article{mazalov,
  ids = {mazalov2016,mazalova},
  title = {Understanding {{Spatial Semantics}} in {{Natural Language}}},
  author = {Mazalov, Alexey},
  pages = {6},
  abstract = {In this MSc thesis we focus on the Spatial Role Labeling (SpRL) task, which emerged as a variation of Semantic Role Labeling (SRL). The approach we advocate for SpRL fully exploits its kinship with SRL. It is based on adopting the NLPNET system, that is built on a multi-layer convolutional neural network and has proved its efficiency in dealing with the SRL task. We demonstrate that SpRL can be performed by a system originally designed for SRL, as well as that by applying several relatively simple input format transformations, it is possible to achieve a gain in the system's performance. We present the results for the distinct cases of static and dynamic spatial relations. Although being relatively modest, the performance demonstrated by the system is comparable with that of the known state-of-the-art systems.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Mazalov_Understanding Spatial Semantics in Natural Language.pdf;/home/cjber/drive/pdf/Mazalov_Understanding Spatial Semantics in Natural Language2.pdf;/home/cjber/drive/pdf/Mazalov_Understanding Spatial Semantics in Natural Language3.pdf}
}

@inproceedings{mazalov2015,
  ids = {mazalov2015a},
  title = {Spatial Role Labeling with Convolutional Neural Networks},
  booktitle = {Proceedings of the 9th {{Workshop}} on {{Geographic Information Retrieval}} - {{GIR}} '15},
  author = {Mazalov, Alexey and Martins, Bruno and Matos, David},
  year = {2015},
  pages = {1--7},
  publisher = {{ACM Press}},
  address = {{Paris, France}},
  doi = {10/gg24h6},
  abstract = {Many natural language processing applications require information about the spatial locations of objects referenced in text, or spatial relations between these objects in space. For example, the phrase a book on the shelf contains information about the location of the object book, corresponding to a trajector, with respect to the object shelf, which in turn corresponds to a landmark. Spatial role labeling concerns with the task of automatically processing textual sentences and identifying objects of spatial scenes and relations between them. In this paper, we describe the application of modern machine learning methods to extract spatial roles and their relations, specifically by adapting a pre-existing system based on a convolutional neural network architecture that has been recently proposed for the more general task of semantic role labeling. We report on experiments with datasets from the SemEval challenges on spatial role labeling, showing that our method can achieve results in line with the current state-of-the-art. We therefore argue that that spatial role labeling can leverage on recent developments in semantic role labeling, requiring only minimal adaptations.},
  isbn = {978-1-4503-3937-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Mazalov et al_2015_Spatial role labeling with convolutional neural networks.pdf;/home/cjber/drive/pdf/Mazalov et al_2015_Spatial role labeling with convolutional neural networks2.pdf;/home/cjber/drive/pdf/Mazalov et al_2015_Spatial role labeling with convolutional neural networks3.pdf;/home/cjber/drive/pdf/Mazalov et al_2015_Spatial role labeling with convolutional neural networks4.pdf;/home/cjber/drive/pdf/Mazalov et al_2015_Spatial role labeling with convolutional neural networks5.pdf}
}

@book{mazzoleni2017,
  title = {Improving {{Flood Prediction Assimilating Uncertain Crowdsourced Data}} into {{Hydrologic}} and {{Hydraulic Models}}},
  author = {Mazzoleni, Maurizio},
  year = {2017},
  abstract = {Cover -- Half Title -- Title Page -- Copyright Page -- Dedication -- Acknoledgments -- Summary -- Samenvatting -- Sommario -- Table of Contents -- 1: Introduction -- 1.1 Background -- 1.1.1 Flood forecasting and early warning systems -- 1.1.2 Hydrological and hydrodynamic modelling -- 1.1.3 Uncertainty in hydrological and hydrodynamic modelling -- 1.1.4 Data assimilation -- 1.1.5 Citizen Science -- 1.2 Motivation -- 1.3 Terminology -- 1.4 Research objectives -- 1.5 Outline of the thesis -- 2: Case studies and models -- 2.1 Introduction -- 2.2 Case 1 - Brue Catchment (UK) -- 2.2.1 Catchment description -- 2.2.2 Model description -- 2.3 Case 2 - Bacchiglione Catchment (Italy) -- 2.3.1 Catchment description -- 2.3.2 Model description -- 2.4 Case 3 - Trinity and Sabine Rivers (USA) -- 2.4.1 Rivers description -- 2.4.2 Model description -- 2.5 Case 4 - Synthetic river reach -- 3: Data assimilation methods -- 3.1 Introduction -- 3.2 Direct insertion -- 3.3 Nudging scheme -- 3.4 Kalman Filter -- 3.5 Ensemble Kalman Filter -- 3.6 Asynchronous Ensemble Kalman Filter -- 4: Assimilation of synchronous data in hydrological models -- 4.1 Introduction -- 4.2 Methodology -- 4.2.1 Assimilation of intermittent observations -- 4.2.2 Observation and model error -- 4.2.3 Generation of synthetic observations -- 4.3 Experimental setup -- 4.3.1 Experiment 4.1: Streamflow data from static physical (StPh) sensors -- 4.3.2 Experiment 4.2: Streamflow data from static social (StSc) sensors -- 4.3.3 Experiment 4.3: Intermittent streamflow data from static social (StSc) sensors -- 4.3.4 Experiment 4.4: Heterogeneous network of static physical (StPh) and static social (StSc) sensors -- 4.4 Results and discussion -- 4.4.1 Experiment 4.1 -- 4.4.2 Experiment 4.2 -- 4.4.3 Experiment 4.3 -- 4.4.4 Experiment 4.4 -- 4.5 Conclusions 5: Assimilation of asynchronous data in hydrological models -- 5.1 Introduction -- 5.2 Methodology -- 5.2.1 Assimilation of asynchronous observations -- 5.2.2 Observation and model error -- 5.2.3 Generation of synthetic observations -- 5.3 Experimental setup -- 5.3.1 Experiment 5.1: Observations from a single static social (StSc) sensor -- 5.3.2 Experiments 5.2: Observations from distributed static physical (StPh) and static social (StSc) sensors -- 5.4 Results and discussion -- 5.4.1 Experiment 5.1 -- 5.4.2 Experiment 5.2 -- 5.5 Conclusions -- 6: Assimilation of synchronous data in hydraulic models -- 6.1 Introduction -- 6.2 Methodology -- 6.2.1 Data assimilation methods -- 6.2.2 Observation and model error -- 6.2.3 Streamflow observations -- 6.3 Experimental setup -- 6.3.1 Experiment 6.1: Effect of different DA methods -- 6.3.2 Experiment 6.2: Effect of sensors location on KF performances -- 6.4 Results and discussions -- 6.4.1 Experiment 6.1 -- 6.4.2 Experiment 6.2 -- 6.5 Conclusions -- 7: Assimilation of synchronous data in a cascade of models -- 7.1 Introduction -- 7.2 Methodology -- 7.2.1 Data assimilation method -- 7.2.2 Observation and model error -- 7.2.3 Generation of synthetic observations -- 7.3 Experimental setup -- 7.3.1 Experiment 7.1: Assimilation of data from static physical (StPh) sensors -- 7.3.2 Experiment 7.2: Assimilation of data from static social (StSc) sensors -- 7.3.3 Experiment 7.3: Assimilation of data from dynamic social (DySc) sensors -- 7.3.4 Experiment 7.4: Realistic scenarios of engagements -- 7.4 Results and discussion -- 7.4.1 Experiment 7.1 -- 7.4.2 Experiment 7.2 -- 7.4.3 Experiment 7.3 -- 7.4.4 Experiment 7.4 -- 7.5 Conclusions -- 8: Conclusions and recommendations -- 8.1 Overview -- 8.2 Research outcomes -- 8.3 Limitations and recommendations -- References -- List of acronyms -- List of Table -- List of Figures},
  isbn = {978-1-351-65256-8 978-1-138-03590-4},
  langid = {english},
  annotation = {OCLC: 1021121623},
  file = {/home/cjber/drive/pdf/Mazzoleni_2017_Improving Flood Prediction Assimilating Uncertain Crowdsourced Data into.pdf}
}

@article{mcculloch2001,
  title = {Ward-{{Level Deprivation}} and {{Individual Social}} and {{Economic Outcomes}} in the {{British Household Panel Study}}},
  author = {McCulloch, Andrew},
  year = {2001},
  pages = {18},
  doi = {10/dqz73b},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/mcculloch2001.pdf}
}

@article{mcmillen2010,
  title = {Issues in Spatial Data Analysis},
  author = {McMillen, Daniel P},
  year = {2010},
  journal = {Journal of Regional Science},
  volume = {50},
  number = {1},
  pages = {119--141},
  issn = {0022-4146},
  doi = {10/dnkvk8},
  keywords = {\#nosource}
}

@inproceedings{mcnamee2020,
  title = {Tagging {{Location Phrases}} in {{Text}}},
  booktitle = {Proceedings of {{The}} 12th {{Language Resources}} and {{Evaluation Conference}}},
  author = {McNamee, Paul and Mayfield, James and Costello, Cash and Bishop, Caitlyn and Anderson, Shelby},
  year = {2020},
  month = may,
  pages = {4521--4528},
  publisher = {{European Language Resources Association}},
  address = {{Marseille, France}},
  abstract = {For over thirty years researchers have studied the problem of automatically detecting named entities in written language. Throughout this time the majority of such work has focused on detection and classification of entities into coarse-grained types like: PERSON, ORGANIZATION, and LOCATION. Less attention has been focused on non-named mentions of entities, including non-named location phrases such as ``the medical clinic in Telonge'' or ``2 km below the Dolin Maniche bridge''. In this work we describe the Location Phrase Detection task to identify such spans. Our key accomplishments include: developing a sequential tagging approach; crafting annotation guidelines; building annotated datasets for English and Russian news; and, conducting experiments in automated detection of location phrases with both statistical and neural taggers. This work is motivated by extracting rich location information to support situational awareness during humanitarian crises such as natural disasters.},
  isbn = {979-10-95546-34-4},
  langid = {english},
  file = {/home/cjber/drive/pdf/McNamee et al_2020_Tagging Location Phrases in Text.pdf;/home/cjber/drive/pdf/McNamee et al_2020_Tagging Location Phrases in Text2.pdf}
}

@book{meier2014,
  title = {Migrant Professionals in the City: Local Encounters, Identities and Inequalities},
  author = {Meier, Lars},
  year = {2014},
  publisher = {{Routledge}},
  isbn = {1-134-67461-9},
  keywords = {\#nosource}
}

@article{mena2005,
  title = {An Automatic Method for Road Extraction in Rural and Semi-Urban Areas Starting from High Resolution Satellite Imagery},
  author = {Mena, J.B. and Malpica, J.A.},
  year = {2005},
  month = jul,
  journal = {Pattern Recognition Letters},
  volume = {26},
  number = {9},
  pages = {1201--1220},
  issn = {01678655},
  doi = {10/dhn6ck},
  abstract = {In this paper an efficient method for automatic road extraction in rural and semi-urban areas is presented. This work seeks the GIS update starting from color images and using preexisting vectorial information. As input data only the RGB bands of a satellite or aerial color image of high resolution is required. The system includes four different modules: data preprocessing; binary segmentation based on three levels of texture statistical evaluation; automatic vectorization by means of skeletal extraction; and finally a module for system evaluation. In the first module the color image is rectified and geo-referenced. The second module uses a new technique, named Texture Progressive Analysis (TPA), in order to obtain the segmented binary image. The TPA technique is developed in the evidence theory framework, and it consists in fusing information streaming from three different sources for the image. In the third module the obtained binary image is vectorized using an algorithm based on skeleton extraction techniques and morphological methods. The result is an extracted road network which is defined as a structural set of elements geometrically and topologically corrects. The fourth module is an evaluation of the procedure using a popular method. Experimental results show that this method is efficient in extracting and defining road networks from high resolution satellite and aerial imagery.},
  langid = {english},
  annotation = {00195},
  file = {/home/cjber/drive/pdf/ENVS492/Mena_Malpica_2005_.pdf;/home/cjber/drive/pdf/ENVS492/Mena_Malpica_2005_2.pdf}
}

@book{metcalfe2009,
  title = {Introductory {{Time Series}} with {{R}}},
  author = {Metcalfe, Andrew V. and Cowpertwait, Paul S.P.},
  year = {2009},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-88698-5},
  isbn = {978-0-387-88697-8 978-0-387-88698-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Metcalfe_Cowpertwait_2009_Introductory Time Series with R.pdf}
}

@misc{metoffice,
  title = {Met {{Office WOW}}},
  author = {{Met Office}},
  abstract = {The UK Met Office Weather Observation Website (WOW). WOW allows anyone to submit their own weather data, anywhere in the world.},
  howpublished = {https://wow.metoffice.gov.uk/},
  file = {/home/cjber/drive/zotero/storage/2RSJF9FZ/wow.metoffice.gov.uk.html}
}

@article{miao2014,
  title = {Road Centreline Extraction from Classified Images by Using the Geodesic Method},
  author = {Miao, Zelang and Shi, Wenzhong},
  year = {2014},
  month = apr,
  journal = {Remote Sensing Letters},
  volume = {5},
  number = {4},
  pages = {367--376},
  issn = {2150-704X, 2150-7058},
  doi = {10/gf3tdg},
  langid = {english},
  annotation = {ZSCC: 0000010},
  file = {/home/cjber/drive/pdf/ENVS492/Miao_Shi_2014_.pdf}
}

@article{middleton2014,
  title = {Real-{{Time Crisis Mapping}} of {{Natural Disasters Using Social Media}}},
  author = {Middleton, Stuart E. and Middleton, Lee and Modafferi, Stefano},
  year = {2014},
  month = mar,
  journal = {IEEE Intelligent Systems},
  volume = {29},
  number = {2},
  pages = {9--17},
  issn = {1541-1672},
  doi = {10/gfv7c6},
  abstract = {We present a social media crisis mapping platform for natural disasters. We take locations from gazetteer, street map and volunteered geographic information (VGI) sources for areas at risk of disaster and match them to geo-parsed real-time tweet data streams. We use statistical analysis to generate real-time crisis maps. Geo-parsing results are benchmarked against existing published work and evaluated across multi-lingual datasets. We report two case studies comparing 5-day tweet crisis maps to official post-event impact assessment from the US National Geospatial Agency (NGA) compiled from verified satellite and aerial imagery sources.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Middleton et al_2014_Real-Time Crisis Mapping of Natural Disasters Using Social Media.pdf}
}

@article{middleton2016,
  title = {Geoparsing and {{Geosemantics}} for {{Social Media}}: {{Spatio-Temporal Grounding}} of {{Content Propagating Rumours}} to Support {{Trust}} and {{Veracity Analysis}} during {{Breaking News}}},
  author = {Middleton, Stuart E and Krivcovs, Vadims},
  year = {2016},
  journal = {ACM Transactions on Information Systems},
  pages = {27},
  doi = {10/ggwjtt},
  langid = {english},
  file = {/home/cjber/drive/pdf/Middleton_Krivcovs_2016_Geoparsing and Geosemantics for Social Media.pdf}
}

@article{mikolov2013,
  title = {Efficient Estimation of Word Representations in Vector Space},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  journal = {arXiv preprint arXiv:1301.3781},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{miller1991,
  title = {Contextual Correlates of Semantic Similarity},
  author = {Miller, George A. and Charles, Walter G.},
  year = {1991},
  month = jan,
  journal = {Language and Cognitive Processes},
  volume = {6},
  number = {1},
  pages = {1--28},
  issn = {0169-0965, 1464-0732},
  doi = {10/bvrnz5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Miller_Charles_1991_Contextual correlates of semantic similarity.pdf}
}

@article{minten1999,
  title = {The Effect of Distance and Road Quality on Food Collection, Marketing Margins, and Traders' Wages: Evidence from the Former {{Zaire}}},
  shorttitle = {The Effect of Distance and Road Quality on Food Collection, Marketing Margins, and Traders' Wages},
  author = {Minten, Bart and Kyle, Steven},
  year = {1999},
  month = dec,
  journal = {Journal of Development Economics},
  volume = {60},
  number = {2},
  pages = {467--495},
  issn = {03043878},
  doi = {10/cdw8tb},
  abstract = {Food price variation is typical of the food economies of many low income countries. The presence or absence of road infrastructure is perceived to be one of the main determinants of this variation. This analysis shows that in the case of the former Zaire, food price dispersion is significant both across products and across regions. It is demonstrated that transportation costs explain most of the differences in food prices between producer regions and that road quality is an important factor in the transportation costs. However, food prices decrease relatively faster than transportation costs increase and traders' wages are higher on bad roads. q 1999 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  annotation = {ZSCC: 0000206},
  file = {/home/cjber/drive/pdf/ENVS492/Minten_Kyle_1999_.pdf}
}

@inproceedings{mintz2009,
  title = {Distant Supervision for Relation Extraction without Labeled Data},
  booktitle = {Proceedings of the {{Joint Conference}} of the 47th {{Annual Meeting}} of the {{ACL}} and the 4th {{International Joint Conference}} on {{Natural Language Processing}} of the {{AFNLP}}: {{Volume}} 2 - {{ACL-IJCNLP}} '09},
  author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  year = {2009},
  volume = {2},
  pages = {1003},
  publisher = {{Association for Computational Linguistics}},
  address = {{Suntec, Singapore}},
  doi = {10/fg9q43},
  abstract = {Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6\%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.},
  isbn = {978-1-932432-46-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Mintz et al_2009_Distant supervision for relation extraction without labeled data.pdf;/home/cjber/drive/pdf/Mintz et al_2009_Distant supervision for relation extraction without labeled data2.pdf}
}

@inproceedings{mirrezaei2016,
  title = {A Distantly Supervised Method for Extracting Spatio-Temporal Information from Text},
  booktitle = {Proceedings of the 24th {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  author = {Mirrezaei, Seyed Iman and Martins, Bruno and Cruz, Isabel F.},
  year = {2016},
  month = oct,
  pages = {1--4},
  publisher = {{ACM}},
  address = {{Burlingame California}},
  doi = {10/ggfmsw},
  abstract = {This paper describes TRIPLEX-ST, a novel information extraction system for collecting spatio-temporal information from textual resources. TRIPLEX-ST is based on a distantly supervised approach, which leverages rich linguistic annotations together with information in existing knowledge bases. In particular, we leverage triples associated with temporal and/or spatial contexts, e.g., as available from the YAGO knowledge base, so as to infer templates that capture new facts from previously unseen sentences.},
  isbn = {978-1-4503-4589-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Mirrezaei et al_2016_A distantly supervised method for extracting spatio-temporal information from.pdf;/home/cjber/drive/pdf/Mirrezaei et al_2016_A distantly supervised method for extracting spatio-temporal information from2.pdf}
}

@article{mitchell2001,
  title = {Multilevel Modeling Might Not Be the Answer},
  author = {Mitchell, R},
  year = {2001},
  journal = {Environment and Planning A},
  volume = {33},
  number = {8},
  pages = {1357--1360},
  keywords = {\#nosource,⛔ No DOI found}
}

@book{moller2013,
  title = {Modelling {{Computing Systems}}},
  author = {Moller, Faron and Struth, Georg},
  year = {2013},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-84800-322-4},
  isbn = {978-1-84800-321-7 978-1-84800-322-4},
  langid = {english},
  file = {/home/cjber/drive/pdf/Moller_Struth_2013_Modelling Computing Systems.pdf;/home/cjber/drive/pdf/Moller_Struth_2013_Modelling Computing Systems2.pdf}
}

@inproceedings{moncla2014,
  title = {Geocoding for Texts with Fine-Grain Toponyms: An Experiment on a Geoparsed Hiking Descriptions Corpus},
  shorttitle = {Geocoding for Texts with Fine-Grain Toponyms},
  booktitle = {Proceedings of the 22nd {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}} - {{SIGSPATIAL}} '14},
  author = {Moncla, Ludovic and {Renteria-Agualimpia}, Walter and {Nogueras-Iso}, Javier and Gaio, Mauro},
  year = {2014},
  pages = {183--192},
  publisher = {{ACM Press}},
  address = {{Dallas, Texas}},
  doi = {10/ggwjs8},
  abstract = {Geoparsing and geocoding are two essential middleware services to facilitate final user applications such as locationaware searching or different types of location-based services. The objective of this work is to propose a method for establishing a processing chain to support the geoparsing and geocoding of text documents describing events strongly linked with space and with a frequent use of fine-grain toponyms. The geoparsing part is a Natural Language Processing approach which combines the use of part of speech and syntactico-semantic combined patterns (cascade of transducers). However, the real novelty of this work lies in the geocoding method. The geocoding algorithm is unsupervised and takes profit of clustering techniques to provide a solution for disambiguating the toponyms found in gazetteers, and at the same time estimating the spatial footprint of those other fine-grain toponyms not found in gazetteers. The feasibility of the proposal has been tested with a corpus of hiking descriptions in French, Spanish and Italian.},
  isbn = {978-1-4503-3131-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/Moncla et al_2014_Geocoding for texts with fine-grain toponyms.pdf}
}

@article{montello2003,
  title = {Where's {{Downtown}}?: {{Behavioral Methods}} for {{Determining Referents}} of {{Vague Spatial Queries}}},
  author = {Montello, Daniel R and Goodchild, Michael F and Gottsegen, Jonathon and Fohl, Peter},
  year = {2003},
  pages = {20},
  langid = {english},
  keywords = {❓ Multiple DOI},
  file = {/home/cjber/drive/pdf/Montello et al_2003_Where's Downtown.pdf}
}

@article{montello2003a,
  title = {Regions in Geography: {{Process}} and Content},
  author = {Montello, Daniel R},
  year = {2003},
  journal = {Foundations of geographic information science},
  pages = {173--189},
  publisher = {{Taylor \& Francis London}},
  doi = {10/d4tbk4}
}

@article{montello2014,
  title = {Vague Cognitive Regions in Geography and Geographic Information Science},
  author = {Montello, Daniel R. and Friedman, Alinda and Phillips, Daniel W.},
  year = {2014},
  month = sep,
  journal = {International Journal of Geographical Information Science},
  volume = {28},
  number = {9},
  pages = {1802--1820},
  issn = {1365-8816, 1362-3087},
  doi = {10/gg2vnp},
  langid = {english},
  file = {/home/cjber/drive/pdf/Montello et al_2014_Vague cognitive regions in geography and geographic information science.pdf}
}

@inproceedings{moore2006,
  title = {Recent Landslide Impacts on the {{UK Scottish}} Road Network: Investigation into the Mechanisms, Causes and Management of Landslide Risk},
  booktitle = {Proceedings of the {{International Conference}} on {{Slopes}}, {{Kuala Lumpur}}, {{Malaysia}}},
  author = {Moore, R and Carey, J and Mills, A and Martin, S and Irinder, S and Kerry, L and Leask, G and Simmons, A and Ashaari, M},
  year = {2006},
  pages = {223--237},
  publisher = {{Public Works Department, Kuala Lumpur, Malaysia}},
  keywords = {\#nosource},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{morstatter2013,
  title = {Is the {{Sample Good Enough}}? {{Comparing Data}} from {{Twitter}}'s {{Streaming API}} with {{Twitter}}'s {{Firehose}}},
  author = {Morstatter, Fred and Pfeffer, Juergen and Liu, Huan and Carley, Kathleen M},
  year = {2013},
  pages = {9},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Morstatter et al_2013_Is the Sample Good Enough.pdf}
}

@article{mortensen2012,
  title = {Attitudes towards Motherhood and Fertility Awareness among 20\textendash 40-Year-Old Female Healthcare Professionals},
  author = {Mortensen, Luise Lermark and Hegaard, Hanne Kristine and Andersen, Anders Nyboe and Bentzen, Janne Gasseholm},
  year = {2012},
  journal = {The European Journal of Contraception \& Reproductive Health Care},
  volume = {17},
  number = {6},
  pages = {468--481},
  issn = {1362-5187},
  doi = {10/gf33r3},
  keywords = {\#nosource}
}

@article{mountain1996,
  title = {Accident Prediction Models for Roads with Minor Junctions},
  author = {Mountain, Linda and Fawaz, Bachir and Jarrett, David},
  year = {1996},
  month = nov,
  journal = {Accident Analysis \& Prevention},
  volume = {28},
  number = {6},
  pages = {695--707},
  issn = {00014575},
  doi = {10/bq4gkw},
  langid = {english},
  annotation = {00136},
  file = {/home/cjber/drive/pdf/ENVS492/Road Assessment/Mountain et al_1996_.pdf}
}

@article{moura2017,
  title = {Reference Data Enhancement for Geographic Information Retrieval Using Linked Data},
  author = {Moura, Tiago H. V. M. and Davis, Clodoveu A. and Fonseca, Frederico T.},
  year = {2017},
  journal = {Transactions in GIS},
  volume = {21},
  number = {4},
  pages = {683--700},
  issn = {1467-9671},
  doi = {10/gbr3x7},
  abstract = {Gazetteers are instrumental in recognizing place names in documents such as Web pages, news, and social media messages. However, creating and maintaining gazetteers is still a complex task. Even though some online gazetteers provide rich sets of geographic names in planetary scale (e.g. GeoNames), other sources must be used to recognize references to urban locations, such as street names, neighborhood names or landmarks. We propose integrating Linked Data sources to create a gazetteer that combines a broad coverage of places with urban detail, including content on geographic and semantic relationships involving places, their multiple names and related non-geographic entities. Our final goal is to expand the possibilities for recognizing, disambiguating and filtering references to places in texts for geographic information retrieval (GIR) and related applications. The resulting ontological gazetteer, named LoG (Linked OntoGazetteer), is accessible through Web services by applications and research initiatives on GIR, text processing, named entity recognition and others. The gazetteer currently contains over 13 million places, 140 million attributes and relationships, and 4.5 million non-geographic entities. Data sources include GeoNames, Freebase, DBPedia and LinkedGeoData, which is based on OpenStreetMap data. An analysis on how these datasets overlap and complement one another is also presented.},
  langid = {english},
  keywords = {gazetteer,geocoding,knowledge bases,linked data},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tgis.12238},
  file = {/home/cjber/drive/pdf/Moura et al_2017_Reference data enhancement for geographic information retrieval using linked.pdf;/home/cjber/drive/zotero/storage/8V75C2BN/tgis.html}
}

@article{mullen2015,
  title = {Assessing the Impact of Demographic Characteristics on Spatial Error in Volunteered Geographic Information Features},
  author = {Mullen, William F. and Jackson, Steven P. and Croitoru, Arie and Crooks, Andrew and Stefanidis, Anthony and Agouris, Peggy},
  year = {2015},
  month = aug,
  journal = {GeoJournal},
  volume = {80},
  number = {4},
  pages = {587--605},
  issn = {0343-2521, 1572-9893},
  doi = {10/gfj2fm},
  langid = {english},
  file = {/home/cjber/drive/pdf/Mullen et al_2015_Assessing the impact of demographic characteristics on spatial error in.pdf}
}

@article{muller2015,
  title = {Crowdsourcing for Climate and Atmospheric Sciences: Current Status and Future Potential},
  shorttitle = {Crowdsourcing for Climate and Atmospheric Sciences},
  author = {Muller, C. L. and Chapman, L. and Johnston, S. and Kidd, C. and Illingworth, S. and Foody, G. and Overeem, A. and Leigh, R. R.},
  year = {2015},
  journal = {International Journal of Climatology},
  volume = {35},
  number = {11},
  pages = {3185--3203},
  issn = {1097-0088},
  doi = {10/f7qrps},
  abstract = {Crowdsourcing is traditionally defined as obtaining data or information by enlisting the services of a (potentially large) number of people. However, due to recent innovations, this definition can now be expanded to include `and/or from a range of public sensors, typically connected via the Internet.' A large and increasing amount of data is now being obtained from a huge variety of non-traditional sources \textendash{} from smart phone sensors to amateur weather stations to canvassing members of the public. Some disciplines (e.g. astrophysics, ecology) are already utilizing crowdsourcing techniques (e.g. citizen science initiatives, web 2.0 technology, low-cost sensors), and while its value within the climate and atmospheric science disciplines is still relatively unexplored, it is beginning to show promise. However, important questions remain; this paper introduces and explores the wide-range of current and prospective methods to crowdsource atmospheric data, investigates the quality of such data and examines its potential applications in the context of weather, climate and society. It is clear that crowdsourcing is already a valuable tool for engaging the public, and if appropriate validation and quality control procedures are adopted and implemented, it has much potential to provide a valuable source of high temporal and spatial resolution, real-time data, especially in regions where few observations currently exist, thereby adding value to science, technology and society.},
  langid = {english},
  keywords = {Amateur,Applications,Big data,Citizen science,Internet of things,Sensors},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/joc.4210},
  file = {/home/cjber/drive/pdf/Muller et al_2015_Crowdsourcing for climate and atmospheric sciences.pdf;/home/cjber/drive/zotero/storage/VUUZPAPQ/joc.html}
}

@article{mulley2009,
  title = {Flexible Transport Services: {{A}} New Market Opportunity for Public Transport},
  shorttitle = {Flexible Transport Services},
  author = {Mulley, Corinne and Nelson, John D.},
  year = {2009},
  month = jan,
  journal = {Research in Transportation Economics},
  volume = {25},
  number = {1},
  pages = {39--45},
  issn = {07398859},
  doi = {10/chspr5},
  abstract = {The term Demand-Responsive Transport (DRT) has been increasingly applied in the last 10 years to a niche market that replaces or feeds (usually via small low floor buses or taxis) conventional transport where demand is low and often spread over a large area. More recently, the concept of DRT as a niche market has been broadened to include a wider range of flexible, demand-responsive transport services and is increasingly referred to as Flexible Transport Services (FTSs). The contention of this paper is that well-implemented FTS has the potential to revitalise bus-based public transport services which are traditionally based on fixed networks with variable geographical coverage and levels of service.},
  langid = {english},
  annotation = {ZSCC: 0000133},
  file = {/home/cjber/drive/pdf/ENVS492/Mulley_Nelson_2009_.pdf}
}

@misc{mungall2005,
  title = {Trend towards Centralisation of Hospital Services,},
  author = {Mungall, IJ},
  year = {2005},
  abstract = {There are trends towards centralising hospital care within the NHS, for many excellent reasons. Yet this has a disproportionate  impact upon those patients who live at a distance from their hospital, and this impact has not been well reported or researched, but  studies have demonstrated that the utilisation of services is inversely related to the distance of the patient from the hospital; so-called  `distance decay'. This article examines the trend and describes the reasons for it, and the impact on those living in remote and rural  communities. It argues that health service planning needs to be patient centred, and points out that, although providing services to  rural communities is more expensive than that for urban populations, a balance needs to be struck between cost-effectiveness and  the provision of accessible and equitable services for all of our patients. It argues for a debate to help define where that balance point  should be, and makes certain recommendations. This debate is especially important, because the professions urgently need to decide},
  annotation = {00000},
  file = {/home/cjber/drive/pdf/ENVS492/Road Assessment/mungall2005.pdf}
}

@article{muscat2010,
  title = {Area {{Based Initiatives}} - Do They Deliver?},
  author = {Muscat, Richard},
  year = {2010},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/Muscat_2010_.pdf}
}

@article{nadeau2007,
  title = {A Survey of Named Entity Recognition and Classification},
  author = {Nadeau, David and Sekine, Satoshi},
  year = {2007},
  pages = {20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Nadeau_Sekine_2007_A survey of named entity recognition and classification.pdf}
}

@article{nain,
  title = {Approaching {{Almost Any Machine Learning Problem}}},
  author = {Nain, Aakash and Soni, Aditya and M{\"u}ller, Andreas and Lukyanenko, Andrey and Roy, Ayon and Tunguz, Bojan and Jr, Gilberto Titericz and Banachewicz, Konrad and Massaron, Luca and Barman, Nabajeet and Pandey, Parul and Ramrakhya, Ram and Bhutani, Sanyam and Rajkumar, Sudalai and Abraham, Tanishq and Reade, Walter and Reina, Yuval},
  pages = {300},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Nain et al_Approaching Almost Any Machine Learning Problem.pdf;/home/cjber/drive/pdf/Nain et al_Approaching Almost Any Machine Learning Problem2.pdf}
}

@article{nakayama2018,
  title = {Doccano: Text Annotation for Humans},
  author = {Nakayama, Hiroki and Kubo, Takahiro and Kamura, Junya and Taniguchi, Yasufumi and Liang, Xu},
  year = {2018},
  keywords = {⛔ No DOI found}
}

@article{napoli2020,
  title = {Accelerating {{Multi-attribute Unsupervised Seismic Facies Analysis With RAPIDS}}},
  author = {Napoli, Ot{\'a}vio O. and do Rosario, Vanderson Martins and Navarro, Jo{\~a}o Paulo and e Silva, Pedro M{\'a}rio Cruz and Borin, Edson},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.15152 [cs]},
  eprint = {2007.15152},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Classification of seismic facies is done by clustering seismic data samples based on their attributes. Year after year, the 3D datasets used in exploration geophysics constantly increase in size, complexity, and number of attributes, requiring a continuous rise in the classification efficiency. In this work, we explore the use of Graphics Processing Units (GPUs) to perform the classification of seismic surveys using the well-established machine learning method k-means. We show that the high-performance distributed implementation of the k-means algorithm available at the NVIDIA RAPIDS library can be used to classify facies of large seismic datasets much faster than a classical parallel CPU implementation (up to 258-fold faster in NVIDA TESLAs GPUs), especially for large seismic blocks. We tested the algorithm with different real seismic volumes, including netherlands, parihaka, and kahu (from 12GB to 66GB).},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/home/cjber/drive/pdf/Napoli et al_2020_Accelerating Multi-attribute Unsupervised Seismic Facies Analysis With RAPIDS.pdf;/home/cjber/drive/zotero/storage/R4PEQYT2/2007.html}
}

@inproceedings{narvaez2012,
  title = {Space Syntax Economics: Decoding Accessibility Using Property Value and Housing Price in {{Cardiff}}, {{Wales}}},
  booktitle = {Proceedings of the {{Eighth International Space Syntax Symposium}}, {{Santiago}} de {{Chile}}},
  author = {Narvaez, Laura and Penn, Alan and Griffiths, Sam},
  year = {2012},
  pages = {1--19},
  file = {/home/cjber/drive/pdf/ENVS453/narvaez2012.pdf}
}

@article{navigli2009,
  title = {Word Sense Disambiguation: {{A}} Survey},
  shorttitle = {Word Sense Disambiguation},
  author = {Navigli, Roberto},
  year = {2009},
  month = feb,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {2},
  pages = {1--69},
  issn = {03600300},
  doi = {10/dhsjt2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Navigli_2009_Word sense disambiguation.pdf}
}

@article{nguyen2017,
  title = {Robust {{Classification}} of {{Crisis-Related Data}} on {{Social Networks Using Convolutional Neural Networks}}},
  author = {Nguyen, Dat and Mannai, Kamela Ali Al and Joty, Shafiq and Sajjad, Hassan and Imran, Muhammad and Mitra, Prasenjit},
  year = {2017},
  month = may,
  journal = {Proceedings of the International AAAI Conference on Web and Social Media},
  volume = {11},
  number = {1},
  pages = {632--635},
  issn = {2334-0770},
  abstract = {The role of social media, in particular microblogging platforms such as Twitter, as a conduit for actionable and tactical information during disasters is increasingly acknowledged. However, time-critical analysis of big crisis data on social media streams brings challenges to machine learning techniques, especially the ones that use supervised learning. The scarcity of labeled data, particularly in the early hours of a crisis, delays the learning process. Existing classification methods require a significant amount of labeled data specific to a particular event for training plus a lot of feature engineering to achieve best results. In this work, we introduce neural network based classification methods for identifying useful tweets during a crisis situation. At the onset of a disaster when no labeled data is available, our proposed method makes the best use of the out-of-event data and achieves good results.},
  copyright = {Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Nguyen et al_2017_Robust Classification of Crisis-Related Data on Social Networks Using.pdf}
}

@article{nguyen2020,
  title = {{{BERTweet}}: {{A}} Pre-Trained Language Model for {{English Tweets}}},
  shorttitle = {{{BERTweet}}},
  author = {Nguyen, Dat Quoc and Vu, Thanh and Nguyen, Anh Tuan},
  year = {2020},
  month = oct,
  journal = {arXiv:2005.10200 [cs]},
  eprint = {2005.10200},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present BERTweet, the first public largescale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERTbase (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTabase and XLM-Rbase (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at: https://github.com/ VinAIResearch/BERTweet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/cjber/drive/zotero/storage/KEMYF82B/Nguyen et al. - 2020 - BERTweet A pre-trained language model for English.pdf}
}

@misc{nhsengland2018,
  title = {Ambulance {{Response Programme Review}}},
  author = {{NHS England}},
  year = {2018},
  howpublished = {https://www.england.nhs.uk/wp-content/uploads/2018/10/ambulance-response-programme-review.pdf},
  file = {/home/cjber/drive/pdf/NHS England_2018_Ambulance Response Programme Review.pdf}
}

@misc{nhsengland2021,
  title = {Statistics \guillemotright{} {{COVID-19 Vaccinations}}},
  author = {NHS England},
  year = {2021},
  howpublished = {https://www.england.nhs.uk/statistics/statistical-work-areas/covid-19-vaccinations/},
  file = {/home/cjber/drive/zotero/storage/57MLBPYX/covid-19-vaccinations.html}
}

@article{nicholl2007,
  title = {The Relationship between Distance to Hospital and Patient Mortality in Emergencies: An Observational Study},
  shorttitle = {The Relationship between Distance to Hospital and Patient Mortality in Emergencies},
  author = {Nicholl, J. and West, J. and Goodacre, S. and Turner, J.},
  year = {2007},
  month = sep,
  journal = {Emergency Medicine Journal},
  volume = {24},
  number = {9},
  pages = {665--668},
  issn = {1472-0205, 1472-0213},
  doi = {10/bqkh5k},
  abstract = {Objectives: Reconfiguration of emergency services could lead to patients with life-threatening conditions travelling longer distances to hospital. Concerns have been raised that this could increase the risk of death. We aimed to determine whether distance to hospital was associated with mortality in patients with lifethreatening emergencies. Methods: We undertook an observational cohort study of 10 315 cases transported with a potentially lifethreatening condition (excluding cardiac arrests) by four English ambulance services to associated acute hospitals, to determine whether distance to hospital was associated with mortality, after adjustment for age, sex, clinical category and illness severity. Results: Straight-line ambulance journey distances ranged from 0 to 58 km with a median of 5 km, and 644 patients died (6.2\%). Increased distance was associated with increased risk of death (odds ratio 1.02 per kilometre; 95\% CI 1.01 to 1.03; p,0.001). This association was not changed by adjustment for confounding by age, sex, clinical category or illness severity. Patients with respiratory emergencies showed the greatest association between distance and mortality. Conclusion: Increased journey distance to hospital appears to be associated with increased risk of mortality. Our data suggest that a 10-km increase in straight-line distance is associated with around a 1\% absolute increase in mortality.},
  langid = {english},
  keywords = {ES},
  annotation = {00162},
  file = {/home/cjber/drive/pdf/ENVS492/Nicholl et al_2007_.pdf}
}

@inproceedings{nichols2015,
  title = {{{SpRL-CWW}}: {{Spatial Relation Classification}} with {{Independent Multi-class Models}}},
  shorttitle = {{{SpRL-CWW}}},
  booktitle = {Proceedings of the 9th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval}} 2015)},
  author = {Nichols, Eric and Botros, Fadi},
  year = {2015},
  month = jun,
  pages = {895--901},
  publisher = {{Association for Computational Linguistics}},
  address = {{Denver, Colorado}},
  doi = {10/gg25xp},
  file = {/home/cjber/drive/pdf/Nichols_Botros_2015_SpRL-CWW.pdf}
}

@article{noble2006,
  title = {Measuring {{Multiple Deprivation}} at the {{Small-Area Level}}},
  author = {Noble, Michael and Wright, Gemma and Smith, George and Dibben, Chris},
  year = {2006},
  month = jan,
  journal = {Environment and Planning A: Economy and Space},
  volume = {38},
  number = {1},
  pages = {169--185},
  issn = {0308-518X, 1472-3409},
  doi = {10/bpkvf2},
  abstract = {Indices to measure deprivation at a small-area level have been used in the United Kingdom to target regeneration policy for over thirty years. The development of the Indices of Deprivation 2000 for England and comparable indices for Northern Ireland, Wales, and Scotland, involved a fundamental reappraisal and reconceptualisation of small-area level multiple deprivation and its measurement. Multiple deprivation is articulated as an accumulation of discrete dimensions or `domains' of deprivation. This paper presents the key principles that were taken into consideration when constructing these four indices and the more recent English Indices of Deprivation 2004, and provides an account of the statistical techniques that were used to operationalise them.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Noble et al_2006_.pdf}
}

@misc{noctor2004,
  title = {Change to Kph Limit to Cost \texteuro 30m},
  author = {Noctor, Ian},
  year = {2004},
  journal = {The Irish Times},
  abstract = {Ireland's road sign manufacturers can expect a bumper year with the Minister for Transport's confirmation that metric speed limits\ldots},
  howpublished = {https://www.irishtimes.com/life-and-style/motors/change-to-kph-limit-to-cost-30m-1.1133469},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/zotero/storage/WIC6XM8Y/change-to-kph-limit-to-cost-30m-1.html}
}

@article{noland2004,
  title = {A Spatially Disaggregate Analysis of Road Casualties in {{England}}},
  author = {Noland, Robert B. and Quddus, Mohammed A.},
  year = {2004},
  month = nov,
  journal = {Accident Analysis \& Prevention},
  volume = {36},
  number = {6},
  pages = {973--984},
  issn = {00014575},
  doi = {10/bnfnj3},
  abstract = {Spatially disaggregate ward level data for England is used in an analysis of various area-wide factors on road casualties. Data on 8414 wards was input into a geographic information system that contained data on land use types, road characteristics and road casualties. Demographic data on area-wide deprivation (the index of multiple deprivation) for each ward was also included. Negative binomial count data models were used to analyze the associations between these factors with traffic fatalities, serious injuries and slight injuries. Results suggest that urbanized areas are associated with fewer casualties (especially fatalities) while areas of higher employment density are associated with more casualties. More deprived areas tend to have higher levels of casualties, though not of motorized casualties (except slight injuries). The effect of road characteristics are less significant but there are some positive associations with the density of ``A'' and ``B'' level roads.},
  langid = {english},
  annotation = {ZSCC: 0000280},
  file = {/home/cjber/drive/pdf/ENVS492/Noland_Quddus_2004_.pdf}
}

@article{northcutt2021,
  title = {Pervasive {{Label Errors}} in {{Test Sets Destabilize Machine Learning Benchmarks}}},
  author = {Northcutt, Curtis G. and Athalye, Anish and Mueller, Jonas},
  year = {2021},
  month = apr,
  journal = {arXiv:2103.14749 [cs, stat]},
  eprint = {2103.14749},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of 3.4\% errors across the 10 datasets,1 where for example 2916 label errors comprise 6\% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (54\% of the algorithmically-flagged candidates are indeed erroneously labeled). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy \textemdash{} our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet50 if the prevalence of originally mislabeled test examples increases by just 6\%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/cjber/drive/pdf/Northcutt et al_2021_Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.pdf}
}

@article{nothman2013,
  title = {Learning Multilingual Named Entity Recognition from {{Wikipedia}}},
  author = {Nothman, Joel and Ringland, Nicky and Radford, Will and Murphy, Tara and Curran, James R.},
  year = {2013},
  month = jan,
  journal = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
  volume = {194},
  pages = {151--175},
  issn = {0004-3702},
  doi = {10/f4kjjw},
  abstract = {We automatically create enormous, free and multilingual silver-standard training annotations for named entity recognition (ner) by exploiting the text and structure of Wikipedia. Most ner systems rely on statistical models of annotated data to identify and classify names of people, locations and organisations in text. This dependence on expensive annotation is the knowledge bottleneck our work overcomes. We first classify each Wikipedia article into named entity (ne) types, training and evaluating on 7200 manually-labelled Wikipedia articles across nine languages. Our cross-lingual approach achieves up to 95\% accuracy. We transform the links between articles into ne annotations by projecting the target article's classifications onto the anchor text. This approach yields reasonable annotations, but does not immediately compete with existing gold-standard data. By inferring additional links and heuristically tweaking the Wikipedia corpora, we better align our automatic annotations to gold standards. We annotate millions of words in nine languages, evaluating English, German, Spanish, Dutch and Russian Wikipedia-trained models against conll shared task data and other gold-standard corpora. Our approach outperforms other approaches to automatic ne annotation (Richman and Schone, 2008 [61], Mika et al., 2008 [46]) competes with gold-standard training when tested on an evaluation corpus from a different source; and performs 10\% better than newswire-trained models on manually-annotated Wikipedia text.},
  keywords = {Annotated corpora,Information extraction,Named entity recognition,Semi-structured resources,Semi-supervised learning,Wikipedia},
  file = {/home/cjber/drive/pdf/pdf}
}

@article{oberguggenberger2011,
  title = {Analysis for Computer Scientists: Foundations, Methods, and Algorithms},
  shorttitle = {Analysis for Computer Scientists},
  author = {Oberguggenberger, Michael},
  year = {2011},
  month = sep,
  journal = {Choice Reviews Online},
  volume = {49},
  number = {01},
  pages = {49-0324-49-0324},
  issn = {0009-4978, 1523-8253},
  doi = {10.5860/CHOICE.49-0324},
  langid = {english},
  file = {/home/cjber/drive/pdf/Oberguggenberger_2011_Analysis for computer scientists.pdf}
}

@article{oeppen2002,
  title = {Broken Limits to Life Expectancy},
  author = {Oeppen, Jim and Vaupel, James W},
  year = {2002},
  issn = {0036-8075},
  doi = {10/bhggx2},
  file = {/home/cjber/drive/pdf/ENVS418/Oeppen_Vaupel_2002_.pdf}
}

@misc{ofcom2014,
  title = {Location Information for Emergency Calls from Mobile Phones},
  author = {{Ofcom}},
  year = {2014},
  keywords = {ES},
  file = {/home/cjber/drive/pdf/Ofcom_2014_Location information for emergency calls from mobile phones.pdf;/home/cjber/drive/pdf/Ofcom_2014_Location information for emergency calls from mobile phones2.pdf}
}

@article{officefornationalstatistics2010,
  title = {Comparing across {{Countries}}' {{Indices}} of {{Deprivation Guidance Paper}}},
  author = {Office For National Statistics},
  year = {2010},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/Office For National Statistics_2010_.pdf}
}

@article{officefornationalstatistics2011,
  title = {2011 {{Census}} Aggegate Data},
  shorttitle = {2011 {{Census}} Aggegate Data ({{Data}} Downloaded},
  author = {Office For National Statistics},
  year = {2011},
  doi = {10/gfzjpp},
  keywords = {\#nosource}
}

@article{ogren2006,
  title = {Building and {{Evaluating Annotated Corpora}} for {{Medical NLP Systems}}},
  author = {Ogren, Philip V. and Savova, Guergana and Buntrock, James D. and Chute, Christopher G.},
  year = {2006},
  journal = {AMIA Annual Symposium Proceedings},
  volume = {2006},
  pages = {1050},
  issn = {1942-597X},
  abstract = {We report efforts from a pilot annotation project in which we annotated sixty clinical notes with disorder mentions. Four retrieval experts annotated the same clinical notes so that four-way inter-annotator agreement could be calculated. We find the inter-annotator agreement results encouraging to scale up the project using the same guidelines and annotation schema.},
  pmcid = {PMC1839264},
  pmid = {17238668},
  file = {/home/cjber/drive/pdf/Ogren et al_2006_Building and Evaluating Annotated Corpora for Medical NLP Systems.pdf}
}

@article{okeefe2018,
  title = {Deep {{Learning}} and {{Word Embeddings}} for {{Tweet Classification}} for {{Crisis Response}}},
  author = {O'Keefe, Simon Edward Marius and Alrashdi, Reem Mansour M},
  year = {2018},
  pages = {6},
  abstract = {\~NTradition tweet classification models for crisis response focus on convolutional layers and domain-specific word embeddings. In this paper, we study the application of different neural networks with general-purpose and domain-specific word embeddings to investigate their ability to improve the performance of tweet classification models. We evaluate four tweet classification models on CrisisNLP dataset and obtain comparable results which indicates that general-purpose word embedding such as GloVe can be used instead of domain-specific word embedding especially with Bi-LSTM where results reported the highest performance of 62.04\% F1 score.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/O'Keefe_Alrashdi_2018_Deep Learning and Word Embeddings for Tweet Classification for Crisis Response.pdf}
}

@article{okeeffe2011,
  title = {Role of Ambulance Response Times in the Survival of Patients with Out-of-Hospital Cardiac Arrest},
  author = {O'Keeffe, C. and Nicholl, J. and Turner, J. and Goodacre, S.},
  year = {2011},
  month = aug,
  journal = {Emergency Medicine Journal},
  volume = {28},
  number = {8},
  pages = {703--706},
  issn = {1472-0205, 1472-0213},
  doi = {10/bnd72s},
  abstract = {Objectives To evaluate the role of ambulance response times in improving survival for out-of-hospital cardiac arrest (OHCA). Methods OHCAs were identified by sampling consecutive life-threatening category A emergency ambulance calls on an annual basis for the 5 years 1996/ 7e2000/1 from four ambulance services in England. From these, all calls where an ambulance arrived at the scene and treated or transported a patient were included in the study. These cohorts of patients were followed up to discharge from hospital. Results Overall, 30 (2.6\%) of the 1161 patients with cardiac arrest survived to hospital discharge. If the patient arrested while the paramedics were on scene, survival to hospital discharge was 14\%. The most important predictive factors for survival were response time, initial presenting heart rhythm in ventricular fibrillation and whether the arrest was witnessed. The estimated effect of a 1 min reduction in response time was to improve the odds of survival by 24\% (95\% CI 4\% to 48\%). The costs of reducing response times across the board by 1 min at the time of this study were estimated at around \textsterling 54 million. Conclusions The arrival of a crew prior to OHCA means that the chance of surviving the arrest increases sevenfold. Overall it is possible that rapid response to patients in immediate risk of arrest may be at least as beneficial as rapid response to those who have arrested. Concentrating resources on reducing response times across the board to improve survival for those patients already in arrest is unlikely to be a cost-effective option to the UK National Health Service.},
  langid = {english},
  file = {/home/cjber/drive/pdf/O'Keeffe et al_2011_Role of ambulance response times in the survival of patients with.pdf}
}

@article{olah2003,
  title = {Gendering {{Fertility}}},
  author = {Ol{\'a}h, Livia Sz.},
  year = {2003},
  journal = {Population Research and Policy Review},
  volume = {22},
  number = {2},
  pages = {171--200},
  issn = {01675923},
  doi = {10/c9bvhz},
  abstract = {With the growing prevalence of the dual-earner family model in industrialized countries the gendered nature of the relationship between employment and parenting has become a key issue for childbearing decisions and behavior. In such a context taking into account the societal gender structure (public policies, family-level gender relations) explicitly can enhance our understanding of contemporary fertility trends. In this paper we study the second birth, given its increasing importance in the developed world as large proportions of women remain childless or bear only one child. We focus on Sweden where gender equality is pronounced at both the societal and the family level and on Hungary where the dual-earner model has been accompanied by traditional gender relations in the home sphere. Our analysis is based on data extracted from the Swedish and Hungarian Fertility and Family Surveys of 1992/93. We use the method of hazard regression. The results suggest that the secondbirth intensity increases as the combination of parenthood and labor-force attachment of either parent is facilitated. We see this in the effect of family policies in Sweden and in the higher second-birth intensity of couples who share family responsibilities as compared to those with traditional gender-role behavior in both countries. Also, the lack of any visible impact of men's educational attainment in both Sweden and Hungary is probably linked to public policies as state support for families with children has reduced the importance of income for second childbearing. A positive educational gradient for Swedish women and an essentially zero gradient in Hungary reflects the success of policy measures in reducing fertility cost for more educated women in both countries.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS418/Oláh_2003_.pdf}
}

@inproceedings{olteanu2015,
  title = {What to {{Expect When}} the {{Unexpected Happens}}: {{Social Media Communications Across Crises}}},
  shorttitle = {What to {{Expect When}} the {{Unexpected Happens}}},
  booktitle = {Proceedings of the 18th {{ACM Conference}} on {{Computer Supported Cooperative Work}} \& {{Social Computing}}},
  author = {Olteanu, Alexandra and Vieweg, Sarah and Castillo, Carlos},
  year = {2015},
  month = feb,
  pages = {994--1009},
  publisher = {{ACM}},
  address = {{Vancouver BC Canada}},
  doi = {10/gmmfbh},
  abstract = {The use of social media to communicate timely information during crisis situations has become a common practice in recent years. In particular, the one-to-many nature of Twitter has created an opportunity for stakeholders to disseminate crisis-relevant messages, and to access vast amounts of information they may not otherwise have. Our goal is to understand what affected populations, response agencies and other stakeholders can expect\textemdash and not expect\textemdash from these data in various types of disaster situations. Anecdotal evidence suggests that different types of crises elicit different reactions from Twitter users, but we have yet to see whether this is in fact the case. In this paper, we investigate several crises\textemdash including natural hazards and human-induced disasters\textemdash in a systematic manner and with a consistent methodology. This leads to insights about the prevalence of different information types and sources across a variety of crisis situations.},
  isbn = {978-1-4503-2922-4},
  langid = {english},
  file = {/home/cjber/drive/pdf/Olteanu et al_2015_What to Expect When the Unexpected Happens.pdf}
}

@book{olver2018,
  title = {Applied {{Linear Algebra}}},
  author = {Olver, Peter J. and Shakiban, Chehrzad},
  year = {2018},
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-91041-3},
  isbn = {978-3-319-91040-6 978-3-319-91041-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Olver_Shakiban_2018_Applied Linear Algebra.pdf;/home/cjber/drive/pdf/Olver_Shakiban_2018_Applied Linear Algebra2.pdf}
}

@article{ons2018,
  title = {Ageing - {{Office}} for {{National Statistics}}},
  author = {{ONS} and {Office for National Statistics}},
  year = {2018},
  abstract = {Ageing population - office for national statistics.},
  mendeley-groups = {ENVS450/Assessment 2,ENVS453/Assessment 2},
  keywords = {\#nosource}
}

@misc{ons2019,
  title = {Census Geography - {{Office}} for {{National Statistics}}},
  author = {ONS},
  year = {2019},
  howpublished = {https://www.ons.gov.uk/methodology/geography/ukgeographies/censusgeography},
  file = {/home/cjber/drive/zotero/storage/49CIWLNG/censusgeography.html}
}

@misc{ons2019a,
  title = {Middle {{Super Output Area}} Population Estimates (Supporting Information) - {{Office}} for {{National Statistics}}},
  author = {{ONS}},
  year = {2019},
  howpublished = {https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/middlesuperoutputareamidyearpopulationestimates},
  file = {/home/cjber/drive/zotero/storage/7XIDTAKC/middlesuperoutputareamidyearpopulationestimates.html}
}

@article{ooms2014,
  title = {The Jsonlite Package: {{A}} Practical and Consistent Mapping between {{JSON}} Data and r Objects},
  author = {Ooms, Jeroen},
  year = {2014},
  journal = {arXiv:1403.2805 [stat.CO]},
  eprint = {1403.2805},
  eprinttype = {arxiv},
  primaryclass = {stat.CO},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{openshaw1983,
  title = {Doomsday: {{Britain}} after Nuclear Attack},
  author = {Openshaw, Stan and Steadman, Philip and Greene, Owen},
  year = {1983},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{openshaw1984,
  title = {The Modifiable Areal Unit Problem},
  author = {Openshaw, Stan},
  year = {1984},
  journal = {Concepts and techniques in modern geography},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/openshaw12.pdf}
}

@article{openshaw1984a,
  title = {Ecological {{Fallacies}} and the {{Analysis}} of {{Areal Census Data}}},
  author = {Openshaw, S},
  year = {1984},
  month = jan,
  journal = {Environment and Planning A: Economy and Space},
  volume = {16},
  number = {1},
  pages = {17--31},
  issn = {0308-518X, 1472-3409},
  doi = {10/bdn84m},
  abstract = {In many countries census data are only reported for areal units and not at the individual level. This custom raises the spectre of ecological fallacy problems. In this paper, a 10\% sample census (from the United Kingdom) and individual census data (from Italy) are used to provide an empirical demonstration of the nature and magnitude of these problems. It is concluded that ecological fallacy effects are endemic to areal census data, although their magnitude is perhaps not as large as might have been expected. The principal difficulty is that there is at present no way of predicting in advance the degree of severity likely to be associated with particular variables and particular techniques. Finally, a suggestion is made concerning how the potentially serious practical consequences can be reduced.},
  langid = {english},
  annotation = {ZSCC: 0000486},
  file = {/home/cjber/drive/pdf/ENVS416/Openshaw_1984_.pdf}
}

@article{openshaw1995,
  title = {Algorithms for {{Reengineering}} 1991 {{Census Geography}}},
  author = {Openshaw, S and Rao, L},
  year = {1995},
  month = mar,
  journal = {Environment and Planning A: Economy and Space},
  volume = {27},
  number = {3},
  pages = {425--446},
  issn = {0308-518X, 1472-3409},
  doi = {10/fmr58r},
  abstract = {The availability of GIS technology and digital boundaries of census output areas now makes it possible for users to design their own census geography. Three algorithms are described that can be used for this purpose. An Arc/Info implementation is briefly outlined and case studies presented to demonstrate some of the results of explicitly designing zoning systems for use with 1991 Census data.},
  langid = {english},
  annotation = {ZSCC: 0000330},
  file = {/home/cjber/drive/pdf/ENVS416/Openshaw_Rao_1995_.pdf}
}

@book{openshaw1997,
  title = {Artificial Intelligence in Geography},
  author = {Openshaw, Stan},
  year = {1997},
  isbn = {0-471-96991-5},
  keywords = {\#nosource}
}

@misc{ordnancesurvey2019,
  title = {Public {{Sector Mapping Agreement}} ({{PSMA}})},
  author = {{Ordnance Survey}},
  year = {2019},
  howpublished = {https://www.ordnancesurvey.co.uk/business-and-government/public-sector/mapping-agreements/public-sector-mapping-agreement.html},
  annotation = {ZSCC: 0000000[s0]},
  file = {/home/cjber/drive/zotero/storage/B5CRCWGM/public-sector-mapping-agreement.html}
}

@article{ordnancesurvey2019a,
  title = {{{OS Open Roads}}},
  author = {{Ordnance Survey}},
  year = {2019},
  pages = {28},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS492/Ordnance Survey_2019_.pdf}
}

@misc{ordnancesurvey2020,
  title = {Boundary-{{Line}}\texttrademark{} [{{SHAPE}} Geospatial Data], {{Scale}} 1:10000, {{Tiles}}: {{GB}}},
  author = {{Ordnance Survey}},
  year = {2020},
  month = sep
}

@book{oregan2016,
  title = {Guide to {{Discrete Mathematics}}},
  author = {O'Regan, Gerard},
  year = {2016},
  series = {Texts in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-44561-8},
  isbn = {978-3-319-44560-1 978-3-319-44561-8},
  langid = {english},
  file = {/home/cjber/drive/pdf/O'Regan_2016_Guide to Discrete Mathematics.pdf}
}

@article{overell2008,
  title = {Using Co-Occurrence Models for Placename Disambiguation},
  author = {Overell, Simon and R{\"u}ger, Stefan},
  year = {2008},
  month = mar,
  journal = {International Journal of Geographical Information Science},
  volume = {22},
  number = {3},
  pages = {265--287},
  issn = {1365-8816, 1362-3087},
  doi = {10/bh9ktw},
  langid = {english},
  keywords = {Ontology},
  file = {/home/cjber/drive/zotero/storage/ZGPVUHYV/Overell and Rüger - 2008 - Using co‐occurrence models for placename disambigu.asp}
}

@article{overell2009,
  title = {Geographic {{Information Retrieval}}: {{Classification}}, {{Disambiguation}} and {{Modelling}}},
  author = {Overell, Simon E},
  year = {2009},
  pages = {181},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Overell_2009_Geographic Information Retrieval.pdf}
}

@article{palacio2015,
  title = {Development and Evaluation of a Geographic Information Retrieval System Using Fine Grained Toponyms},
  author = {Palacio, Damien and Derungs, Curdin and Purves, Ross},
  year = {2015},
  month = dec,
  journal = {Journal of Spatial Information Science},
  number = {11},
  pages = {1--29},
  issn = {1948-660X},
  doi = {10/ggwjsw},
  abstract = {Geographic information retrieval (GIR) is concerned with returning information in response to an information need, typically expressed in terms of a thematic and spatial component linked by a spatial relationship. However, evaluation initiatives have often failed to show significant differences between simple text baselines and more complex spatially enabled GIR approaches. We explore the effectiveness of three systems (a text baseline, spatial query expansion, and a full GIR system utilizing both text and spatial indexes) at retrieving documents from a corpus describing mountaineering expeditions, centred around fine grained toponyms. To allow evaluation, we use user generated content (UGC) in the form of metadata associated with individual articles to build a test collection of queries and judgments. The test collection allowed us to demonstrate that a GIRbased method significantly outperformed a text baseline for all but very specific queries associated with very small query radii. We argue that such approaches to test collection development have much to offer in the evaluation of GIR.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Palacio et al_2015_Development and evaluation of a geographic information retrieval system using.pdf}
}

@article{palmer2004,
  title = {Impacts of Management Practices and Advanced Technologies on Demand Responsive Transit Systems},
  author = {Palmer, Kurt and Dessouky, Maged and Abdelmaguid, Tamer},
  year = {2004},
  month = aug,
  journal = {Transportation Research Part A: Policy and Practice},
  volume = {38},
  number = {7},
  pages = {495--509},
  issn = {09658564},
  doi = {10/bkqhnk},
  abstract = {Over the past 10 years, operating expenses for Demand Responsive Transit (DRT) have more than doubled as demand for this mandated service has expanded. The DRT systems that we studied consist of dial\textendash a\textendash ride programs that transit agencies use for point\textendash to\textendash point pickup and delivery of the elderly and handicapped. Many advanced technologies and management practices have been proposed and implemented to improve the efficiency of the service; but, evidence for the effectiveness of these actions has been based upon projections or small pilot studies. We present the results of a nationwide study involving 62 transit agencies. Our analysis indicates that the use of Paratransit Computer Aided Dispatching (CAD) system and Agency Service delivery provide a productivity benefit while the use of financial incentives has a detrimental impact on productivity. Also, the use of Advanced Communication technology has a beneficial impact on operating cost while the use of financial incentives has a detrimental impact.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS492/Palmer et al_2004_.pdf}
}

@article{palmer2010,
  title = {Semantic {{Role Labeling}}},
  author = {Palmer, Martha and Gildea, Daniel and Xue, Nianwen},
  year = {2010},
  month = jan,
  journal = {Synthesis Lectures on Human Language Technologies},
  volume = {3},
  number = {1},
  pages = {1--103},
  issn = {1947-4040, 1947-4059},
  doi = {10/cgprcs},
  langid = {english},
  file = {/home/cjber/drive/pdf/Palmer et al_2010_Semantic Role Labeling.pdf}
}

@inproceedings{paraskevopoulos2017,
  title = {{{TweeLoc}}: {{A System}} for {{Geolocalizing Tweets}} at {{Fine-Grain}}},
  shorttitle = {{{TweeLoc}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {Paraskevopoulos, Pavlos and Pellegrini, Giovanni and Palpanas, Themis},
  year = {2017},
  month = nov,
  pages = {1178--1183},
  publisher = {{IEEE}},
  address = {{New Orleans, LA}},
  doi = {10/ggwjt6},
  abstract = {The recent rise in the use of social networks has resulted in an abundance of information on different aspects of everyday social activities that is available online. In the process of analysis of identifying the information originating from social networks, and especially Twitter, an important aspect is that of the geographic coordinates, i.e., geolocalisation, of the relevant information. Geolocalized information can be used by a variety of applications in order to offer better, or new services. However, only a small percentage of the twitter posts are geotagged, which restricts the applicability of location-based applications. In this work, we describe TweeLoc, our prototype system for geolocalizing tweets that are not geotagged, which can effectively estimate the tweet location at the level of a city neighborhood. TweeLoc employs a dashboard that visualizes the social activity of the geographic regions specified by the user, and provides relevant easy-to-access statistics. Moreover, it displays information on the way that these statistics evolve over time. Our system can help end-users and large-scale event organizers to better plan and manage their activities, and can complete this task fast and more accurately than alternative solutions that we compare to.},
  isbn = {978-1-5386-3800-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Paraskevopoulos et al_2017_TweeLoc.pdf}
}

@book{parkinson1998,
  title = {Combating Social Exclusion: Lessons from Area-Based Programmes in {{Europe}}},
  author = {Parkinson, Michael},
  year = {1998},
  publisher = {{Policy Press Bristol}},
  isbn = {1-86134-120-2},
  keywords = {\#nosource}
}

@inproceedings{pasley2007,
  title = {Geo-Tagging for Imprecise Regions of Different Sizes},
  booktitle = {Proceedings of the 4th {{ACM}} Workshop on {{Geographical}} Information Retrieval  - {{GIR}} '07},
  author = {Pasley, Robert C. and Clough, Paul D. and Sanderson, Mark},
  year = {2007},
  pages = {77},
  publisher = {{ACM Press}},
  address = {{Lisbon, Portugal}},
  doi = {10/fpbb3j},
  abstract = {Extracting geographical information from various web sources is likely to be important for a variety of applications. One such use for this information is to enable the study of vernacular regions: informal places referred to on a day-to-day basis, but with no official entry in geographical resources, such as gazetteers. Past work in automatically extracting geographical information from the web to support the creation of vernacular regions has tended to focus on larger regions (e.g. The British Midlands and The South of France). In this paper we report the results of preliminary work to investigate the success of using a simple geotagging approach and resources of varying granularity from the Ordnance Survey to extract geographical information from web pages. We find that the data gathered for smaller regions (compared with larger ones) is more fine-grained which has an effect on the type of resource most useful for geo-tagging and its success.},
  isbn = {978-1-59593-828-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Pasley et al_2007_Geo-tagging for imprecise regions of different sizes.pdf}
}

@incollection{paszke2019,
  title = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}}
}

@article{patacchini2007,
  title = {Spatial Dependence in Local Unemployment Rates},
  author = {Patacchini, E. and Zenou, Y.},
  year = {2007},
  month = mar,
  journal = {Journal of Economic Geography},
  volume = {7},
  number = {2},
  pages = {169--191},
  issn = {1468-2702, 1468-2710},
  doi = {10/bvqbb9},
  abstract = {By explicitly considering the spatial dimension of local regional labor markets, we develop a simple dynamic model that explains the spatial correlation between unemployment rates. We then test this model by using UK local data. Our evidence shows a significant spatial dependence that has been growing over time and characterized by a low distance decay. Highly localized effects are explained by commuting flows. These results are consistent with the theoretical model.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/patacchini2007.pdf}
}

@article{paton2018,
  title = {Cut 60mph Limit on Country Roads to Help Save Lives, Ministers Urged},
  author = {Paton, Graeme},
  year = {2018},
  month = jun,
  journal = {The Times},
  issn = {0140-0460},
  abstract = {Ministers have been told to cut the 60mph speed limit on thousands of miles of rural roads amid concerns over the number of crashes. A report commissioned by the Department for Transport (DfT)...},
  chapter = {News},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/zotero/storage/3NY6RN3X/cut-60mph-limit-on-country-roads-to-help-save-lives-ministers-urged-gfz7s5vxf.html}
}

@article{payne2012,
  title = {{{UK}} Indices of Multiple Deprivation \textendash{} a Way to Make Comparisons across Constituent Countries Easier},
  author = {Payne, Rupert A and Abel, Gary A},
  year = {2012},
  journal = {Health Statistics Quarterly},
  pages = {17},
  abstract = {Background Deprivation is multi-dimensional, and as such can be challenging to quantify. In the UK, each of the four constituent countries measures deprivation using their own distinct index of multiple deprivation (IMD), designed to facilitate targeting of policies within that particular country. Although these four IMD scores are not directly comparable, there are circumstances where comparison across the whole of the UK may be desirable.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/payne2012.pdf}
}

@article{pebesma2018,
  title = {Simple Features for r: {{Standardized}} Support for Spatial Vector Data},
  author = {Pebesma, Edzer},
  year = {2018},
  journal = {The R Journal},
  volume = {10},
  number = {1},
  pages = {439--446},
  doi = {10/gf2ztt}
}

@manual{pedersen2020,
  type = {Manual},
  title = {Patchwork: {{The}} Composer of Plots},
  author = {Pedersen, Thomas Lin},
  year = {2020}
}

@article{pekar2020,
  title = {Early Detection of Heterogeneous Disaster Events Using Social Media},
  author = {Pekar, Viktor and Binner, Jane and Najafi, Hossein and Hale, Chris and Schmidt, Vincent},
  year = {2020},
  month = jan,
  journal = {Journal of the Association for Information Science and Technology},
  volume = {71},
  number = {1},
  pages = {43--54},
  issn = {2330-1635, 2330-1643},
  doi = {10/ggwkh5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Pekar et al_2020_Early detection of heterogeneous disaster events using social media.pdf}
}

@inproceedings{pennington2014,
  ids = {pennington2014a},
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10/gfshwg},
  file = {/home/cjber/drive/pdf/Pennington et al_2014_GloVe.pdf;/home/cjber/drive/pdf/Pennington et al_2014_GloVe2.pdf}
}

@article{perng2013,
  title = {Peripheral {{Response}}: {{Microblogging During}} the 22/7/2011 {{Norway Attacks}}},
  shorttitle = {Peripheral {{Response}}},
  author = {Perng, Sung-Yueh and B{\"u}scher, Monika and Wood, Lisa and Halvorsrud, Ragnhild and Stiso, Michael and Ramirez, Leonardo and {Al-Akkad}, Amro},
  year = {2013},
  month = jan,
  journal = {International Journal of Information Systems for Crisis Response and Management},
  volume = {5},
  number = {1},
  pages = {41--57},
  issn = {1937-9390, 1937-9420},
  doi = {10/gmgncq},
  abstract = {This paper presents a case study of a very recent man-made crisis in Norway on 22 July, 2011, during which a single person first detonated a bomb in downtown Oslo and then killed 69 young people on the island of Ut\o ya. It proposes a novel way of conceptualizing the public contribution to mobilization of resources using microblogging, particularly tweeting. By examining aspects of public and professional response to this crisis, the notion of peripheral response is developed in relation to emergent forms of agile and dialogic emergency response. Through examining the distributed efforts of responding to the crisis, the paper also revisits situation awareness and reflects upon the dynamic and constantly changing environment that social media and crises inhabit together.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Perng et al_2013_Peripheral Response.pdf}
}

@article{perry2012,
  title = {{{OGC GeoSPARQL}} - {{A Geographic Query Language}} for {{RDF Data}}},
  author = {Perry, Matthew and Herring, John},
  year = {2012},
  pages = {75},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Perry_Herring_2012_OGC GeoSPARQL - A Geographic Query Language for RDF Data.pdf}
}

@article{peters2017,
  title = {Semi-Supervised Sequence Tagging with Bidirectional Language Models},
  author = {Peters, Matthew E. and Ammar, Waleed and Bhagavatula, Chandra and Power, Russell},
  year = {2017},
  month = apr,
  journal = {arXiv:1705.00108 [cs]},
  eprint = {1705.00108},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Peters et al_2017_Semi-supervised sequence tagging with bidirectional language models.pdf;/home/cjber/drive/zotero/storage/QCCGM5U7/1705.html}
}

@article{peters2018,
  ids = {peters2018a},
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  journal = {arXiv:1802.05365 [cs]},
  eprint = {1802.05365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Peters et al_2018_Deep contextualized word representations.pdf;/home/cjber/drive/pdf/Peters et al_2018_Deep contextualized word representations.pdf】;/home/cjber/drive/pdf/Peters et al_2018_Deep contextualized word representations2.pdf;/home/cjber/drive/zotero/storage/FH7LJAV5/1802.html}
}

@article{peterson2005,
  title = {On the {{Use}} of {{Beta Coefficients}} in {{Meta-Analysis}}.},
  author = {Peterson, Robert A. and Brown, Steven P.},
  year = {2005},
  journal = {Journal of Applied Psychology},
  volume = {90},
  number = {1},
  pages = {175--181},
  issn = {1939-1854, 0021-9010},
  doi = {10/fksgxd},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS492/Peterson_Brown_2005_.pdf}
}

@misc{petrillo2010,
  title = {Introduction to {{Manual Annotation}}},
  author = {Petrillo, Matthew and Baycroft, Jessica},
  year = {2010},
  file = {/home/cjber/drive/pdf/Petrillo_Baycroft_2010_Introduction to Manual Annotation.pdf}
}

@manual{petukhov2020,
  type = {Manual},
  title = {Ggrastr: {{Raster Layers}} for 'Ggplot2'},
  author = {Petukhov, Viktor and {van den Brand}, Teun and Biederstedt, Evan},
  year = {2020}
}

@article{pickett2001,
  title = {Multilevel Analyses of Neighbourhood Socioeconomic Context and Health Outcomes: A Critical Review},
  author = {Pickett, K E and Pearl, M},
  year = {2001},
  journal = {Logistic regression},
  pages = {12},
  abstract = {Purpose\textemdash Interest in the eVects of neighbourhood or local area social characteristics on health has increased in recent years, but to date the existing evidence has not been systematically reviewed. Multilevel or contextual analyses of social factors and health represent a possible reconciliation between two divergent epidemiological paradigms\textemdash individual risk factor epidemiology and an ecological approach.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/Pickett_Pearl_2001_.pdf}
}

@incollection{piskorski2013,
  title = {Information Extraction: {{Past}}, Present and Future},
  booktitle = {Multi-Source, Multilingual Information Extraction and Summarization},
  author = {Piskorski, Jakub and Yangarber, Roman},
  year = {2013},
  pages = {23--49},
  publisher = {{Springer}}
}

@article{pizam1999,
  title = {A Comprehensive Approach to Classifying Acts of Crime and Violence at Tourism Destinations},
  author = {Pizam, Abraham},
  year = {1999},
  journal = {Journal of travel research},
  volume = {38},
  number = {1},
  pages = {5--12},
  issn = {0047-2875},
  doi = {10/c36m5t},
  keywords = {\#nosource}
}

@article{plewis2003,
  title = {What Is {{Multi-Level Modelling}} for? {{A Critical Response}} to {{Gorard}} (2003)},
  shorttitle = {What Is {{Multi-Level Modelling}} For?},
  author = {Plewis, Ian and Fielding, Antony},
  year = {2003},
  month = dec,
  journal = {British Journal of Educational Studies},
  volume = {51},
  number = {4},
  pages = {408--419},
  issn = {0007-1005, 1467-8527},
  doi = {10/bfzwbt},
  langid = {english},
  annotation = {ZSCC: 0000023},
  file = {/home/cjber/drive/pdf/ENVS416/plewis2003.pdf}
}

@inproceedings{poibeau2007,
  title = {{{UP13}}: Knowledge-Poor Methods (Sometimes) Perform Poorly},
  shorttitle = {{{UP13}}},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Semantic Evaluations}} - {{SemEval}} '07},
  author = {Poibeau, Thierry},
  year = {2007},
  pages = {418--421},
  publisher = {{Association for Computational Linguistics}},
  address = {{Prague, Czech Republic}},
  doi = {10/ffmwvg},
  abstract = {This short paper presents the system developed at the Universit\'e Paris 13 for the Metonymy resolution task, during Semeval 2007 (location name track). We developed a basic strategy only based on plain word forms to see how far one can go using only surface cues. We then discuss the relevance of this approach and compare it with more complex ones.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Poibeau_2007_UP13.pdf}
}

@article{pota2020,
  title = {An {{Effective BERT-Based Pipeline}} for {{Twitter Sentiment Analysis}}: {{A Case Study}} in {{Italian}}},
  shorttitle = {An {{Effective BERT-Based Pipeline}} for {{Twitter Sentiment Analysis}}},
  author = {Pota, Marco and Ventura, Mirko and Catelli, Rosario and Esposito, Massimo},
  year = {2020},
  month = dec,
  journal = {Sensors},
  volume = {21},
  number = {1},
  pages = {133},
  issn = {1424-8220},
  doi = {10/gmhdqv},
  abstract = {Over the last decade industrial and academic communities have increased their focus on sentiment analysis techniques, especially applied to tweets. State-of-the-art results have been recently achieved using language models trained from scratch on corpora made up exclusively of tweets, in order to better handle the Twitter jargon. This work aims to introduce a different approach for Twitter sentiment analysis based on two steps. Firstly, the tweet jargon, including emojis and emoticons, is transformed into plain text, exploiting procedures that are language-independent or easily applicable to different languages. Secondly, the resulting tweets are classified using the language model BERT, but pre-trained on plain text, instead of tweets, for two reasons: (1) pre-trained models on plain text are easily available in many languages, avoiding resource- and time-consuming model training directly on tweets from scratch; (2) available plain text corpora are larger than tweetonly ones, therefore allowing better performance. A case study describing the application of the approach to Italian is presented, with a comparison with other Italian existing solutions. The results obtained show the effectiveness of the approach and indicate that, thanks to its general basis from a methodological perspective, it can also be promising for other languages.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Pota et al_2020_An Effective BERT-Based Pipeline for Twitter Sentiment Analysis.pdf}
}

@article{power2005,
  title = {New {{Labour}} and Educational Disadvantage: The Limits of Area-based Initiatives},
  shorttitle = {New {{Labour}} and Educational Disadvantage},
  author = {Power, Sally and Rees, Gareth and Taylor, Chris},
  year = {2005},
  month = jan,
  journal = {London Review of Education},
  volume = {3},
  number = {2},
  pages = {101--116},
  issn = {1474-8460, 1474-8479},
  doi = {10/cqsj53},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Power et al_2005_.pdf}
}

@article{pretty2007,
  title = {Green Exercise in the {{UK}} Countryside: {{Effects}} on Health and Psychological Well-Being, and Implications for Policy and Planning},
  shorttitle = {Green Exercise in the {{UK}} Countryside},
  author = {Pretty, J. and Peacock, J. and Hine, R. and Sellens, M. and South, N. and Griffin, M.},
  year = {2007},
  month = mar,
  journal = {Journal of Environmental Planning and Management},
  volume = {50},
  number = {2},
  pages = {211--231},
  issn = {0964-0568, 1360-0559},
  doi = {10/dpxn39},
  abstract = {There is evidence that contact with the natural environment and green space promotes good health. It is also well known that participation in regular physical activity generates physical and psychological health benefits. The authors have hypothesised that `green exercise' will improve health and psychological well-being, yet few studies have quantified these effects. This study measured the effects of 10 green exercise case studies (including walking, cycling, horse-riding, fishing, canal-boating and conservation activities) in four regions of the UK on 263 participants. Even though these participants were generally an active and healthy group, it was found that green exercise led to a significant improvement in self-esteem and total mood disturbance (with anger-hostility, confusion-bewilderment, depression-dejection and tension-anxiety all improving post-activity). Self-esteem and mood were found not to be affected by the type, intensity or duration of the green exercise, as the results were similar for all 10 case studies. Thus all these activities generated mental health benefits, indicating the potential for a wider health and well-being dividend from green exercise. Green exercise thus has important implications for public and environmental health, and for a wide range of policy sectors.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Pretty et al_2007_.pdf}
}

@article{propper2005,
  title = {Local Neighbourhood and Mental Health: {{Evidence}} from the {{UK}}},
  shorttitle = {Local Neighbourhood and Mental Health},
  author = {Propper, Carol and Jones, Kelvyn and Bolster, Anne and Burgess, Simon and Johnston, Ron and Sarker, Rebecca},
  year = {2005},
  month = nov,
  journal = {Social Science \& Medicine},
  volume = {61},
  number = {10},
  pages = {2065--2083},
  issn = {02779536},
  doi = {10/fg9v2q},
  abstract = {This paper examines the association between neighbourhood and levels and changes in common mental disorders. Using data from a large scale nationally representative survey of individuals and households (the British Household Panel Survey), it locates individuals in their local neighbourhoods. These are defined as the nearest 500\textendash 800 persons centered around each individual in the survey. These `bespoke' neighbourhoods are characterised according to five dimensions\textemdash disadvantage, mobility, age, ethnicity and urbanness\textemdash derived from factor analysis of the census characteristics of the residents of neighbourhoods in 1991. These dimensions measure characteristics of place that have been argued to be associated with mental ill health.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Propper et al_2005_.pdf}
}

@article{purves2011,
  title = {Describing Place through User Generated Content},
  author = {Purves, Ross and Edwardes, Alistair and Wood, Jo},
  year = {2011},
  journal = {First Monday},
  volume = {16},
  number = {9},
  issn = {1396-0466},
  doi = {1368644681},
  keywords = {⚠️ Invalid DOI},
  file = {/home/cjber/drive/pdf/Purves et al_2011_Describing place through user generated content.pdf}
}

@book{purves2018,
  title = {Geographic {{Information Retrieval}}: {{Progress}} and {{Challenges}} in {{Spatial Search}} of {{Text}}},
  author = {Purves, Ross S. and Clough, Paul and Jones, Christopher B. and Hall, Mark H. and Murdock, Vanessa},
  year = {2018},
  publisher = {{now}},
  doi = {10.1561/1500000034},
  abstract = {Significant amounts of information available today contain references to places on earth. Traditionally such information has been held as structured data and was the concern of Geographic Information Systems (GIS). However, increasing amounts of data in the form of unstructured text are available for indexing and retrieval that also contain spatial references. This monograph describes the field of Geographic Information Retrieval (GIR) that seeks to develop spatially-aware search systems and support user's geographical information needs. Important concepts with respect to storing, querying and analysing geographical information in computers are introduced, before user needs and interaction in the context of GIR are explored. The task of associating documents with coordinates, prior to their indexing and ranking forms the core of any GIR system, and different approaches and their implications are discussed. Evaluating the resulting systems and their components, and different paradigms for doing so continue to be an important area of research in GIR and are illustrated through several examples. The monograph provides an overview of the research field, and in so doing identifies key remaining research challenges in GIR.},
  isbn = {978-1-68083-413-0},
  file = {/home/cjber/drive/pdf/Purves et al_2018_Geographic Information Retrieval.pdf}
}

@misc{purvesinpress,
  title = {Geographic {{Information Retrieval}}: {{Progress}} and Challenges in Spatial Search of Text},
  author = {Purves, Ross and Clough, Paul and Jones, Christopher and Hall, Mark and Murdock, Vanessa},
  year = {in press},
  file = {/home/cjber/drive/pdf/Purves et al_in press_Geographic Information Retrieval.pdf}
}

@article{pustejovsky,
  title = {The {{Semantics}} of {{ISO-Space}}},
  author = {Pustejovsky, James and Lee, Kiyong and Bunt, Harry},
  pages = {8},
  abstract = {An understanding of spatial information in natural language is necessary for many computational linguistics and artificial intelligence applications. In this paper, we outline the basic semantic structure for ISO-Space, an annotation scheme for the markup of spatial relations, both static and dynamic, as expressed in text and other media. We outline the basic formal semantic requirements of a model for spatial information, as expressed in the metamodel for ISO-Space, and demonstrate some illustrative compositions using type-theoretic derivations. We then show how the concrete syntax of the annotation structure for ISO-Space is consistent with the semantics provided for the metamodel.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Pustejovsky et al_The Semantics of ISO-Space.pdf}
}

@article{pustejovsky2011,
  title = {Image {{Annotation}} with {{ISO-Space}}: {{Distinguishing Content}} from {{Structure}}},
  author = {Pustejovsky, James and Yochum, Zachary},
  year = {2011},
  pages = {6},
  abstract = {Natural language descriptions of visual media present interesting problems for linguistic annotation of spatial information. This paper explores the use of ISO-Space, an annotation specification to capturing spatial information, for encoding spatial relations mentioned in descriptions of images. Especially, we focus on the distinction between references to representational content and structural components of images, and the utility of such a distinction within a compositional semantics. We also discuss how such a structure-content distinction within the linguistic annotation can be leveraged to compute further inferences about spatial configurations depicted by images with verbal captions.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Pustejovsky_Yochum_2011_Image Annotation with ISO-Space.pdf}
}

@inproceedings{pustejovsky2011a,
  title = {{{ISO-Space}}: {{The}} Annotation of Spatial Information in Language},
  booktitle = {Proceedings of the {{Sixth Joint ISO-ACL SIGSEM Workshop}} on {{Interoperable Semantic Annotation}}},
  author = {Pustejovsky, James and Moszkowicz, Jessica L and Verhagen, Marc},
  year = {2011},
  volume = {6},
  pages = {1--9}
}

@inproceedings{pustejovsky2015,
  title = {{{SemEval-2015 Task}} 8: {{SpaceEval}}},
  shorttitle = {{{SemEval-2015 Task}} 8},
  booktitle = {Proceedings of the 9th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval}} 2015)},
  author = {Pustejovsky, James and Kordjamshidi, Parisa and Moens, Marie-Francine and Levine, Aaron and Dworman, Seth and Yocum, Zachary},
  year = {2015},
  pages = {884--894},
  publisher = {{Association for Computational Linguistics}},
  address = {{Denver, Colorado}},
  doi = {10/gg66vn},
  abstract = {Human languages exhibit a variety of strategies for communicating spatial information, including toponyms, spatial nominals, locations that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Pustejovsky et al_2015_SemEval-2015 Task 8.pdf}
}

@incollection{pustejovsky2017,
  title = {{{ISO-Space}}: {{Annotating Static}} and {{Dynamic Spatial Information}}},
  shorttitle = {{{ISO-Space}}},
  booktitle = {Handbook of {{Linguistic Annotation}}},
  author = {Pustejovsky, James},
  editor = {Ide, Nancy and Pustejovsky, James},
  year = {2017},
  pages = {989--1024},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-024-0881-2_37},
  abstract = {An understanding of spatial information in natural language is necessary for many computational linguistics and artificial intelligence applications. In this chapter, we describe an annotation scheme for the markup of spatial relations, both static and dynamic, as expressed in text and other media. The desiderata for such a specification language are presented along with what representational mechanisms are required for such a specification to be successful. We review the annotation development process, and the adoption of the initial specification ISOspace, as an ISO standard, renamed ISOspace. We conclude with a discussion of the use of ISOspace in the context of the shared task SpaceEval 2015.},
  isbn = {978-94-024-0879-9 978-94-024-0881-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Pustejovsky_2017_ISO-Space.pdf;/home/cjber/drive/pdf/Pustejovsky_2017_ISO-Space2.pdf}
}

@inproceedings{qi2018,
  title = {Universal {{Dependency Parsing}} from {{Scratch}}},
  booktitle = {Proceedings of The},
  author = {Qi, Peng and Dozat, Timothy and Zhang, Yuhao and Manning, Christopher D.},
  year = {2018},
  pages = {160--170},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10/gf6gst},
  abstract = {This paper describes Stanford's system at the CoNLL 2018 UD Shared Task. We introduce a complete neural pipeline system that takes raw text as input, and performs all tasks required by the shared task, ranging from tokenization and sentence segmentation, to POS tagging and dependency parsing. Our single system submission achieved very competitive performance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2nd, 1st, and 3rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on lowresource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Qi et al_2018_Universal Dependency Parsing from Scratch.pdf}
}

@article{qiu2019,
  title = {Detecting Geo-Relation Phrases from Web Texts for Triplet Extraction of Geographic Knowledge: A Context-Enhanced Method},
  shorttitle = {Detecting Geo-Relation Phrases from Web Texts for Triplet Extraction of Geographic Knowledge},
  author = {Qiu, Peiyuan and Yu, Li and Gao, Jialiang and Lu, Feng},
  year = {2019},
  month = jul,
  journal = {Big Earth Data},
  volume = {3},
  number = {3},
  pages = {297--314},
  issn = {2096-4471, 2574-5417},
  doi = {10/ghgx5d},
  abstract = {As an effective organization form of geographic information, a geographic knowledge graph (GeoKG) facilitates numerous geography-related analyses and services. The completeness of triplets regarding geographic knowledge determines the quality of GeoKG, thus drawing considerable attention in the related domains. Mass unstructured geographic knowledge scattered in web texts has been regarded as a potential source for enriching the triplets in GeoKGs. The crux of triplet extraction from web texts lies in the detection of key phrases indicating the correct geo-relations between geo-entities. However, the current methods for key-phrase detection are ineffective because the sparseness of the terms in the web texts describing geo-relations results in an insufficient training corpus. In this study, an unsupervised context-enhanced method is proposed to detect geo-relation key phrases from web texts for extracting triplets. External semantic knowledge is introduced to relieve the influence of the sparseness of the georelation description terms in web texts. Specifically, the contexts of geo-entities are fused with category semantic knowledge and word semantic knowledge. Subsequently, an enhanced corpus is generated using frequency-based statistics. Finally, the geo-relation key phrases are detected from the enhanced contexts using the statistical lexical features from the enhanced corpus. Experiments are conducted with real web texts. In comparison with the well-known frequency-based methods, the proposed method improves the precision of detecting the key phrases of the geo-relation description by approximately 20\%. Moreover, compared with the well-defined geo-relation properties in DBpedia, the proposed method provides quintuple key-phrases for indicating the geo-relations between geo-entities, which facilitate the generation of new triplets from web texts.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Qiu et al_2019_Detecting geo-relation phrases from web texts for triplet extraction of.pdf;/home/cjber/drive/pdf/Qiu et al_2019_Detecting geo-relation phrases from web texts for triplet extraction of2.pdf}
}

@article{qu,
  title = {Privacy {{Preservation}} in {{Smart Cities}}},
  author = {Qu, Youyang and Yu, Shui},
  pages = {12},
  doi = {10/ggwjvd},
  langid = {english},
  file = {/home/cjber/drive/pdf/Qu_Yu_Privacy Preservation in Smart Cities.pdf}
}

@article{qu2018,
  title = {Distant Supervision for Neural Relation Extraction Integrated with Word Attention and Property Features},
  author = {Qu, Jianfeng and Ouyang, Dantong and Hua, Wen and Ye, Yuxin and Li, Ximing},
  year = {2018},
  month = apr,
  journal = {Neural Networks},
  volume = {100},
  pages = {59--69},
  issn = {08936080},
  doi = {10/gc9g3g},
  abstract = {Distant supervision for neural relation extraction is an efficient approach to extracting massive relations with reference to plain texts. However, the existing neural methods fail to capture the critical words in sentence encoding and meanwhile lack useful sentence information for some positive training instances. To address the above issues, we propose a novel neural relation extraction model. First, we develop a word-level attention mechanism to distinguish the importance of each individual word in a sentence, increasing the attention weights for those critical words. Second, we investigate the semantic information from word embeddings of target entities, which can be developed as a supplementary feature for the extractor. Experimental results show that our model outperforms previous state-of-the-art baselines.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Qu et al_2018_Distant supervision for neural relation extraction integrated with word.pdf}
}

@inproceedings{quesnot2015,
  ids = {quesnot2015a},
  title = {Platial or {{Locational Data}}? {{Toward}} the {{Characterization}} of {{Social Location Sharing}}},
  shorttitle = {Platial or {{Locational Data}}?},
  booktitle = {2015 48th {{Hawaii International Conference}} on {{System Sciences}}},
  author = {Quesnot, Teriitutea and Roche, Stephane},
  year = {2015},
  month = jan,
  pages = {1973--1982},
  publisher = {{IEEE}},
  address = {{HI, USA}},
  issn = {1530-1605},
  doi = {10/ggwjtq},
  abstract = {Sharing "location" information on social media became commonplace since the advent of smartphones. Location-based social networks introduced a derivative form of Volunteered Geographic Information (VGI) known as Social Location Sharing (SLS). It consists of claiming "I am/was at that Place". Since SLS represents a singular form of place-based (i.e. platial) communication, we argue that SLS data are more platial than locational. According to our data classification of VGI, locational data (e.g. a geotagged tweet which geographic dimension is limited to its coordinate information) are a reduced form of platial data (e.g. a Swarm check-in). Therefore, we believe these two kinds of data should not be analyzed on the same spatial level. This distinction needs to be clarified because a large part of geosocial data (i.e. spatial data published from social media) tends to be analyzed on the basis of a locational equivalence and not on a platial one.},
  isbn = {978-1-4799-7367-5},
  langid = {english},
  keywords = {Context,Data Classification,Facebook,geographic information systems,Geographic information systems,geosocial data,geotagged tweet,Location,location-based social network,locational data,Media,mobile computing,Place,place-based communication,Platial,platial data,Production,smartphones,social location sharing,Social Location Sharing,social media,social networking (online),Space,Spatial databases,VGI,volunteered geographic information,Volunteered Geographic Information},
  file = {/home/cjber/drive/pdf/Quesnot_Roche_2015_Platial or Locational Data.pdf;/home/cjber/drive/pdf/Quesnot_Roche_2015_Platial or Locational Data2.pdf;/home/cjber/drive/zotero/storage/SZ8PHQHP/7070048.html}
}

@article{radford2018,
  title = {Improving Language Understanding with Unsupervised Learning},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Time and Sutskever, Ilya},
  year = {2018},
  journal = {Technical report, OpenAI},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{radke2019,
  title = {Detecting the {{Geospatialness}} of {{Prepositions}} from {{Natural Language Text}} ({{Short Paper}})},
  author = {Radke, Mansi and Das, Prarthana and Stock, Kristin and Jones, Christopher B.},
  year = {2019},
  pages = {8 pages},
  publisher = {{Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany}},
  doi = {10.4230/LIPICS.COSIT.2019.11},
  abstract = {There is increasing interest in detecting the presence of geospatial locative expressions that include spatial relation terms such as near or within {$<$}some distance{$>$}. Being able to do so provides a foundation for interpreting relative descriptions of location and for building corpora that facilitate the development of methods for spatial relation extraction and interpretation. Here we evaluate the use of a spatial role labelling procedure to distinguish geospatial uses of prepositions from other spatial and non-spatial uses and experiment with the use of additional machine learning features to improve the quality of detection of geospatial prepositions. An annotated corpus of nearly 2000 instances of preposition usage was created for training and testing the classifiers.},
  collaborator = {Wagner, Michael},
  copyright = {Creative Commons Attribution 3.0 Unported license (CC-BY 3.0)},
  langid = {english},
  keywords = {000 Computer science; knowledge; general works,Computer Science},
  file = {/home/cjber/drive/pdf/Radke et al_2019_Detecting the Geospatialness of Prepositions from Natural Language Text (Short.pdf}
}

@inproceedings{rae2012,
  ids = {rae2012a},
  title = {Mining the Web for Points of Interest},
  booktitle = {Proceedings of the 35th International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval - {{SIGIR}} '12},
  author = {Rae, Adam and Murdock, Vanessa and Popescu, Adrian and Bouchard, Hugues},
  year = {2012},
  pages = {711},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon, USA}},
  doi = {10/ggwjtn},
  abstract = {A point of interest (POI) is a focused geographic entity such as a landmark, a school, an historical building, or a business. Points of interest are the basis for most of the data supporting location-based applications. In this paper we propose to curate POIs from online sources by bootstrapping training data from Web snippets, seeded by POIs gathered from social media. This large corpus is used to train a sequential tagger to recognize mentions of POIs in text. Using Wikipedia data as the training data, we can identify POIs in free text with an accuracy that is 116\% better than the state of the art POI identifier in terms of precision, and 50\% better in terms of recall. We show that using Foursquare and Gowalla checkins as seeds to bootstrap training data from Web snippets, we can improve precision between 16\% and 52\%, and recall between 48\% and 187\% over the state-of-theart. The name of a POI is not sufficient, as the POI must also be associated with a set of geographic coordinates. Our method increases the number of POIs that can be localized nearly three-fold, from 134 to 395 in a sample of 400, with a median localization accuracy of less than one kilometer.},
  isbn = {978-1-4503-1472-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Rae et al_2012_Mining the web for points of interest.pdf;/home/cjber/drive/pdf/Rae et al_2012_Mining the web for points of interest2.pdf;/home/cjber/drive/pdf/Rae et al_2012_Mining the web for points of interest3.pdf}
}

@inproceedings{rahgooy2018,
  title = {Visually {{Guided Spatial Relation Extraction}} from {{Text}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of           the {{Association}} for {{Computational Linguistics}}: {{Human Language}}           {{Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  author = {Rahgooy, Taher and Manzoor, Umar and Kordjamshidi, Parisa},
  year = {2018},
  pages = {788--794},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10/ggwjt8},
  abstract = {Extraction of spatial relations from sentences with complex/nesting relationships is very challenging as often needs resolving inherent semantic ambiguities. We seek help from visual modality to fill the information gap in the text modality and resolve spatial semantic ambiguities. We use various recent vision and language datasets and techniques to train inter-modality alignment models, visual relationship classifiers and propose a novel global inference model to integrate these components into our structured output prediction model for spatial role and relation extraction. Our global inference model enables us to utilize the visual and geometric relationships between objects and improves the state-of-art results of spatial information extraction from text.},
  langid = {english},
  keywords = {Key Paper,Spatial Indicators},
  file = {/home/cjber/drive/pdf/Rahgooy et al_2018_Visually Guided Spatial Relation Extraction from Text.pdf}
}

@incollection{ramrakhiyani2019,
  ids = {ramrakhiyani2019a},
  title = {A {{Simple Neural Approach}} to {{Spatial Role Labelling}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Ramrakhiyani, Nitin and Palshikar, Girish and Varma, Vasudeva},
  editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
  year = {2019},
  volume = {11438},
  pages = {102--108},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-15719-7_13},
  isbn = {978-3-030-15718-0 978-3-030-15719-7},
  langid = {english},
  keywords = {\#nosource},
  file = {/home/cjber/drive/pdf/Ramrakhiyani et al_2019_A Simple Neural Approach to Spatial Role Labelling.pdf}
}

@article{randell1992,
  title = {A {{Spatial Logic}} Based on {{Regions}} and {{Connection}}},
  author = {Randell, David A and Cui, Zhan and Cohn, Anthony G},
  year = {1992},
  pages = {12},
  abstract = {We describe an interval logic for reasoning about space. The logic simpli es an earlier theory developed by Randell and Cohn, and that of Clarke upon which the former was based. The theory supports a simpler ontology, has fewer de ned functions and relations, yet does not su er in terms of its useful expressiveness. An axiomatisation of the new theory and a comparison with the two original theories is given.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Randell et al_1992_A Spatial Logic based on Regions and Connection.pdf}
}

@inproceedings{ratinov2009,
  title = {Design {{Challenges}} and {{Misconceptions}} in {{Named Entity Recognition}}},
  booktitle = {Proceedings of the {{Thirteenth Conference}} on {{Computational Natural Language Learning}} ({{CoNLL-2009}})},
  author = {Ratinov, Lev and Roth, Dan},
  year = {2009},
  month = jun,
  pages = {147--155},
  publisher = {{Association for Computational Linguistics}},
  address = {{Boulder, Colorado}},
  file = {/home/cjber/drive/pdf/Ratinov_Roth_2009_Design Challenges and Misconceptions in Named Entity Recognition.pdf;/home/cjber/drive/pdf/Ratinov_Roth_2009_Design Challenges and Misconceptions in Named Entity Recognition2.pdf}
}

@article{ratnaparkhi1996,
  title = {A {{Maximum Entropy Model}} for {{Part-Of-Speech Tagging}}},
  author = {Ratnaparkhi, Adwait},
  year = {1996},
  pages = {10},
  abstract = {This paper presents a statistical model which trains from a corpus annotated with Part-OfSpeech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6\%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual "features" to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Ratnaparkhi_1996_A Maximum Entropy Model for Part-Of-Speech Tagging.pdf;/home/cjber/drive/pdf/Ratnaparkhi_1996_A Maximum Entropy Model for Part-Of-Speech Tagging2.pdf}
}

@inproceedings{rauch2003,
  title = {A Confidence-Based Framework for Disambiguating Geographic Terms},
  booktitle = {Proceedings of the {{HLT-NAACL}} 2003 Workshop on {{Analysis}} of Geographic References  -},
  author = {Rauch, Erik and Bukatin, Michael and Baker, Kenneth},
  year = {2003},
  volume = {1},
  pages = {50--54},
  publisher = {{Association for Computational Linguistics}},
  address = {{Not Known}},
  doi = {10.3115/1119394.1119402},
  abstract = {We describe a purely confidence-based geographic term disambiguation system that crucially relies on the notion of ``positive'' and ``negative'' context and methods for combining confidence-based disambiguation with measures of relevance to a user's query.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Rauch et al_2003_A confidence-based framework for disambiguating geographic terms.pdf}
}

@manual{rcoreteam2020,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@inproceedings{recchia2013,
  title = {A {{Comparison}} of {{String Similarity Measures}} for {{Toponym Matching}}},
  booktitle = {Proceedings of {{The First ACM SIGSPATIAL International Workshop}} on {{Computational Models}} of {{Place}} - {{COMP}} '13},
  author = {Recchia, Gabriel and Louwerse, Max},
  year = {2013},
  pages = {54--61},
  publisher = {{ACM Press}},
  address = {{Orlando FL, USA}},
  doi = {10.1145/2534848.2534850},
  abstract = {The diversity of ways in which toponyms are specified often results in mismatches between queries and the place names contained in gazetteers. Search terms that include unofficial variants of official place names, unanticipated transliterations, and typos are frequently similar but not identical to the place names contained in the gazetteer. String similarity measures can mitigate this problem, but given their task-dependent performance, the optimal choice of measure is unclear. We constructed a task in which place names had to be matched to variants of those names listed in the GEOnet Names Server, comparing 21 different measures on datasets containing romanized toponyms from 11 different countries. Best-performing measures varied widely across datasets, but were highly consistent within-country and within-language. We discuss which measures worked best for particular languages and provide recommendations for selecting appropriate string similarity measures.},
  isbn = {978-1-4503-2535-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Recchia_Louwerse_2013_A Comparison of String Similarity Measures for Toponym Matching.pdf}
}

@article{redfern2003,
  title = {What {{Makes Gentrification}} '{{Gentrification}}'?},
  author = {Redfern, P.A.},
  year = {2003},
  month = nov,
  journal = {Urban Studies},
  volume = {40},
  number = {12},
  pages = {2351--2366},
  issn = {0042-0980, 1360-063X},
  doi = {10/ddfwxg},
  abstract = {This paper looks at demand issues in gentrification. It takes as its basic proposition that everyone involved in the demand side belongs to the same economic class and therefore possesses the same set of motivations; that `otherness' in gentrification is something that needs to be problematised rather than assumed. It argues that the presumption of otherness arises because accounts of demand for gentrification begin at the end, from achieved housing situation, and argue back, rather than at the beginning, with means. The motivations of gentrifiers, suburbanites and displacees are the same, a concern for defining and preserving identity in the modern world: what differ between them are the means at the disposal of each group. Concern with identity means taking seriously the importance of fashion in gentrification: gentrifiers and suburbanites are members of different status groups, using housing as status symbols to define and claim membership of those groups. Displacees are just as concerned with the maintenance of their identity, but do not have access to the same amount of resources as gentrifiers. Because the solution to the gentrifiers' identity crisis takes place at the expense of the displacee, gentrification takes on a synecdochal quality: the concerns expressed in struggles over gentrification exemplify the general concern with identity in conditions of modernity, which should be understood as the subjective experience of everyday life within a capitalist mode of production. The context within which these struggles over status take place is nonetheless class-constituted and class-laden. Gentrification and the struggles it engenders should be interpreted as a form of hegemonic practice. Ultimately, it is this that makes gentrification `gentrification'.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/redfern2003.pdf}
}

@article{reilly2021,
  title = {Organizational {{Hashtags During Times}} of {{Crisis}}: {{Analyzing}} the {{Broadcasting}} and {{Gatekeeping Dynamics}} of \#{{PorteOuverte During}} the {{November}} 2015 {{Paris Terror Attacks}}},
  shorttitle = {Organizational {{Hashtags During Times}} of {{Crisis}}},
  author = {Reilly, Paul and Vicari, Stefania},
  year = {2021},
  month = jan,
  journal = {Social Media + Society},
  volume = {7},
  number = {1},
  pages = {205630512199578},
  issn = {2056-3051, 2056-3051},
  doi = {10/gmdkxv},
  abstract = {Twitter hashtags allow citizens to share vital information and make sense of acute crisis events such as terrorist attacks. They also enable those watching from afar to express their sympathy and solidarity with the victims. Perhaps the most well known of these has been \#PorteOuverte (translated into English as ``Open Door''), first used during the November 2015 terrorist attacks in Paris before re-emerging during subsequent atrocities in Brussels (March 2016) and Nice (July 2016). The hashtag was originally created by journalist Sylvain Lapoix in order to connect those in Paris looking for somewhere to stay with those able to offer them refuge, before reaching an international audience courtesy of its amplification by public figures and citizens based overseas. This article adds to this emergent literature by analyzing the networked gatekeeping dynamics of \#PorteOuverte during the Paris terror attacks. It does so by reviewing the literature on Twitter hashtags and acute crisis events, exploring how Twitter was used during the Paris terror attacks, and presenting the results of a Social Network Analysis (SNA) of 399,256 \#PorteOuverte tweets posted as the attacks unfolded on 13 November 2015. Results indicate that professional journalists were key broadcasters during four identified peaks within \#PorteOuverte, helping to promote the informational hashtag and connect those directly affected. However, citizens and bloggers played an increasingly important gatekeeping function in the aftermath of events such as the Bataclan siege in Paris.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Reilly_Vicari_2021_Organizational Hashtags During Times of Crisis.pdf}
}

@inproceedings{reyes-palacios,
  title = {Georeference {{Assignment}} of {{Locations}} Based on {{Context}}},
  booktitle = {Proceedings of the 1st {{International Conference}} on {{Geospatial Information Sciences}}},
  author = {{Reyes-Palacios}, Shanel and {Aldana-Bobadilla}, Edwin and {Lopez-Arevalo}, Ivan and {Molina-Villegas}, Alejandro},
  pages = {39--30},
  doi = {10/gg2597},
  abstract = {Modern Text Mining techniques seek for extract information in useful formats such as georeferences in digital documents. Automatic recognition of location names in texts is usually solved through Named Entity Recognition (NER) systems. Most current NER are based on Machine Learning and have very high accuracy in detection of location entities in digital documents, especially if the texts are in English due to the lack of available annotated corpora in other languages. However, recent studies are dealing with the challenge of taking the output labels of a NER system and then gather, from a gazetteer, their exact unambiguous geographical coordinates. This is challenging mainly because toponyms use to be very ambiguous, so research in disambiguation methods is relevant. In this paper we describe some of the main ideas towards a method to associate locations with geographical data removing possible confusion between entities with the same name. So far, we have already accomplished Geographic NER and coordinates retrieval but the main research is still in course. We largely discuss about the state of the art around Geoparsing; we explain how our Geographic Entity Recognition module works and finally we describe the research proposal focusing in ambiguity detection.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Reyes-Palacios et al_Georeference Assignment of Locations based on Context.pdf}
}

@article{rhodes2003,
  title = {New {{Developments}} in {{Area-based Initiatives}} in {{England}}: {{The Experience}} of the {{Single Regeneration Budget}}},
  shorttitle = {New {{Developments}} in {{Area-based Initiatives}} in {{England}}},
  author = {Rhodes, John and Tyler, Peter and Brennan, Angela},
  year = {2003},
  month = jul,
  journal = {Urban Studies},
  volume = {40},
  number = {8},
  pages = {1399--1426},
  issn = {0042-0980, 1360-063X},
  doi = {10/crxdsb},
  abstract = {This article reports recent research that has assessed the achievements of one of the most extensive area-based initiatives (ABIs) that has ever been adopted in England\textemdash namely, the Single Regeneration Budget. This research has sought to ascertain whether, at least in relation to the findings from SRB, there is evidence that the ABI has delivered what was expected of it and thus whether the underlying rationale for its deployment was valid. The article focuses on three key areas of relevance to the attainment of local area regeneration. These are: targeting social deprivation; bringing about effective partnership working; and, `bending' the activities of mainstream service providers. This article concludes by considering what the lessons from SRB are for the future shape and form of ABIs.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Rhodes et al_2003_.pdf}
}

@article{ribeiro,
  title = {Scalable and {{Memory-Efficient Approaches}} for {{Spatial Data Downscaling Leveraging Machine Learning}}},
  author = {Ribeiro, Carlos},
  pages = {20},
  abstract = {In the context of spatial analysis, spatial disaggregation or spatial downscaling are processes by which information at a coarse spatial scale is translated to finer scales, while maintaining consistency with the original dataset. Fine-grained descriptions of geographical information is a key resource in fields such as social-economic studies, urban and regional planning, transport planning, or environmental impact analysis. One such method for spacial disaggregation is the dissever algorithm, a hybrid approach that combines pycnophylactic interpolation and regression-based dasymetric mapping, leveraging auxiliary data to increase desegregation precision. Implemented in R, the current dissever version assumes a sequential execution model and the usage of smaller-than-ram computational sequences, limiting the resolution and scale of usable datasets. This paper presents a variant of the dissever algorithm, called S-Dissever (Scalable Dissever) suitable for efficiently executing in parallel environments, taking advantage of modern multi-core architectures, while using secondary memory to scale out computations that exceed RAM capacity. Experimental evaluations on the S-Dissever demonstrate the feasibility of the desegregation procedure while using high-resolution auxiliary data at significant geographical extents, as well as notable speedups obtained by the utilization of multi-core environments.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000000},
  file = {/home/cjber/drive/pdf/ENVS492/Ribeiro_.pdf}
}

@article{richards2009,
  title = {The {{Relationship}} between {{Speed}} and {{Car Driver Injury Severity}}},
  author = {Richards, D and Cuerden, R},
  year = {2009},
  pages = {16},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000045},
  file = {/home/cjber/drive/pdf/ENVS492/Richards_Cuerden_.pdf}
}

@article{richardson2013,
  title = {Role of Physical Activity in the Relationship between Urban Green Space and Health},
  author = {Richardson, E.A. and Pearce, J. and Mitchell, R. and Kingham, S.},
  year = {2013},
  month = apr,
  journal = {Public Health},
  volume = {127},
  number = {4},
  pages = {318--324},
  issn = {00333506},
  doi = {10/gfgn2j},
  abstract = {Objectives Local availability of green space has been associated with a wide range of health benefits. Possible causative mechanisms underpinning the green space and health relationship include the provision of physical activity opportunities, the stress-relieving effects of nature and the facilitation of social contacts. We sought to investigate whether urban green space was related to individual-level health outcomes, and whether physical activity levels were likely to be a mediating factor in any relationships found.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/richardson2013.pdf}
}

@article{richter2012,
  title = {How People Describe Their Place: Identifying Predominant Types of Place Descriptions},
  author = {Richter, Daniela and Winter, Stephan and Richter, Kai-Florian and Stirling, Lesley},
  year = {2012},
  pages = {8},
  doi = {10/ggwjtx},
  abstract = {People communicate about locations using place descriptions. Despite the growth of mobile location- and contextaware applications, the automatic interpretation of place descriptions remains a challenge. Currently no software tools exist that are capable of understanding complex verbal spatial language. This paper explores a corpus of place descriptions collected through crowdsourcing mechanisms within a mobile game. It introduces a general classification scheme to annotate place descriptions according to di{$\carriagereturn$}erent characteristic parameters and uses this scheme to demonstrate the existence of certain clusters of prevalent types of place descriptions in human communication. Research outcomes contribute to the common understanding of the way people refer to places, which is essential to support the development of intelligent tools and location based technologies.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Richter et al_2012_How people describe their place.pdf}
}

@article{richter2014,
  title = {Landmarks: {{GIScience}} for {{Intelligent Services}}},
  author = {Richter, Kai-Florian and Winter, Stephan},
  year = {2014},
  publisher = {{Springer Publishing Company, Incorporated}},
  issn = {3319057316},
  keywords = {\#nosource,⛔ No DOI found}
}

@article{ritter2011,
  title = {Named Entity Recognition in Tweets: An Experimental Study},
  author = {Ritter, Alan and Clark, Sam},
  year = {2011},
  pages = {11},
  abstract = {People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-NER system doubles F1 score compared with the Stanford NER system. T-NER leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms cotraining, increasing F1 by 25\% over ten common entity types.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Ritter_Clark_2011_Named entity recognition in tweets.pdf;/home/cjber/drive/pdf/Ritter_Clark_2011_Named entity recognition in tweets2.pdf}
}

@article{riva2011,
  title = {Residential Mobility within {{England}} and Urban\textendash Rural Inequalities in Mortality},
  author = {Riva, Myl{\`e}ne and Curtis, Sarah and Norman, Paul},
  year = {2011},
  month = dec,
  journal = {Social Science \& Medicine},
  volume = {73},
  number = {12},
  pages = {1698--1706},
  issn = {02779536},
  doi = {10/c5wkhn},
  abstract = {This study is situated within the international literature on geographic health inequalities between urban and rural areas. Using data from the Office for National Statistics Longitudinal Study (ONS LS), this paper assesses the role of residential mobility within England between 1981 and 2001 in explaining geographic inequalities in all-cause mortality between urban and rural Local Authority Districts at the end of the period (deaths occurring between 2001 and 2005). First, the pattern of directly age-standardised death rates (2001e2005) in urban and rural areas of residence in 2001 is examined and compared with the pattern that would have been seen if the observed death/survival of individuals had occurred in their original place of residence in 1981, or in 1991. Secondly, logistic regression is applied to examine whether individuals' residential mobility between urban and rural areas predict the risk of mortality, adjusting for people's socio-demographic characteristics. Findings show that, for this sample, residential mobility 1981e2001 accounts for about 30\% of the urbanerural inequalities in mortality observed at the end of the period. LS members who were residentially mobile between urban and rural areas were relatively healthier than long-term urban residents, with better mortality outcomes among rural in-migrants. In age-stratified analysis, LS members of working age (20e64 years) moving out of rural areas, and LS members of retirement age (65 years and older) moving into rural areas, were shown to be healthier. Processes of selective migration in and out of rural areas in England are complex and may partly explain urbanerural health inequalities. In terms of varying mortality risk, findings also highlight the possible marginalisation and disadvantage of sub-groups of the rural population.},
  langid = {english},
  annotation = {ZSCC: 0000050},
  file = {/home/cjber/drive/pdf/ENVS492/Riva et al_2011_.pdf}
}

@article{rizzo2015,
  title = {Making {{Sense}} of {{Microposts}} (\#{{Microposts2015}}) {{Named Entity rEcognition}} \& {{Linking Challenge}}},
  author = {Rizzo, Giuseppe and Cano, Amparo E},
  year = {2015},
  pages = {10},
  abstract = {Microposts are small fragments of social media content and a popular medium for sharing facts, opinions and emotions. Collectively, they comprise a wealth of data that is increasing exponentially, and which therefore presents new challenges for the Information Extraction community, among others. This paper describes the Making Sense of Microposts (\#Microposts2015) Workshop's Named Entity rEcognition and Linking (NEEL) Challenge, held as part of the 2015 World Wide Web conference (WWW'15). The challenge task comprised automatic recognition and linking of entities appearing in different event streams of English Microposts on Twitter. Participants were set the task of investigating novel strategies for extracting entities in a tweet stream, typing these based on a set of pre-defined classes, and linking to DBpedia or NIL referents. They were also asked to implement a web service to run their systems, to minimize human involvement in the evaluation and allow measuring of processing times. The challenge attracted a lot of interest: 29 research groups expressed an intent to participate, out of which 21 signed the agreement required to be given a copy of the training and development datasets. Seven teams participated in the final evaluation of the challenge task, out of which six completed all requirements, including submission of an abstract describing their approach. The submissions covered sequential and joint linguistic methods, end-to-end and hybrid end-to-end, and linguistic approaches for tackling the challenge task. We describe the evaluation process and discuss the performance of the different approaches to the \#Microposts2015 NEEL Challenge. We also release, with this paper, the \#Microposts2015 NEEL Challenge Gold Standard, comprising the set of manually annotated tweets.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Rizzo_Cano_2015_Making Sense of Microposts (#Microposts2015) Named Entity rEcognition & Linking.pdf}
}

@misc{roadsafetyandenvironment2000,
  title = {New Directions in Speed Management: A Review of Policy},
  author = {{Road Safety and Environment}},
  year = {2000},
  annotation = {ZSCC: 0000002},
  file = {/home/cjber/drive/pdf/ENVS492/Transport and the Department of the Environment_2000_.pdf}
}

@misc{roadsafetygb,
  title = {Government Should Review National Speed Limits `as Soon as Possible'},
  author = {Road Safety GB},
  langid = {american},
  annotation = {ZSCC: 0000000[s0]},
  file = {/home/cjber/drive/zotero/storage/FI565MT5/review-national-speed-limits-as-soon-as-possible.html}
}

@article{roberts2010,
  title = {Toponym {{Disambiguation Using Events}}},
  author = {Roberts, Kirk},
  year = {2010},
  pages = {6},
  abstract = {Spatial information that grounds events geographically is often ambiguous, mainly because the same location name can be used in different states, countries, or continents. Spatial mentions, known as toponyms, must be disambiguated in order to understand many spatial relations within a document. Previous methods have utilized both ``flat'' and ontologybased ranking techniques to identify the correct reference. We argue that the use of location ontologies alone is not sufficient. Since toponyms are used in documents that refer to events grounded geographically, additional pragmatic knowledge can thus be used. To be able to identify the correct reference we enhanced ontology-based methods previously reported with techniques that consider the participants in events including people, organizations, and locations. Disambiguating geographical names over an ontology is cast as a probabilistic problem resolved by logistic regression. Our experimental results on the SpatialML corpus (Mani et al. 2008) indicate that event structures do indeed play an important role in understanding toponyms.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Roberts_2010_Toponym Disambiguation Using Events.pdf}
}

@inproceedings{roberts2012,
  title = {{{UTD-SpRL}}: {{A Joint Approach}} to {{Spatial Role Labeling}}},
  shorttitle = {{{UTD-SpRL}}},
  booktitle = {*{{SEM}} 2012: {{The First Joint Conference}} on {{Lexical}} and {{Computational Semantics}} \textendash{} {{Volume}} 1: {{Proceedings}} of the Main Conference and the Shared Task, and {{Volume}} 2: {{Proceedings}} of the {{Sixth International Workshop}} on {{Semantic Evaluation}} ({{SemEval}} 2012)},
  author = {Roberts, Kirk and Harabagiu, Sanda},
  year = {2012},
  pages = {419--424},
  publisher = {{Association for Computational Linguistics}},
  address = {{Montr\'eal, Canada}},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Roberts_Harabagiu_2012_UTD-SpRL.pdf}
}

@book{robinson,
  title = {1 {{The}} Tidy Text Format | {{Text Mining}} with {{R}}},
  author = {Robinson, Julia Silge {and} David},
  abstract = {A guide to text analysis within the tidy data framework, using the tidytext package and other tidy tools},
  file = {/home/cjber/drive/zotero/storage/CHVAW8GB/tidytext.html}
}

@article{robinson2008,
  title = {Determining {{London}} Bus Stop Locations by Means of an Automatic Vehicle Location System},
  author = {Robinson, Stephen P},
  year = {2008},
  journal = {Transportation Research Record},
  volume = {2064},
  number = {1},
  pages = {24--32},
  issn = {0361-1981},
  doi = {10/bj5m3d},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000007}
}

@article{robson1994,
  title = {Urban Policy at the Crossroads},
  author = {Robson, Brian},
  year = {1994},
  journal = {Local Economy},
  volume = {9},
  number = {3},
  pages = {216--223},
  issn = {0269-0942},
  doi = {10/chxjkx},
  keywords = {\#nosource}
}

@book{robson1994a,
  title = {Assessing the Impact of Urban Policy},
  author = {Robson, Brian Turnbull},
  year = {1994},
  publisher = {{HMSO}},
  isbn = {0-11-752982-6},
  keywords = {\#nosource}
}

@book{robson1998,
  title = {Updating and Revising the Index of Local Deprivation},
  author = {Robson, Brian Turnbull and Bradford, Michael and Tomlinson, Rosemary},
  year = {1998},
  publisher = {{Department of the Environment, Transport and the Regions}},
  isbn = {1-85112-128-5},
  keywords = {\#nosource}
}

@article{roche2012,
  title = {Wiki-Place: {{Building}} Place-Based {{GIS}} from {{VGI}}},
  author = {Roche, St{\'e}phane and Feick, Rob},
  year = {2012},
  journal = {position paper from VGI Worshop Role of Volunteered Geographic Information in Advancing Science: Quality and Credibility, in conjunction with GIScience 2012},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Roche_Feick_2012_Wiki-place.pdf}
}

@article{roller2012,
  title = {Supervised {{Text-based Geolocation Using Language Models}} on an {{Adaptive Grid}}},
  author = {Roller, Stephen and Speriosu, Michael and Rallapalli, Sarat and Wing, Benjamin and Baldridge, Jason},
  year = {2012},
  pages = {11},
  abstract = {The geographical properties of words have recently begun to be exploited for geolocating documents based solely on their text, often in the context of social media and online content. One common approach for geolocating texts is rooted in information retrieval. Given training documents labeled with latitude/longitude coordinates, a grid is overlaid on the Earth and pseudo-documents constructed by concatenating the documents within a given grid cell; then a location for a test document is chosen based on the most similar pseudo-document. Uniform grids are normally used, but they are sensitive to the dispersion of documents over the earth. We define an alternative grid construction using k-d trees that more robustly adapts to data, especially with larger training sets. We also provide a better way of choosing the locations for pseudo-documents. We evaluate these strategies on existing Wikipedia and Twitter corpora, as well as a new, larger Twitter corpus. The adaptive grid achieves competitive results with a uniform grid on small training sets and outperforms it on the large Twitter corpus. The two grid constructions can also be combined to produce consistently strong results across all training sets.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Roller et al_2012_Supervised Text-based Geolocation Using Language Models on an Adaptive Grid.pdf}
}

@misc{ropensci2019,
  title = {Reproducibility {{Guide}}},
  author = {{R Open Sci}},
  year = {2019},
  howpublished = {https://ropensci.github.io/reproducibility-guide/sections/introduction/},
  file = {/home/cjber/drive/zotero/storage/M3ACNR3U/introduction.html}
}

@article{rosser2010,
  title = {Rate-My-Place: A Social Network Application for Crowd-Sourcing Vernacular Geographic Areas},
  author = {Rosser, Julian and Morley, Jeremy},
  year = {2010},
  pages = {7},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Rosser_Morley_2010_Rate-my-place.pdf}
}

@article{rossi2018,
  title = {Early Detection and Information Extraction for Weather-Induced Floods Using Social Media Streams},
  author = {Rossi, C. and Acerbo, F.S. and Ylinen, K. and Juga, I. and Nurmi, P. and Bosca, A. and Tarasconi, F. and Cristoforetti, M. and Alikadic, A.},
  year = {2018},
  month = sep,
  journal = {International Journal of Disaster Risk Reduction},
  volume = {30},
  pages = {145--157},
  issn = {22124209},
  doi = {10/gd3k8z},
  abstract = {Today we are using an unprecedented wealth of social media platforms to generate and share information regarding a wide class of events, which include extreme meteorological conditions and natural hazards such as floods. This paper proposes an automated set of services that start from the availability of weather forecasts, including both an event detection technique and a selective information retrieval from on-line social media. The envisioned services aim to provide qualitative feedback for meteorological models, detect the occurrence of an emergency event and extract informative content that can be used to complement the situational awareness. We implement such services and evaluate them during a recent weather induced flood. Our approach could be highly beneficial for monitoring agencies and meteorological offices, who act in the early warning phase, and also for authorities and first responders, who manage the emergency response phase.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Rossi et al_2018_Early detection and information extraction for weather-induced floods using.pdf}
}

@article{rottensteiner2003,
  title = {Building {{Detection Using LIDAR Data}} and {{Multi-spectral Images}}},
  author = {Rottensteiner, Franz and Trinder, John and Clode, Simon and Kubik, Kurt},
  year = {2003},
  pages = {10},
  abstract = {A method the automatic detection of buildings from LIDAR data and multi-spectral images is presented. A classification technique using various cues derived from these data is applied in a hierarchic way to overcome the problems encountered in areas of heterogeneous appearance of buildings. Both first and last pulse data and the normalised difference vegetation index are used in that process. We describe the algorithms involved, giving examples for a test site in Fairfield (NSW).},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Rottensteiner et al_2003_.pdf}
}

@book{ruppert2015,
  title = {Statistics and {{Data Analysis}} for {{Financial Engineering}}},
  author = {Ruppert, David and Matteson, David S.},
  year = {2015},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4939-2614-5},
  isbn = {978-1-4939-2613-8 978-1-4939-2614-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Ruppert_Matteson_2015_Statistics and Data Analysis for Financial Engineering.pdf}
}

@article{saeedi2009,
  title = {Object {{Extraction}} from {{LiDAR Data}} Using an {{Artificial Swarm Bee Colony Clustering Algorithm}}},
  author = {Saeedi, S and Samadzadegan, F and {El-Sheimy}, N},
  year = {2009},
  pages = {6},
  abstract = {Light Detection and Ranging (LIDAR) systems have become a standard data collection technology for capturing object surface information and 3D modeling of urban areas. Although, LIDAR systems provide detailed valuable geometric information, they still require extensive interpretation of their data for object extraction and recognition to make it practical for mapping purposes. A fundamental step in the transformation of the LIDAR data into objects is the segmentation of LIDAR data through a clustering process. Nevertheless, due to scene complexity and the variety of objects in urban areas, e.g. buildings, roads, and trees, clustering using only one single cue will not reach meaningful results. The multi dimensionality nature of LIDAR data, e.g. laser range and intensity information in both first and last echo, allow the use of more information in the data clustering process and ultimately into the reconstruction scheme. Multi dimensionality nature of LIDAR data with a dense sampling interval in urban applications, provide a huge amount of valuable information. However, this amount of information produces a lot of problems for traditional clustering techniques. This paper describes the potential of an artificial swarm bee colony optimization algorithm to find global solutions to the clustering problem of multi dimensional LIDAR data in urban areas. The artificial bee colony algorithm performs neighborhood search combined with random search in a way that is reminiscent of the food foraging behavior of swarms of honey bees. Hence, by integrating the simplicity of the k-means algorithm with the capability of the artificial bee colony algorithm, a robust and efficient clustering method for object extraction from LIDAR data is presented in this paper. This algorithm successfully applied to different LIDAR data sets in different urban areas with different size and complexities.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000018},
  file = {/home/cjber/drive/pdf/ENVS492/Saeedi et al_2009_.pdf}
}

@inproceedings{sagcan2015,
  title = {Toponym {{Recognition}} in {{Social Media}} for {{Estimating}} the {{Location}} of {{Events}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Data Mining Workshop}} ({{ICDMW}})},
  author = {Sagcan, Meryem and Karagoz, Pinar},
  year = {2015},
  month = nov,
  pages = {33--39},
  publisher = {{IEEE}},
  address = {{Atlantic City, NJ, USA}},
  doi = {10/ggwjt4},
  abstract = {Prominence of social media such as Twitter and Facebook led to a huge collection of data over which event detection provides useful results. An important dimension of event detection is location estimation for detected events. Social media provides a variety of clues for location, such as geographical annotation from smart devices, location field in the user profile and the content of the message. Among these clues, message content needs more effort for processing, yet it is generally more informative. In this paper, we focus on extraction of location names, i.e., toponym recognition, from social media messages. We propose a a hybrid system, which uses both rule based and machine learning based techniques to extract toponyms from tweets. Conditional Random Fields (CRF) is used as the machine learning tool and features such as Part-of-Speech tags and conjunction window are defined in order to construct a CRF model for toponym recognition. In the rule based part, regular expressions are used in order to define some of the toponym recognition patterns as well as to provide a simple level of normalization in order to handle the informality in the text. Experimental results show that the proposed method has higher toponym recognition ratio in comparison to the previous studies.},
  isbn = {978-1-4673-8493-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Sagcan_Karagoz_2015_Toponym Recognition in Social Media for Estimating the Location of Events.pdf}
}

@article{saito2016,
  title = {Multiple {{Object Extraction}} from {{Aerial Imagery}} with {{Convolutional Neural Networks}}},
  author = {Saito, Shunta and Yamashita, Takayoshi and Aoki, Yoshimitsu},
  year = {2016},
  month = jan,
  journal = {Journal of Imaging Science and Technology},
  volume = {60},
  number = {1},
  pages = {104021--104029},
  issn = {1062-3701},
  doi = {10/gf3tdj},
  abstract = {An automatic system to extract terrestrial objects from aerial imagery has many applications in a wide range of areas. However, in general, this task has been performed by human experts manually, so that it is very costly and time consuming. There have been many attempts at automating this task, but many of the existing works are based on class-specific features and classifiers. In this article, the authors propose a convolutional neural network (CNN)-based building and road extraction system. This takes raw pixel values in aerial imagery as input and outputs predicted three-channel label images (building\textendash road\textendash background). Using CNNs, both feature extractors and classifiers are automatically constructed. The authors propose a new technique to train a single CNN efficiently for extracting multiple kinds of objects simultaneously. Finally, they show that the proposed technique improves the prediction performance and surpasses state-of-the-art results tested on a publicly available aerial imagery dataset. c 2016 Society for Imaging Science and Technology.},
  langid = {english},
  annotation = {ZSCC: 0000067},
  file = {/home/cjber/drive/pdf/ENVS492/Saito et al_2016_.pdf}
}

@article{sakaki2010,
  title = {Earthquake Shakes {{Twitter}} Users: Real-Time Event Detection by Social Sensors},
  author = {Sakaki, Takeshi and Okazaki, Makoto and Matsuo, Yutaka},
  year = {2010},
  pages = {10},
  doi = {10/b6zm4b},
  abstract = {Twitter, a popular microblogging service, has received much attention recently. An important characteristic of Twitter is its real-time nature. For example, when an earthquake occurs, people make many Twitter posts (tweets) related to the earthquake, which enables detection of earthquake occurrence promptly, simply by observing the tweets. As described in this paper, we investigate the real-time interaction of events such as earthquakes in Twitter and propose an algorithm to monitor tweets and to detect a target event. To detect a target event, we devise a classifier of tweets based on features such as the keywords in a tweet, the number of words, and their context. Subsequently, we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location. We consider each Twitter user as a sensor and apply Kalman filtering and particle filtering, which are widely used for location estimation in ubiquitous/pervasive computing. The particle filter works better than other comparable methods for estimating the centers of earthquakes and the trajectories of typhoons. As an application, we construct an earthquake reporting system in Japan. Because of the numerous earthquakes and the large number of Twitter users throughout the country, we can detect an earthquake with high probability (96\% of earthquakes of Japan Meteorological Agency (JMA) seismic intensity scale 3 or more are detected) merely by monitoring tweets. Our system detects earthquakes promptly and sends e-mails to registered users. Notification is delivered much faster than the announcements that are broadcast by the JMA.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Sakaki et al_2010_Earthquake shakes Twitter users.pdf}
}

@article{sampath2008,
  title = {Building {{Roof Segmetnation}} and {{Reconstruction}} from {{LiDAR Point Clouds}} Using {{Clustering Techniques}}},
  author = {Sampath, Aparajithan and Shan, Jie},
  year = {2008},
  pages = {6},
  abstract = {This paper presents an approach to creating a polyhedral model of building roof from LiDAR point clouds using clustering techniques. A building point cloud is first separated into planar and breakline sections using the eigenvalues of the covariance matrix in a small neighbourhood. The planar components from the point cloud are then grouped into small patches containing 6-8 points and their normal vector parameters are determined. The normal vectors are then clustered together to determine the principal directions of the roof planes. Directly using a clustering algorithm on normal vectors presents difficulties due to a lack of a-priori information on approximate roof directions. Therefore, a potential based approach is used iteratively with the k-means algorithm. This generates the necessary planar parameters, and segments the LiDAR roof points. For reconstruction, a plane adjacency matrix is created for the roof using the segmented roof points. Planes that intersect each other are identified and breaklines and roof vertices are generated by solving the intersecting planar equations. A vector polyhedral model of the roof is created.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000022},
  file = {/home/cjber/drive/pdf/ENVS492/Sampath_Shan_2008_.pdf}
}

@article{sanh2020,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2020},
  month = feb,
  journal = {arXiv:1910.01108 [cs]},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Sanh et al_2020_DistilBERT, a distilled version of BERT.pdf}
}

@inproceedings{sankaranarayanan2009,
  title = {{{TwitterStand}}: News in Tweets},
  shorttitle = {{{TwitterStand}}},
  booktitle = {Proceedings of the 17th {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}} - {{GIS}} '09},
  author = {Sankaranarayanan, Jagan and Samet, Hanan and Teitler, Benjamin E. and Lieberman, Michael D. and Sperling, Jon},
  year = {2009},
  pages = {42},
  publisher = {{ACM Press}},
  address = {{Seattle, Washington}},
  doi = {10/cqf7gr},
  isbn = {978-1-60558-649-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Sankaranarayanan et al_2009_TwitterStand.pdf}
}

@article{santos2018,
  title = {Toponym Matching through Deep Neural Networks},
  author = {Santos, Rui and {Murrieta-Flores}, Patricia and Calado, P{\'a}vel and Martins, Bruno},
  year = {2018},
  month = feb,
  journal = {International Journal of Geographical Information Science},
  volume = {32},
  number = {2},
  pages = {324--348},
  issn = {1365-8816, 1362-3087},
  doi = {10/ggh858},
  abstract = {Toponym matching, i.e. pairing strings that represent the same real-world location, is a fundamental problemfor several practical applications. The current state-of-the-art relies on string similarity metrics, either specifically developed for matching place names or integrated within methods that combine multiple metrics. However, these methods all rely on common sub-strings in order to establish similarity, and they do not effectively capture the character replacements involved in toponym changes due to transliterations or to changes in language and culture over time. In this article, we present a novel matching approach, leveraging a deep neural network to classify pairs of toponyms as either matching or nonmatching. The proposed network architecture uses recurrent nodes to build representations from the sequences of bytes that correspond to the strings that are to be matched. These representations are then combined and passed to feed-forward nodes, finally leading to a classification decision. We present the results of a wide-ranging evaluation on the performance of the proposed method, using a large dataset collected from the GeoNames gazetteer. These results show that the proposed method can significantly outperform individual similarity metrics from previous studies, as well as previous methods based on supervised machine learning for combining multiple metrics.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Santos et al_2018_Toponym matching through deep neural networks.pdf}
}

@inproceedings{saravanou2015,
  title = {Twitter {{Floods}} When It {{Rains}}: {{A Case Study}} of the {{UK Floods}} in Early 2014},
  shorttitle = {Twitter {{Floods}} When It {{Rains}}},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{World Wide Web}}},
  author = {Saravanou, Antonia and Valkanas, George and Gunopulos, Dimitrios and Andrienko, Gennady},
  year = {2015},
  month = may,
  pages = {1233--1238},
  publisher = {{ACM}},
  address = {{Florence Italy}},
  doi = {10/ghcxcv},
  abstract = {Twitter is one of the most prominent social media platforms nowadays. A primary reason that has brought the medium at the spotlight of academic attention is its real-time nature, with people constantly uploading information regarding their surroundings. This trait, coupled with the service's data access policy for researchers and developers, has allowed the community to explore Twitter's potential as a news reporting tool. Finding out promptly about newsworthy events can prove extremely useful in crisis management situations. In this paper, we explore the use of Twitter as a mechanism used in disaster relief, and consequently in public safety. In particular, we perform a case study on the floods that occurred in the United Kingdom during January 2014, and how these were reflected on Twitter, according to tweets (i.e., posts) submitted by the users. We present a systematic algorithmic analysis of tweets collected with respect to our use case scenario, supplemented by visual analytic tools. Our objective is to identify meaningful and effective ways to take advantage of the wealth of Twitter data in crisis management, and we report on the findings of our analysis.},
  isbn = {978-1-4503-3473-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Saravanou et al_2015_Twitter Floods when it Rains.pdf}
}

@inproceedings{scheider2013,
  title = {Semantic Place Localization from Narratives},
  booktitle = {Proceedings of {{The First ACM SIGSPATIAL International Workshop}} on {{Computational Models}} of {{Place}} - {{COMP}} '13},
  author = {Scheider, Simon and Purves, Ross},
  year = {2013},
  pages = {16--19},
  publisher = {{ACM Press}},
  address = {{Orlando FL, USA}},
  doi = {10/ggwjts},
  abstract = {Place narratives provide a rich resource of learning how humans localize places. Place localization can be done in various ways, relative to other spatial referents, and relative to agents and their activities in which these referents may be involved. How can we describe places based on their spatial and semantic relationships to objects, qualities, and activities? How can these relations help us improve automated localization of places implicit in textual descriptions? In this paper, we motivate research on extraction of semantic place localization statements from text corpora which can be used for improving document retrieval and for reconstructing locations. The idea is to combine Semantic Web reasoning with existing geographic information retrieval (GIR) and structural text extraction for this purpose. GIR and Semantic Web technology have matured during the last years, but still largely exist in parallel. Current localization approaches have been focusing on the extraction of unstructured word lists from texts, including toponyms and geographic features, not on human place descriptions on a sentence level.},
  isbn = {978-1-4503-2535-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Scheider_Purves_2013_Semantic place localization from narratives.pdf}
}

@article{schilder2004,
  title = {Extracting Spatial Information: Grounding, Classifying and Linking Spatial Expressions},
  author = {Schilder, Frank and Versley, Yannick and Habel, Christopher},
  year = {2004},
  pages = {3},
  abstract = {This paper is concerned with the tagging of spatial expressions in German newspaper articles, assigning a meaning to the expression and classifying the usages of the spatial expression and linking the derived referent to an event description.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Schilder et al_2004_Extracting spatial information.pdf}
}

@article{schneider2010,
  title = {Read All about It: {{The}} Role of the Media in Improving Construction Safety and Health},
  shorttitle = {Read All about It},
  author = {Schneider, Scott and Check, Pietra},
  year = {2010},
  month = jun,
  journal = {Journal of Safety Research},
  series = {Special {{Topic}}: {{Construction Safety}}},
  volume = {41},
  number = {3},
  pages = {283--287},
  issn = {0022-4375},
  doi = {10/bh8jrq},
  langid = {english},
  file = {/home/cjber/drive/pdf/Schneider_Check_2010_Read all about it.pdf;/home/cjber/drive/zotero/storage/CDF6334Z/S0022437510000460.html}
}

@article{schockaert2008,
  title = {Location Approximation for Local Search Services Using Natural Language Hints},
  author = {Schockaert, S. and De Cock, M. and Kerre, E. E.},
  year = {2008},
  month = mar,
  journal = {International Journal of Geographical Information Science},
  volume = {22},
  number = {3},
  pages = {315--336},
  issn = {1365-8816, 1362-3087},
  doi = {10/dqxk27},
  langid = {english},
  keywords = {Ontology},
  file = {/home/cjber/drive/zotero/storage/GH5JD2GU/Schockaert et al. - 2008 - Location approximation for local search services u.asp}
}

@book{schumacker2013,
  title = {Understanding {{Statistics Using R}}},
  author = {Schumacker, Randall and Tomek, Sara},
  year = {2013},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-6227-9},
  isbn = {978-1-4614-6226-2 978-1-4614-6227-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/Schumacker_Tomek_2013_Understanding Statistics Using R.pdf}
}

@inproceedings{seabold2010,
  title = {Statsmodels: {{Econometric}} and {{Statistical Modeling}} with {{Python}}},
  shorttitle = {Statsmodels},
  booktitle = {Python in {{Science Conference}}},
  author = {Seabold, Skipper and Perktold, Josef},
  year = {2010},
  pages = {92--96},
  address = {{Austin, Texas}},
  doi = {10/ggq6ff},
  abstract = {Statsmodels is a library for statistical and econometric analysis in Python. This paper discusses the current relationship between statistics and Python and open source more generally, outlining how the statsmodels package fills a gap in this relationship. An overview of statsmodels is provided, including a discussion of the overarching design and philosophy, what can be found in the package, and some usage examples. The paper concludes with a look at what the future holds.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Seabold_Perktold_2010_Statsmodels.pdf}
}

@article{see2016,
  title = {Crowdsourcing, {{Citizen Science}} or {{Volunteered Geographic Information}}? {{The Current State}} of {{Crowdsourced Geographic Information}}},
  shorttitle = {Crowdsourcing, {{Citizen Science}} or {{Volunteered Geographic Information}}?},
  author = {See, Linda and Mooney, Peter and Foody, Giles and Bastin, Lucy and Comber, Alexis and Estima, Jacinto and Fritz, Steffen and Kerle, Norman and Jiang, Bin and Laakso, Mari and Liu, Hai-Ying and Mil{\v c}inski, Grega and Nik{\v s}i{\v c}, Matej and Painho, Marco and P{\H o}d{\"o}r, Andrea and {Olteanu-Raimond}, Ana-Maria and Rutzinger, Martin},
  year = {2016},
  month = may,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {5},
  number = {5},
  pages = {55},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/ijgi5050055},
  abstract = {Citizens are increasingly becoming an important source of geographic information, sometimes entering domains that had until recently been the exclusive realm of authoritative agencies. This activity has a very diverse character as it can, amongst other things, be active or passive, involve spatial or aspatial data and the data provided can be variable in terms of key attributes such as format, description and quality. Unsurprisingly, therefore, there are a variety of terms used to describe data arising from citizens. In this article, the expressions used to describe citizen sensing of geographic information are reviewed and their use over time explored, prior to categorizing them and highlighting key issues in the current state of the subject. The latter involved a review of \textasciitilde 100 Internet sites with particular focus on their thematic topic, the nature of the data and issues such as incentives for contributors. This review suggests that most sites involve active rather than passive contribution, with citizens typically motivated by the desire to aid a worthy cause, often receiving little training. As such, this article provides a snapshot of the role of citizens in crowdsourcing geographic information and a guide to the current status of this rapidly emerging and evolving subject.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {citizen science,crowdsourcing,mapping,volunteered geographic information},
  file = {/home/cjber/drive/pdf/See et al_2016_Crowdsourcing, Citizen Science or Volunteered Geographic Information.pdf;/home/cjber/drive/zotero/storage/TFTKUQ3P/55.html}
}

@article{senaratne2017,
  title = {A Review of Volunteered Geographic Information Quality Assessment Methods},
  author = {Senaratne, Hansi and Mobasheri, Amin and Ali, Ahmed Loai and Capineri, Cristina and Haklay, Mordechai (Muki)},
  year = {2017},
  month = jan,
  journal = {International Journal of Geographical Information Science},
  volume = {31},
  number = {1},
  pages = {139--167},
  issn = {1365-8816, 1362-3087},
  doi = {10/gddwx5},
  abstract = {With the ubiquity of advanced web technologies and locationsensing hand held devices, citizens regardless of their knowledge or expertise, are able to produce spatial information. This phenomenon is known as volunteered geographic information (VGI). During the past decade VGI has been used as a data source supporting a wide range of services, such as environmental monitoring, events reporting, human movement analysis, disaster management, etc. However, these volunteer-contributed data also come with varying quality. Reasons for this are: data is produced by heterogeneous contributors, using various technologies and tools, having different level of details and precision, serving heterogeneous purposes, and a lack of gatekeepers. Crowd-sourcing, social, and geographic approaches have been proposed and later followed to develop appropriate methods to assess the quality measures and indicators of VGI. In this article, we review various quality measures and indicators for selected types of VGI and existing quality assessment methods. As an outcome, the article presents a classification of VGI with current methods utilized to assess the quality of selected types of VGI. Through these findings, we introduce data mining as an additional approach for quality handling in VGI.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Senaratne et al_2017_A review of volunteered geographic information quality assessment methods.pdf}
}

@book{seu1998,
  title = {Bringing {{Britain}} Together : A National Strategy for Neighbourhood Renewal.},
  author = {SEU},
  year = {1998},
  publisher = {{Stationery Office}},
  address = {{London}},
  isbn = {0-10-140452-2 978-0-10-140452-5},
  langid = {english},
  keywords = {\#nosource}
}

@article{shao1993,
  title = {Linear {{Model Selection}} by {{Cross-validation}}},
  author = {Shao, Jun},
  year = {1993},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {422},
  pages = {486--494},
  issn = {0162-1459, 1537-274X},
  doi = {10/gdq9pn},
  langid = {english},
  keywords = {\#nosource}
}

@article{sharafi,
  title = {{{LIDAR Image Segmentation Using Hierarchical Clustering}}},
  author = {Sharafi, Tahereh and Akbarizadeh, Gholamreza},
  journal = {American Journal of Computing Research Repository},
  pages = {9},
  abstract = {Hierarchical clustering method is adopted for LIDAR image segmentation after extracting the intended features for identifying complex objects. In the experiments, four LIDAR images with different numbers of areas (sea, forest, desert, and urban) were used for examining the algorithm. The efficiency of image segmentation was generally evaluated visually because the segments of the main image typically lack certain, fixed features and depend on the criterion used for pattern distance/similarity as well as the threshold for cluster separation. For each experiment, hierarchical clustering method was employed by creating the clustering hierarchy tree and specifying the optimal number of clusters on the basis of tree data. Once the optimal number of clusters was determined, the similarity matrix of data image patterns was separated according to Euclidean distance algorithm in terms of greatest similarity among the patterns to the number of clusters. Clustering was then performed. The program output comprised labeled images for samples specifying which pattern pertains to which cluster. The images associated with each cluster are displayed separated from other clusters with other areas eliminated. The results indicated that for LIDAR images that lack a certain, fixed feature, the hierarchical clustering method for segmentation can perform separation and labeling.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {/home/cjber/drive/pdf/ENVS492/LiDAR/Sharafi_Akbarizadeh_.pdf}
}

@book{shaw1999,
  title = {The Widening Gap: Health Inequalities and Policy in {{Britain}}},
  author = {Shaw, Mary and Dorling, Daniel and Gordon, David and Smith, George Davey},
  year = {1999},
  publisher = {{Policy Press}},
  isbn = {1-86134-142-3},
  keywords = {\#nosource}
}

@article{shi2019,
  title = {Simple {{BERT Models}} for {{Relation Extraction}} and {{Semantic Role Labeling}}},
  author = {Shi, Peng and Lin, Jimmy},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.05255 [cs]},
  eprint = {1904.05255},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Shi_Lin_2019_Simple BERT Models for Relation Extraction and Semantic Role Labeling.pdf;/home/cjber/drive/zotero/storage/7MUB8NUQ/1904.html}
}

@article{shouls1996,
  title = {Modelling Inequality in Reported Long Term Illness in the {{UK}}: Combining Individual and Area Characteristics.},
  shorttitle = {Modelling Inequality in Reported Long Term Illness in the {{UK}}},
  author = {Shouls, S and Congdon, P and Curtis, S},
  year = {1996},
  month = jun,
  journal = {Journal of Epidemiology \& Community Health},
  volume = {50},
  number = {3},
  pages = {366--376},
  issn = {0143-005X},
  doi = {10/cxqx7p},
  abstract = {Study objective - To assess the nature of the relation between health and social factors at both the aggregated scale of geographical areas and the individual scale.},
  langid = {english},
  annotation = {ZSCC: 0000205},
  file = {/home/cjber/drive/pdf/ENVS416/Shouls et al_1996_.pdf}
}

@book{siciliano2009,
  title = {Robotics},
  author = {Siciliano, Bruno and Sciavicco, Lorenzo and Villani, Luigi and Oriolo, Giuseppe},
  editor = {Grimble, Michael J. and Johnson, Michael A.},
  year = {2009},
  series = {Advanced {{Textbooks}} in {{Control}} and {{Signal Processing}}},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-84628-642-1},
  isbn = {978-1-84628-641-4 978-1-84628-642-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Siciliano et al_2009_Robotics.pdf}
}

@article{simon2015,
  title = {Socializing in Emergencies\textemdash{{A}} Review of the Use of Social Media in Emergency Situations},
  author = {Simon, Tomer and Goldberg, Avishay and Adini, Bruria},
  year = {2015},
  month = oct,
  journal = {International Journal of Information Management},
  volume = {35},
  number = {5},
  pages = {609--619},
  issn = {0268-4012},
  doi = {10/gmkgpb},
  abstract = {Social media tools are integrated in most parts of our daily lives, as citizens, netizens, researchers or emergency responders. Lessons learnt from disasters and emergencies that occurred globally in the last few years have shown that social media tools may serve as an integral and significant component of crisis response. Communication is one of the fundamental tools of emergency management. It becomes crucial when there are dozens of agencies and organizations responding to a disaster. Regardless of the type of emergency, whether a terrorist attack, a hurricane or an earthquake, communication lines may be overloaded and cellular networks overwhelmed as too many people attempt to use them to access information. Social scientists have presented that post-disaster active public participation was largely altruistic, including activities such as search and rescue, first aid treatment, victim evacuation, and on-line help. Social media provides opportunities for engaging citizens in the emergency management by both disseminating information to the public and accessing information from them. During emergency events, individuals are exposed to large quantities of information without being aware of their validity or risk of misinformation, but users are usually swift to correct them, thus making the social media ``self-regulating''.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Simon et al_2015_Socializing in emergencies—A review of the use of social media in emergency.pdf;/home/cjber/drive/zotero/storage/EPA2RTHI/S0268401215000638.html}
}

@article{singh2014,
  title = {Road {{Detection}} from {{Remote Sensing Images}} Using {{Impervious Surface Characteristics}}: {{Review}} and {{Implication}}},
  shorttitle = {Road {{Detection}} from {{Remote Sensing Images}} Using {{Impervious Surface Characteristics}}},
  author = {Singh, P. P. and Garg, R. D.},
  year = {2014},
  month = nov,
  journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {XL-8},
  pages = {955--959},
  issn = {2194-9034},
  doi = {10/gcdf7s},
  abstract = {The extraction of road network is an emerging area in information extraction from high-resolution satellite images (HRSI). It is also an interesting field that incorporates various tactics to achieve road network. The process of road detection from remote sensing images is quite complex, due to the presence of various noises. These noises could be the vehicles, crossing lines and toll bridges. Few small and large false road segments interrupt the extraction of road segments that happens due to the similar spectral behavior in heterogeneous objects. To achieve a better level of accuracy, numerous factors play their important role, such as spectral data of satellite sensor and the information related to land surface area. Therefore the interpretation varies on processing of images with different heuristic parameters. These parameters have tuned according to the road characteristics of the terrain in satellite images. There are several approaches proposed and implemented to extract the roads from HRSI comprising a single or hybrid method. This kind of hybrid approach has also improved the accuracy of road extraction in comparison to a single approach. Some characteristics related to impervious and non-impervious surfaces are used as salient features that help to improve the extraction of road area only in the correct manner. These characteristics also used to utilize the spatial, spectral and texture features to increase the accuracy of classified results. Therefore, aforesaid characteristics have been utilized in combination of road spectral properties to extract road network only with improved accuracy. This evaluated road network is quite accurate with the help of these defined methodologies.},
  langid = {english},
  annotation = {ZSCC: 0000005},
  file = {/home/cjber/drive/pdf/ENVS492/Singh_Garg_2014_.pdf}
}

@book{skansi2018,
  title = {Introduction to {{Deep Learning}}: {{From Logical Calculus}} to {{Artificial Intelligence}}},
  shorttitle = {Introduction to {{Deep Learning}}},
  author = {Skansi, Sandro},
  year = {2018},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-73004-2},
  isbn = {978-3-319-73003-5 978-3-319-73004-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Skansi_2018_Introduction to Deep Learning.pdf}
}

@book{skiena2017,
  title = {The {{Data Science Design Manual}}},
  author = {Skiena, Steven S.},
  year = {2017},
  series = {Texts in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-55444-0},
  isbn = {978-3-319-55443-3 978-3-319-55444-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Skiena_2017_The Data Science Design Manual.pdf}
}

@article{skoumas2016,
  title = {Location {{Estimation Using Crowdsourced Spatial Relations}}},
  author = {Skoumas, Georgios and Pfoser, Dieter and Kyrillidis, Anastasios and Sellis, Timos},
  year = {2016},
  month = jul,
  journal = {ACM Transactions on Spatial Algorithms and Systems},
  volume = {2},
  number = {2},
  pages = {1--23},
  issn = {2374-0353, 2374-0361},
  doi = {10.1145/2894745},
  langid = {english},
  file = {/home/cjber/drive/pdf/Skoumas et al_2016_Location Estimation Using Crowdsourced Spatial Relations.pdf;/home/cjber/drive/pdf/Skoumas et al_2016_Location Estimation Using Crowdsourced Spatial Relations2.pdf}
}

@article{smadja2010,
  title = {Road {{Extraction}} and {{Environment Interpretation}} from {{LiDAR Sensors}}},
  author = {Smadja, Laurent and Ninot, Jerome and Gavrilovic, Thomas},
  year = {2010},
  pages = {6},
  abstract = {We present in this article a new vehicle dedicated to road surveying, equipped with a highly precise positioning system, 2D lidar scans and high definition color images. We focus at first on the sensors extrinsic calibration process. Once all sensors have been positioned in the same coordinates system, 3D realistic environments can be computed and interpreted. Moreover, an original algorithm for road extraction has been developed. This two-step method is based on the local road shape and does not rely on the presence of curbs or guardrails. Different uses of the RanSaC algorithm are employed, for road sides rough estimation in the first place, then for unlikely candidates elimination. Road boundary and center points are further processed for road width and curvature computation in order to feed a geographic information system. Finally, a simple extraction of traffic signs and road markings is presented.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000040},
  file = {/home/cjber/drive/pdf/ENVS492/Smadja et al_2010_.pdf}
}

@article{smith,
  title = {Ontological Relations},
  author = {Smith, Barry},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Smith_Ontological relations.pdf;/home/cjber/drive/zotero/storage/THF2B23C/288430800.html}
}

@article{smith1999,
  title = {Area-Based {{Initiatives}}: {{The}} Rationale and Options for Area Targeting},
  author = {Smith, Gillian R},
  year = {1999},
  pages = {78},
  abstract = {This paper explores the rationale for area targeting and the growth of new area-based initiatives. The author examines the geographical concentration of deprivation, the extent, and whether there is a polarisation between areas. The evidence confirms that there is a clear rationale for area-based approaches, although it should not be assumed that they will be the most effective means to improve conditions in all cases. The evidence suggests that there should be a closer link between area-based approaches and national level main programmes, given the time limited nature of the former and the fact that they only reach a minority of all deprived people. The issues involved in identifying target areas are also reviewed including the question of who decides and on what basis. It is concluded that understanding the spatial distribution of deprivation is crucial and that there is an urgent need for better, more up to date statistical and other data relating to small geographical areas.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/Smith_1999_.pdf}
}

@incollection{smith2001,
  title = {Disambiguating {{Geographic Names}} in a {{Historical Digital Library}}},
  booktitle = {Research and {{Advanced Technology}} for {{Digital Libraries}}},
  author = {Smith, David A. and Crane, Gregory},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Constantopoulos, Panos and S{\o}lvberg, Ingeborg T.},
  year = {2001},
  volume = {2163},
  pages = {127--136},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44796-2_12},
  abstract = {Geographic interfaces provide natural, scalable visualizations for many digital library collections, but the wide range of data in digital libraries presents some particular problems for identifying and disambiguating place names. We describe the toponym-disambiguation system in the Perseus digital library and evaluate its performance. Name categorization varies significantly among different types of documents, but toponym disambiguation performs at a high level of precision and recall with a gazetteer an order of magnitude larger than most other applications.},
  isbn = {978-3-540-42537-3 978-3-540-44796-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Smith_Crane_2001_Disambiguating Geographic Names in a Historical Digital Library.pdf}
}

@inproceedings{smith2003,
  title = {Bootstrapping Toponym Classifiers},
  booktitle = {Proceedings of the {{HLT-NAACL}} 2003 Workshop on {{Analysis}} of Geographic References  -},
  author = {Smith, David A. and Mann, Gideon S.},
  year = {2003},
  volume = {1},
  pages = {45--49},
  publisher = {{Association for Computational Linguistics}},
  address = {{Not Known}},
  doi = {10/bsxpc9},
  abstract = {We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems. We train data-driven place name classifiers using toponyms already disambiguated in the training text \textemdash{} by such existing cues as ``Nashville, Tenn.'' or ``Springfield, MA'' \textemdash{} and test the system on texts where these cues have been stripped out and on hand-tagged historical texts. We experiment on three English-language corpora of varying provenance and complexity: newsfeed from the 1990s, personal narratives from the 19th century American west, and memoirs and records of the U.S. Civil War. Disambiguation accuracy ranges from 87\% for news to 69\% for some historical collections.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Smith_Mann_2003_Bootstrapping toponym classifiers.pdf}
}

@article{smith2010,
  title = {Neighbourhood Food Environment and Area Deprivation: Spatial Accessibility to Grocery Stores Selling Fresh Fruit and Vegetables in Urban and Rural Settings},
  shorttitle = {Neighbourhood Food Environment and Area Deprivation},
  author = {Smith, D. M and Cummins, S. and Taylor, M. and Dawson, J. and Marshall, D. and Sparks, L. and Anderson, A. S},
  year = {2010},
  month = feb,
  journal = {International Journal of Epidemiology},
  volume = {39},
  number = {1},
  pages = {277--284},
  issn = {0300-5771, 1464-3685},
  doi = {10/cvgs3j},
  abstract = {Background The `deprivation amplification' hypothesis suggests that residents of deprived neighbourhoods have universally poorer access to highquality food environments, which in turn contributes to the development of spatial inequalities in diet and diet-related chronic disease. This paper presents results from a study that quantified access to grocery stores selling fresh fruit and vegetables in four environmental settings in Scotland, UK. Methods Spatial accessibility, as measured by network travel times, to 457 grocery stores located in 205 neighbourhoods in four environmental settings (island, rural, small town and urban) in Scotland was calculated using Geographical Information Systems. The distribution of accessibility by neighbourhood deprivation in each of these four settings was investigated. Results Overall, the most deprived neighbourhoods had the best access to grocery stores and grocery stores selling fresh produce. Stratified analysis by environmental setting suggests that the least deprived compared with the most deprived urban neighbourhoods have greater accessibility to grocery stores than their counterparts in island, rural and small town locations. Access to fresh produce is better in more deprived compared with less deprived urban and small town neighbourhoods, but poorest in the most affluent island communities with mixed results for rural settings. Conclusions The results presented here suggest that the assumption of a universal `deprivation amplification' hypothesis in studies of the neighbourhood food environment may be misguided. Associations between neighbourhood deprivation and grocery store accessibility vary by environmental setting. Theories and policies aimed at understanding and rectifying spatial inequalities in the distribution},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Smith et al_2010_.pdf}
}

@article{smith2012,
  title = {Accessibility and Capability: The Minimum Transport Needs and Costs of Rural Households},
  shorttitle = {Accessibility and Capability},
  author = {Smith, Noel and Hirsch, Donald and Davis, Abigail},
  year = {2012},
  month = mar,
  journal = {Journal of Transport Geography},
  volume = {21},
  pages = {93--101},
  issn = {09666923},
  doi = {10/f3w23n},
  abstract = {As a minimum, how much do rural households need to be able to afford adequate transport? This paper is drawn from the Minimum Income Standards (MIS) research programme, which primarily involves groups of members of the public reaching consensus about what households need for a minimum, socially acceptable standard of living. The paper looks at the additional needs and costs of rural households, compared with their urban counterparts, and focuses on the methodology used to research these costs. This discussion is framed in terms of transport disadvantage, and the Capability Approach. The results of the research are presented: how travel needs and costs vary for different household types; and how minimum transport costs impact on overall household budgets. The paper concludes by considering the possible application of the `MIS Rural' approach in practice.},
  langid = {english},
  annotation = {ZSCC: 0000052},
  file = {/home/cjber/drive/pdf/ENVS492/Smith et al_2012_.pdf}
}

@article{smith2015,
  title = {The {{English}} Indices of Deprivation 2015},
  author = {Smith, Tom and Noble, Michael and Noble, Stefan and Wright, Gemma and McLennan, David and Plunkett, Emma},
  year = {2015},
  journal = {London: Department for Communities and Local Government},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/smith2015.pdf}
}

@misc{smith2017,
  title = {20mph Speed Limit Sees an Increase in Road Deaths but Here's Why They Won't Be Scrapped},
  author = {Smith, Luke John},
  year = {2017},
  month = dec,
  journal = {Express.co.uk},
  abstract = {NEW research has revealed that the `safer' 20mph zones have caused a rise in the number of road deaths and crashes, but the council isn't scrapping them and here's why.},
  howpublished = {https://www.express.co.uk/life-style/cars/893770/20mph-speed-limit-increase-road-deaths-crash},
  langid = {english},
  annotation = {ZSCC: 0000000[s0]},
  file = {/home/cjber/drive/zotero/storage/EQ4TDNLL/20mph-speed-limit-increase-road-deaths-crash.html}
}

@article{smole,
  title = {Evaluation of Inductive Logic Programming for Information Extraction from Natural Language Texts to Support Spatial Data Recommendation Services},
  author = {Smole, Domen},
  pages = {22},
  doi = {10/cqq3zk},
  langid = {english},
  file = {/home/cjber/drive/pdf/Smole_Evaluation of inductive logic programming for information extraction from.pdf;/home/cjber/drive/pdf/Smole_Evaluation of inductive logic programming for information extraction from2.pdf}
}

@article{snooks2002,
  title = {{{NHS}} Emergency Response to 999 Calls: Alternatives for Cases That Are Neither Life Threatening nor Serious},
  shorttitle = {{{NHS}} Emergency Response to 999 Calls},
  author = {Snooks, H.},
  year = {2002},
  month = aug,
  journal = {BMJ},
  volume = {325},
  number = {7359},
  pages = {330--333},
  issn = {09598138, 14685833},
  doi = {10.1136/bmj.325.7359.330},
  langid = {english},
  keywords = {ES},
  file = {/home/cjber/drive/pdf/Snooks_2002_NHS emergency response to 999 calls.pdf;/home/cjber/drive/pdf/Snooks_2002_NHS emergency response to 999 calls2.pdf}
}

@article{soares2019,
  title = {Matching the {{Blanks}}: {{Distributional Similarity}} for {{Relation Learning}}},
  shorttitle = {Matching the {{Blanks}}},
  author = {Soares, Livio Baldini and FitzGerald, Nicholas and Ling, Jeffrey and Kwiatkowski, Tom},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.03158 [cs]},
  eprint = {1906.03158},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Soares et al_2019_Matching the Blanks.pdf;/home/cjber/drive/pdf/Soares et al_2019_Matching the Blanks2.pdf}
}

@misc{soh2020,
  title = {{{BERT}}({{S}}) for {{Relation Extraction}} in {{NLP}}},
  author = {Soh, Wee Tee},
  year = {2020},
  month = dec,
  journal = {Medium},
  abstract = {Implementation of ``Matching the Blanks: Distributional Similarity for Relation Learning``, a paper by Google Research},
  howpublished = {https://towardsdatascience.com/bert-s-for-relation-extraction-in-nlp-2c7c3ab487c4},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/XG8L62VY/bert-s-for-relation-extraction-in-nlp-2c7c3ab487c4.html}
}

@article{soja1980,
  title = {The Socio-Spatial Dialectic},
  author = {Soja, Edward W},
  year = {1980},
  journal = {Annals of the Association of American geographers},
  volume = {70},
  number = {2},
  pages = {207--225},
  issn = {0004-5608},
  doi = {10/dgwsjq},
  keywords = {\#nosource}
}

@article{solomon2009,
  title = {Setting Accessibility Standards for Social Inclusion: Some Obstacles},
  author = {Solomon, J and Titheridge, H},
  year = {2009},
  pages = {11},
  abstract = {One of the principal rationales for accessibility planning in the UK is the potential reduction of social exclusion. Although there are multiple causes of social exclusion, transport and accessibility limitations contribute to a greater or lesser extent. It is in the light of this understanding the transport authorities are asked to devise policies which will promote inclusion. However, although the literature on social exclusion/inclusion is now quite substantial, and there are a number of Government documents available explaining the connections and their consequences, transport authorities are given little, if any, guidance by Central Government about the levels of accessibility to which they should and could reasonably be expected to aspire. This paper is based around research initially undertaken as part of an EPSRC-funded project \textendash{} AUNT-SUE (Accessibility and User Needs in Transport for Sustainable Urban Environments) which aims ``to develop and test sustainable policies and practice that will deliver effective socially inclusive design and operation in transport and the associated public realm from macro down to micro level.'' As part of this project, accessibility benchmarks and standards appropriate to various socially-excluded groups have been, and are being, developed and tested using both existing data and field research. The paper discusses the progress of this work and the difficulties of arriving at solutions which can both adequately reflect the needs of the affected groups, as groups rather than individuals, and which can also be successfully modelled.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS492/Solomon_Titheridge_2009_.pdf}
}

@article{songchon2021,
  title = {Quality Assessment of Crowdsourced Social Media Data for Urban Flood Management},
  author = {Songchon, Chanin and Wright, Grant and Beevers, Lindsay},
  year = {2021},
  month = nov,
  journal = {Computers, Environment and Urban Systems},
  volume = {90},
  pages = {101690},
  issn = {01989715},
  doi = {10/gmnpsd},
  abstract = {Urban flooding can cause widespread devastation in terms of loss of life and damage to property. As such, monitoring urban flood evolution is crucial in identifying the most affected areas, where emergency response resources should be directed. Flood monitoring through airborne or satellite remote sensing is often limited due to weather conditions and urban topography. In contrast, crowdsourced data is not affected by weather or topog\- raphy, and they hence offer great potential for urban flood monitoring through real-time information shared by individuals. Despite the benefits, there is no guarantee of quality associated with crowdsourced data, which hampers its usability. In this paper, we present and evaluate two different approaches (binary logistic regression and fuzzy logic) to assess the quality of crowdsourced social media data retrieved from the public Twitter archive. Input variables were constructed based on Twitter metadata and spatiotemporal analysis. Both models were trained and tested using actual flood-related information Tweeted during three consecutive years of flooding in Phetchaburi City, Thailand (2016 to 2018), and produced good results. The fuzzy logic approach is shown to perform better, however its implementation involves significantly more subjectivity. The ability to assess data quality enables the uncertainty associated with crowdsourced social media data to be estimated, which allows this type of data to supplement conventional observations, and hence improve flood management activities.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Songchon et al_2021_Quality assessment of crowdsourced social media data for urban flood management.pdf}
}

@inproceedings{sorokin2017,
  title = {Context-{{Aware Representations}} for {{Knowledge Base Relation Extraction}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Sorokin, Daniil and Gurevych, Iryna},
  year = {2017},
  pages = {1784--1789},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10/gf4v8t},
  abstract = {We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation. Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence. We combine the context representations with an attention mechanism to make the final prediction.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Sorokin_Gurevych_2017_Context-Aware Representations for Knowledge Base Relation Extraction.pdf}
}

@manual{south2017,
  type = {Manual},
  title = {Rnaturalearth: {{World}} Map Data from Natural Earth},
  author = {South, Andy},
  year = {2017}
}

@article{speriosu2013,
  title = {Methods and {{Applications}} of {{Text-Driven Toponym Resolution}} with {{Indirect Supervision}}},
  author = {Speriosu, Michael},
  year = {2013},
  pages = {173},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Speriosu_2013_Methods and Applications of Text-Driven Toponym Resolution with Indirect.pdf}
}

@article{speriosu2013a,
  title = {Text-{{Driven Toponym Resolution}} Using {{Indirect Supervision}}},
  author = {Speriosu, Michael and Baldridge, Jason},
  year = {2013},
  pages = {10},
  abstract = {Toponym resolvers identify the specific locations referred to by ambiguous placenames in text. Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population. This paper shows that text-driven disambiguation for toponyms is far more effective. We exploit document-level geotags to indirectly generate training instances for text classifiers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/zotero/storage/P8FKGB6M/Speriosu and Baldridge - Text-Driven Toponym Resolution using Indirect Supe.pdf}
}

@inproceedings{spielhofer2016,
  title = {Data Mining {{Twitter}} during the {{UK}} Floods: {{Investigating}} the Potential Use of Social Media in Emergency Management},
  shorttitle = {Data Mining {{Twitter}} during the {{UK}} Floods},
  booktitle = {2016 3rd {{International Conference}} on {{Information}} and {{Communication Technologies}} for {{Disaster Management}} ({{ICT-DM}})},
  author = {Spielhofer, Thomas and Greenlaw, Reynold and Markham, Deborah and Hahne, Anna},
  year = {2016},
  month = dec,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Vienna, Austria}},
  doi = {10/ggwjsv},
  isbn = {978-1-5090-5234-9},
  keywords = {data,data mining,Decision support systems,emergency services,flooding,Twitter},
  file = {/home/cjber/drive/pdf/Spielhofer et al_2016_Data mining Twitter during the UK floods.pdf;/home/cjber/drive/pdf/Spielhofer et al_2016_Data mining Twitter during the UK floods2.pdf;/home/cjber/drive/pdf/Spielhofer et al_2016_Data mining Twitter during the UK floods3.pdf;/home/cjber/drive/zotero/storage/R8BEDMVV/7857213.html}
}

@article{sprague1988,
  title = {Post-{{War Fertility}} and {{Female Labour Force Participation Rates}}},
  author = {Sprague, Alison},
  year = {1988},
  journal = {The Economic Journal},
  volume = {98},
  number = {392},
  pages = {682--700},
  issn = {0013-0133},
  doi = {10/fw3nfp},
  keywords = {\#nosource}
}

@article{statista2016,
  title = {\textbullet{} {{Countries}} with the Highest Birth Rates 2016 | {{Statista}}},
  author = {{Statista}},
  year = {2016},
  mendeley-groups = {ENVS418},
  keywords = {\#nosource}
}

@article{statista2019,
  title = {\textbullet{} {{Life}} Expectancy in {{Africa}} in 2018 | {{Statistic}}},
  author = {{Statista}},
  year = {2019},
  mendeley-groups = {ENVS418},
  keywords = {\#nosource}
}

@misc{statista2021,
  title = {Twitter: Most Users by Country},
  shorttitle = {Twitter},
  author = {{Statista}},
  year = {2021},
  journal = {Statista},
  abstract = {The United States, Japan, and India were the three countries with the most Twitter users as of January 2021.},
  howpublished = {https://www.statista.com/statistics/242606/number-of-active-twitter-users-in-selected-countries/},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/8R9JAJYG/number-of-active-twitter-users-in-selected-countries.html}
}

@article{stedman,
  title = {Sense of {{Place}} and {{Forest Science}}: {{Toward}} a {{Program}} of {{Quantitative Research}}},
  author = {Stedman, Richard C},
  pages = {8},
  abstract = {Sense of place is rich in theory, but quantitative research approaches often fail to reflect this richness. This schism between theory and application not only impedes the development of theory, but also the ultimate utility of the concept for integration into resource management planning. Here, several fundamental points in sense of place theory that can readily be translated into testable hypotheses are identified, as are suggestions for how they may be reformulated into hypothesis language. Sense of place is composed of descriptive and evaluative components that are a function of landscape attributes, experience with the landscape. In turn, any of these elements may affect place-related behaviors. FOR. SCI. 49(6):822\textendash 829.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Stedman_Sense of Place and Forest Science.pdf}
}

@book{stephenson2014,
  ids = {stephenson2014a},
  title = {The {{Python Workbook}}: {{A Brief Introduction}} with {{Exercises}} and {{Solutions}}},
  shorttitle = {The {{Python Workbook}}},
  author = {Stephenson, Ben},
  year = {2014},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-14240-1},
  isbn = {978-3-319-14239-5 978-3-319-14240-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Stephenson_2014_The Python Workbook.pdf;/home/cjber/drive/pdf/Stephenson_2014_The Python Workbook2.pdf;/home/cjber/drive/pdf/Stephenson_2014_The Python Workbook3.pdf}
}

@article{steyvers2000,
  title = {Road-Edge Delineation in Rural Areas: Effects on Driving Behaviour},
  author = {Steyvers, F. J. J. M. and De Waard, D.},
  year = {2000},
  month = feb,
  journal = {Ergonomics},
  volume = {43},
  number = {2},
  pages = {223--238},
  issn = {0014-0139},
  doi = {10/ddvg7m},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000078}
}

@incollection{stock2013,
  title = {Creating a {{Corpus}} of {{Geospatial Natural Language}}},
  booktitle = {Spatial {{Information Theory}}},
  author = {Stock, Kristin and Pasley, Robert C. and Gardner, Zoe and Brindley, Paul and Morley, Jeremy and Cialone, Claudia},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Tenbrink, Thora and Stell, John and Galton, Antony and Wood, Zena},
  year = {2013},
  volume = {8116},
  pages = {279--298},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-01790-7_16},
  abstract = {The description of location using natural language is of interest for a number of research activities including the automated interpretation and generation of natural language to ease interaction with geographic information systems. For such activities, examples of geospatial natural language are usually collected from the personal knowledge of researchers, or in small scale collection activities specific to the project concerned. This paper describes the process used to develop a more generic corpus of geospatial natural language.},
  isbn = {978-3-319-01789-1 978-3-319-01790-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Stock et al_2013_Creating a Corpus of Geospatial Natural Language.pdf}
}

@article{stock2014,
  title = {A {{Geometric Configuration Ontology}} to {{Support Spatial Querying}}},
  author = {Stock, Kristin},
  year = {2014},
  pages = {8},
  abstract = {A number of ontologies of spatial relations have been defined in the literature, but most of these are either confined to a small subset of relations, or focussed on language expressions, and not specified geometrically. This paper presents an ontology of geometric configurations, to reflect and specify the range of spatial relations that have been discussed by previous researchers and that are commonly expressed in natural language, and to provide a sufficiently specific definition of the relations to allow them to be executed as spatial queries. Although this work was motivated by a goal to translate natural language describing location into spatial queries, we anticipate wider applications of the ontology for other purposes.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Stock_2014_A Geometric Configuration Ontology to Support Spatial Querying.pdf}
}

@article{stock2015,
  title = {Discovering {{Order}} in {{Chaos}}: {{Using}} a {{Heuristic Ontology}} to {{Derive Spatio-Temporal Sequences}} for {{Cadastral Data}}},
  shorttitle = {Discovering {{Order}} in {{Chaos}}},
  author = {Stock, Kristin and Leibovici, Didier and Delazari, Luciene and Santos, Roberto},
  year = {2015},
  month = apr,
  journal = {Spatial Cognition \& Computation},
  volume = {15},
  number = {2},
  pages = {115--141},
  issn = {1387-5868, 1542-7633},
  doi = {10/ggwjsz},
  langid = {english},
  file = {/home/cjber/drive/pdf/Stock et al_2015_Discovering Order in Chaos.pdf}
}

@article{stock2018,
  ids = {stock2018a},
  title = {Context-Aware Automated Interpretation of Elaborate Natural Language Descriptions of Location through Learning from Empirical Data},
  author = {Stock, Kristin and Yousaf, Javid},
  year = {2018},
  month = jun,
  journal = {International Journal of Geographical Information Science},
  volume = {32},
  number = {6},
  pages = {1087--1116},
  issn = {1365-8816, 1362-3087},
  doi = {10/ggwjst},
  abstract = {Natural language descriptions of location can be complex, involving many different elements and often describing location by reference to other objects. Descriptions may be vague, and their meaning often depends upon the context within which the description has been expressed. Many previous approaches use mathematical models, focus on prepositions, and have had limited success and application. We present an approach to the interpretation of geospatial natural language expressions that uses a knowledge base of expressions for which human interpretations (in the form of degree of match to one of 50 geometric configurations) are known. Our approach interprets new expressions by finding the most similar knowledge base expression and adopting its meaning. We determine expression similarity using four different methods: element match; linguistic collocation approaches (Cosine); wordnet semantic network distance and a new approach that incorporates the contextual aspects of the expression including scale, geometry type, axial structure, image-schema and liquid/solid. As well as preposition, relatum and locatum, we consider spatial adjectives, adverbs, verb and sub-parts of the relatum and locatum. The method that incorporates context was the most successful of the four tested, selecting the same geometric configuration as human respondents in 69\% of cases.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Stock_Yousaf_2018_Context-aware automated interpretation of elaborate natural language.pdf;/home/cjber/drive/zotero/storage/BX8HVEFD/Stock_Yousaf_2018_Context-aware automated interpretation of elaborate natural language2.pdf}
}

@incollection{stock2018a,
  title = {The {{Role}} of {{Context}} in the {{Interpretation}} of {{Natural Language Location Descriptions}}},
  booktitle = {Proceedings of {{Workshops}} and {{Posters}} at the 13th {{International Conference}} on {{Spatial Information Theory}} ({{COSIT}} 2017)},
  author = {Stock, Kristin and Hall, Mark},
  editor = {Fogliaroni, Paolo and Ballatore, Andrea and Clementini, Eliseo},
  year = {2018},
  pages = {245--254},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-63946-8_40},
  abstract = {Research into methods for automated interpretation of human language descriptions of location has recognized the importance of spatial relations (usually represented by prepositions in English). However, the role of context has been widely acknowledged in natural language processing research and in linguistic studies of spatial language. There are a large number of different aspects of context that may be important in automated interpretation of location descriptions. In this paper, we present a summary of the contextual factors that have been discussed in the literature, and also describe and test a methodology for identifying contextual factors that respondents consider important in the use of specific spatial relations. We combine these sources to present a broad typology of contextual factors in the interpretation of geospatial natural language to set the scene for future research.},
  isbn = {978-3-319-63945-1 978-3-319-63946-8},
  langid = {english},
  file = {/home/cjber/drive/pdf/Stock_Hall_2018_The Role of Context in the Interpretation of Natural Language Location.pdf;/home/cjber/drive/pdf/Stock_Hall_2018_The Role of Context in the Interpretation of Natural Language Location2.pdf}
}

@article{stock2018b,
  title = {Context-Aware Automated Interpretation of Elaborate Natural Language Descriptions of Location through Learning from Empirical Data},
  author = {Stock, Kristin and Yousaf, Javid},
  year = {2018},
  month = jun,
  journal = {International Journal of Geographical Information Science},
  volume = {32},
  number = {6},
  pages = {1087--1116},
  issn = {1365-8816, 1362-3087},
  doi = {10/ggwjst},
  abstract = {Natural language descriptions of location can be complex, involving many different elements and often describing location by reference to other objects. Descriptions may be vague, and their meaning often depends upon the context within which the description has been expressed. Many previous approaches use mathematical models, focus on prepositions, and have had limited success and application. We present an approach to the interpretation of geospatial natural language expressions that uses a knowledge base of expressions for which human interpretations (in the form of degree of match to one of 50 geometric configurations) are known. Our approach interprets new expressions by finding the most similar knowledge base expression and adopting its meaning. We determine expression similarity using four different methods: element match; linguistic collocation approaches (Cosine); wordnet semantic network distance and a new approach that incorporates the contextual aspects of the expression including scale, geometry type, axial structure, image-schema and liquid/solid. As well as preposition, relatum and locatum, we consider spatial adjectives, adverbs, verb and sub-parts of the relatum and locatum. The method that incorporates context was the most successful of the four tested, selecting the same geometric configuration as human respondents in 69\% of cases.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Stock_Yousaf_2018_Context-aware automated interpretation of elaborate natural language2.pdf}
}

@article{stock2021,
  title = {Detecting Geospatial Location Descriptions in Natural Language Text},
  author = {Stock, Kristin and Jones, Christopher B. and Russell, Shaun and Radke, Mansi and Das, Prarthana and Aflaki, Niloofar},
  year = {2021},
  month = dec,
  journal = {International Journal of Geographical Information Science},
  pages = {1--38},
  issn = {1365-8816, 1362-3087},
  doi = {10/gph5wt},
  abstract = {References to geographic locations are common in text data sources including social media and web pages. They take different forms from simple place names to relative expressions that describe location through a spatial relationship to a reference object (e.g. the house beside the Waikato River). Often complex, multi-word phrases are employed (e.g. the road and railway cross at right angles; the road in line with the canal) where spatial relationships are commu\- nicated with various parts of speech including prepositions, verbs, adverbs and adjectives. We address the problem of automatically detecting relative geospatial location descriptions, which we define as those that include spatial relation terms referencing geographic objects, and distinguishing them from non-geographical descrip\- tions of location (e.g. the book on the table). We experiment with several methods for automated classification of text expressions, using features for machine learning that include bag of words that detect distinctive words, word embeddings that encode meanings of words and manually identified language patterns that character\- ise geospatial expressions. Using three data sets created for this study, we find that ensemble and meta-classifier approaches, that variously combine predictions from several other classifiers with data features, provide the best F-measure of 0.90 for detecting geospatial expressions.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Stock et al_2021_Detecting geospatial location descriptions in natural language text.pdf}
}

@article{stokes2008,
  title = {An Empirical Study of the Effects of {{NLP}} Components on {{Geographic IR}} Performance},
  author = {Stokes, Nicola and Li, Yi and Moffat, Alistair and Rong, Jiawen},
  year = {2008},
  month = mar,
  journal = {International Journal of Geographical Information Science},
  volume = {22},
  number = {3},
  pages = {247--264},
  issn = {1365-8816, 1362-3087},
  doi = {10/dbp4w6},
  abstract = {Natural Language Processing (NLP) techniques, such as toponym detection and resolution, are an integral part of most Geographic Information Retrieval (GIR) architectures. Without these components, synonym detection, ambiguity resolution and accurate toponym expansion would not be possible. However, there are many important factors affecting the success of an NLP approach to GIR, including toponym detection errors, toponym resolution errors, and query overloading. The aim of this paper is to determine how severe these errors are in state-of-the-art systems, and to what extent they affect GIR performance. We show that a careful choice of weighting schemes in the IR engine can minimize the negative impact of these errors on GIR accuracy. We provide empirical evidence from the GeoCLEF 2005 and 2006 datasets to support our observations.},
  langid = {english},
  keywords = {Key Paper},
  file = {/home/cjber/drive/pdf/Stokes et al_2008_An empirical study of the effects of NLP components on Geographic IR performance.pdf}
}

@book{stradling2008,
  title = {Understanding Inappropriate High Speed: A Quantitative Analysis},
  shorttitle = {Understanding Inappropriate High Speed},
  author = {Stradling, Stephen G and {Great Britain} and {Department for Transport}},
  year = {2008},
  publisher = {{Dept. for Transport}},
  address = {{London}},
  isbn = {978-1-906581-37-4},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: 277067852},
  file = {/home/cjber/drive/pdf/ENVS492/Stradling et al_2008_.pdf}
}

@article{sugumaran2010,
  title = {Spatial {{Decision Support Systems}}},
  author = {Sugumaran, Ramanathan and Degroote, John},
  year = {2010},
  pages = {120},
  doi = {10/c68xrt},
  langid = {english},
  keywords = {SDS},
  file = {/home/cjber/drive/pdf/Sugumaran_Degroote_2010_Spatial Decision Support Systems.pdf}
}

@article{sullivan2006,
  title = {{{COLLABORATIVE CAPACITY AND STRATEGIES IN AREA-BASED INITIATIVES}}},
  author = {Sullivan, Helen and Barnes, Marian and Matka, Elizabeth},
  year = {2006},
  month = jun,
  journal = {Public Administration},
  volume = {84},
  number = {2},
  pages = {289--310},
  issn = {0033-3298, 1467-9299},
  doi = {10/bc6bxj},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Sullivan et al_2006_.pdf}
}

@book{sullivan2017,
  title = {Working across Boundaries: Collaboration in Public Services},
  author = {Sullivan, Helen and Skelcher, Chris},
  year = {2017},
  publisher = {{Macmillan International Higher Education}},
  isbn = {1-4039-4010-X},
  keywords = {\#nosource}
}

@article{sun2019,
  title = {A {{Semantic Expansion Model}} for {{VGI Retrieval}}},
  author = {{Sun} and {Xia} and {Li} and {Shen} and {Liu}},
  year = {2019},
  month = dec,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {8},
  number = {12},
  pages = {589},
  issn = {2220-9964},
  doi = {10/ggwjsp},
  abstract = {OpenStreetMap (OSM) is a representative volunteered geographic information (VGI) project. However, there have been difficulties in retrieving spatial information from OSM. Ontology is an effective knowledge organization and representation method that is often used to enrich the search capabilities of search systems. This paper constructed an OSM ontology model with semantic property items. A query expansion method is also proposed based on the similarity of properties of the ontology model. Moreover, a relevant experiment is conducted using OSM data related to China. The experimental results demonstrate that the recall and precision of the proposed method reach 80\% and 87\% for geographic information retrieval, respectively. This study provides a method that can be used as a reference for subsequent research on spatial information retrieval.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Sun et al_2019_A Semantic Expansion Model for VGI Retrieval.pdf}
}

@article{sun2019a,
  ids = {sun2019b},
  title = {Geospatial Data Ontology: The Semantic Foundation of Geospatial Data Integration and Sharing},
  shorttitle = {Geospatial Data Ontology},
  author = {Sun, Kai and Zhu, Yunqiang and Pan, Peng and Hou, Zhiwei and Wang, Dongxu and Li, Weirong and Song, Jia},
  year = {2019},
  month = jul,
  journal = {Big Earth Data},
  volume = {3},
  number = {3},
  pages = {269--296},
  issn = {2096-4471, 2574-5417},
  doi = {10/ggwjtd},
  abstract = {Effective integration and wide sharing of geospatial data is an important and basic premise to facilitate the research and applications of geographic information science. However, the semantic heterogeneity of geospatial data is a major problem that significantly hinders geospatial data integration and sharing. Ontologies are regarded as a promising way to solve semantic problems by providing a formalized representation of geographic entities and relationships between them in a manner understandable to machines. Thus, many efforts have been made to explore ontology-based geospatial data integration and sharing. However, there is a lack of a specialized ontology that would provide a unified description for geospatial data. In this paper, with a focus on the characteristics of geospatial data, we propose a unified framework for geospatial data ontology, denoted GeoDataOnt, to establish a semantic foundation for geospatial data integration and sharing. First, we provide a characteristics hierarchy of geospatial data. Next, we analyze the semantic problems for each characteristic of geospatial data. Subsequently, we propose the general framework of GeoDataOnt, targeting these problems according to the characteristics of geospatial data. GeoDataOnt is then divided into multiple modules, and we show a detailed design and implementation for each module. Key limitations and challenges of GeoDataOnt are identified, and broad applications of GeoDataOnt are discussed.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Sun et al_2019_Geospatial data ontology.pdf;/home/cjber/drive/pdf/Sun et al_2019_Geospatial data ontology2.pdf}
}

@article{sun2020,
  title = {How to {{Fine-Tune BERT}} for {{Text Classification}}?},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  year = {2020},
  month = feb,
  journal = {arXiv:1905.05583 [cs]},
  eprint = {1905.05583},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Sun et al_2020_How to Fine-Tune BERT for Text Classification.pdf}
}

@article{sun2020a,
  title = {Aligning Geographic Entities from Historical Maps for Building Knowledge Graphs},
  author = {Sun, Kai and Hu, Yingjie and Song, Jia and Zhu, Yunqiang},
  year = {2020},
  month = nov,
  journal = {International Journal of Geographical Information Science},
  pages = {1--30},
  issn = {1365-8816, 1362-3087},
  doi = {10/ghp65p},
  abstract = {Historical maps contain rich geographic information about the past of a region. They are sometimes the only source of information before the availability of digital maps. Despite their valuable content, it is often challenging to access and use the information in historical maps, due to their forms of paper-based maps or scanned images. It is even more time-consuming and labor-intensive to conduct an analysis that requires a synthesis of the information from multiple historical maps. To facilitate the use of the geographic information contained in histor\- ical maps, one way is to build a geographic knowledge graph (GKG) from them. This paper proposes a general workflow for completing one important step of building such a GKG, namely aligning the same geographic entities from different maps. We present this workflow and the related methods for implementation, and systematically evaluate their performances using two different datasets of historical maps. The evaluation results show that machine learning and deep learning models for matching place names are sensitive to the thresholds learned from the training data, and a combination of measures based on string similarity, spatial distance, and approximate topologi\- cal relation achieves the best performance with an average F-score of 0.89.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Sun et al_2020_Aligning geographic entities from historical maps for building knowledge graphs.pdf;/home/cjber/drive/pdf/Sun et al_2020_Aligning geographic entities from historical maps for building knowledge graphs2.pdf}
}

@article{sundararajan2017,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.01365 [cs]},
  eprint = {1703.01365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms\textemdash Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/Sundararajan et al_2017_Axiomatic Attribution for Deep Networks.pdf}
}

@incollection{suwaileh2020,
  title = {Time-{{Critical Geolocation}} for {{Social Good}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Suwaileh, Reem},
  editor = {Jose, Joemon M. and Yilmaz, Emine and Magalh{\~a}es, Jo{\~a}o and Castells, Pablo and Ferro, Nicola and Silva, M{\'a}rio J. and Martins, Fl{\'a}vio},
  year = {2020},
  volume = {12036},
  pages = {624--629},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-45442-5_82},
  abstract = {Twitter has become an instrumental source of news in emergencies where efficient access, dissemination of information, and immediate reactions are critical. Nevertheless, due to several challenges, the current fully-automated processing methods are not yet mature enough for deployment in real scenarios. In this dissertation, I focus on tackling the lack of context problem by studying automatic geo-location techniques. I specifically aim to study the Location Mention Prediction problem in which the system has to extract location mentions in tweets and pin them on the map. To address this problem, I aim to exploit different techniques such as training neural models, enriching the tweet representation, and studying methods to mitigate the lack of labeled data. I anticipate many downstream applications for the Location Mention Prediction problem such as incident detection, real-time action management during emergencies, and fake news and rumor detection among others.},
  isbn = {978-3-030-45441-8 978-3-030-45442-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Suwaileh_2020_Time-Critical Geolocation for Social Good.pdf}
}

@article{sweeney2002,
  title = {K-Anonymity: {{A}} Model for Protecting Privacy},
  author = {Sweeney, Latanya},
  year = {2002},
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume = {10},
  number = {05},
  pages = {557--570},
  publisher = {{World Scientific}},
  issn = {0218-4885},
  keywords = {❓ Multiple DOI}
}

@book{szeliski2011,
  title = {Computer {{Vision}}},
  author = {Szeliski, Richard},
  year = {2011},
  series = {Texts in {{Computer Science}}},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-84882-935-0},
  isbn = {978-1-84882-934-3 978-1-84882-935-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Szeliski_2011_Computer Vision.pdf}
}

@article{tahsin2016,
  title = {A High-Precision Rule-Based Extraction System for Expanding Geospatial Metadata in {{GenBank}} Records},
  author = {Tahsin, Tasnia and Weissenbacher, Davy and Rivera, Robert and Beard, Rachel and Firago, Mari and Wallstrom, Garrick and Scotch, Matthew and Gonzalez, Graciela},
  year = {2016},
  month = sep,
  journal = {Journal of the American Medical Informatics Association},
  volume = {23},
  number = {5},
  pages = {934--941},
  issn = {1067-5027, 1527-974X},
  doi = {10/f84r5x},
  abstract = {Objective The metadata reflecting the location of the infected host (LOIH) of virus sequences in GenBank often lacks specificity. This work seeks to enhance this metadata by extracting more specific geographic information from related full-text articles and mapping them to their latitude/ longitudes using knowledge derived from external geographical databases. Materials and Methods We developed a rule-based information extraction framework for linking GenBank records to the latitude/longitudes of the LOIH. Our system first extracts existing geospatial metadata from GenBank records and attempts to improve it by seeking additional, relevant geographic information from text and tables in related full-text PubMed Central articles. The final extracted locations of the records, based on data assimilated from these sources, are then disambiguated and mapped to their respective geo-coordinates. We evaluated our approach on a manually annotated dataset comprising of 5728 GenBank records for the influenza A virus. Results We found the precision, recall, and f-measure of our system for linking GenBank records to the latitude/longitudes of their LOIH to be 0.832, 0.967, and 0.894, respectively. Discussion Our system had a high level of accuracy for linking GenBank records to the geo-coordinates of the LOIH. However, it can be further improved by expanding our database of geospatial data, incorporating spell correction, and enhancing the rules used for extraction. Conclusion Our system performs reasonably well for linking GenBank records for the influenza A virus to the geo-coordinates of their LOIH based on record metadata and information extracted from related full-text articles. ....................................................................................................................................................},
  langid = {english},
  file = {/home/cjber/drive/pdf/Tahsin et al_2016_A high-precision rule-based extraction system for expanding geospatial metadata.pdf}
}

@book{tateosian2015,
  title = {Python {{For ArcGIS}}},
  author = {Tateosian, Laura},
  year = {2015},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-18398-5},
  isbn = {978-3-319-18397-8 978-3-319-18398-5},
  langid = {english},
  file = {/home/cjber/drive/pdf/Tateosian_2015_Python For ArcGIS.pdf;/home/cjber/drive/pdf/Tateosian_2015_Python For ArcGIS2.pdf}
}

@article{taylor2000,
  title = {The Effects of Drivers' Speed on the Frequency of Road Accidents},
  author = {Taylor, M C and Lynam, D A and Baruya, A},
  year = {2000},
  pages = {56},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000002},
  file = {/home/cjber/drive/pdf/ENVS492/Taylor et al_2000_.pdf}
}

@article{taylor2002,
  title = {The Relationship between Speed and Accidents on Rural Single-Carriageway Roads},
  author = {Taylor, M C and Baruya, A and Kennedy, J V},
  year = {2002},
  pages = {32},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000125},
  file = {/home/cjber/drive/pdf/ENVS492/Taylor et al_2002_.pdf}
}

@techreport{taylor2017,
  type = {Preprint},
  title = {Forecasting at Scale},
  author = {Taylor, Sean J and Letham, Benjamin},
  year = {2017},
  month = sep,
  institution = {{PeerJ Preprints}},
  doi = {10.7287/peerj.preprints.3190v2},
  abstract = {Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high quality forecasts \textendash especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting ``at scale'' that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Taylor_Letham_2017_Forecasting at scale.pdf}
}

@article{tear2020,
  title = {Geotagging {{Matters}}? {{The Interplay}} of {{Space}} and {{Place}} in {{Politicized Online Social Media Networks}}},
  shorttitle = {Geotagging {{Matters}}?},
  author = {Tear, Adrian},
  year = {2020},
  month = jan,
  publisher = {{Zenodo}},
  doi = {10/gg24hk},
  abstract = {Voting results in marginal constituencies often determine wider political outcomes. It is now apparent that key electorates in these areas have been geo-behaviourally targeted by elaborate operations intended to manipulate results through advertising, (mis)information, and/or `fake news' disseminated via online social networks. Attempts to track the geographical diffusion of cyber politicking are hindered by incomplete geospatial referencing in social media (meta)data. Just about 1\textendash 2\% of publicly posted Twitter tweets, and even fewer Facebook posts, are typically `geotagged' with Latitude and Longitude coordinates. Many more records (about 25\%) make toponymic mention of place. This paper examines about 8 million social media interactions, over 350,000 of which are geotagged, created during the 2012 US Presidential Election and the 2014 Scottish Independence Referendum campaigns, to assess the interplay of space and place in online communications. Results of text and data-mining show that coordinate-geotagging users of Twitter and Facebook, (a) make fewer references to place in their message text, (b) link to articles making fewer mentions of place in their content, and (c) make far fewer links to external content than their non-coordinate-geotagging peers. Despite providing some valuable geospatial information, coordinate-geotagged interactions offer only an inadequate proxy for tracking the spread of all places, linked content, or (mis)information shared online. As Twitter retires its tweet spatialization functionality, new regulatory and technical responses together with a better understanding of place will be required if electoral officials, platform operators, and researchers are to more easily and accurately identify nefarious content targeting specific areas as well as specific individuals during democratic elections.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  langid = {english},
  keywords = {place},
  file = {/home/cjber/drive/pdf/Tear_2020_Geotagging Matters.pdf}
}

@book{tenbrink2013,
  title = {Spatial {{Information Theory}}},
  editor = {Tenbrink, Thora and Stell, John and Galton, Antony and Wood, Zena and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {8116},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-01790-7},
  isbn = {978-3-319-01789-1 978-3-319-01790-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Tenbrink et al_2013_Spatial Information Theory.pdf;/home/cjber/drive/pdf/Tenbrink et al_2013_Spatial Information Theory2.pdf}
}

@article{thakor2015,
  title = {Ontology-Based {{Sentiment Analysis Process}} for {{Social Media Content}}},
  author = {Thakor, Pratik and Sasi, Sreela},
  year = {2015},
  journal = {Procedia Computer Science},
  volume = {53},
  pages = {199--207},
  issn = {18770509},
  doi = {10/ggt528},
  abstract = {Social media provides a platform where users share an abundance of information on anything and everything. The information may consist of users' emotions, feedbacks, reviews, and personal experiences. In this research a novel Ontology-based Sentiment Analysis Process for Social Media content (OSAPS) with negative sentiments is presented. The social media content is automatically extracted from the twitter messages. An ontology-based process is designed to retrieve and analyse the customers' tweet with negative sentiments. This idea is demonstrated with the identification of customer dissatisfaction of the delivery service issues of the United States Postal Service, Royal Mail of United Kingdom, and Canada post. The tweets related to the delivery service include delay in delivery, lost package/s or improper customer services at the office in person or at call centres. A combination of technologies for twitter extraction, data cleaning, subjective analysis, ontology model building, and sentiment analysis are used. The results from this analysis could be used by the company to take corrective measures for the problems as well as to generate an automated online reply for the issues. A rulebased classifier could be used for generating the automated online replies.},
  langid = {english},
  keywords = {Ontology},
  file = {/home/cjber/drive/pdf/Thakor_Sasi_2015_Ontology-based Sentiment Analysis Process for Social Media Content.pdf}
}

@misc{theuniversityofedinburgh2019,
  title = {Aerial {{Digimap Educational User Licence}}},
  author = {{The University of Edinburgh}},
  year = {2019},
  howpublished = {https://digimap.edina.ac.uk/},
  annotation = {ZSCC: 0000000[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/The University of Edinborough_2019_.pdf}
}

@article{thomson2006,
  title = {Do Urban Regeneration Programmes Improve Public Health and Reduce Health Inequalities? {{A}} Synthesis of the Evidence from {{UK}} Policy and Practice (1980-2004)},
  shorttitle = {Do Urban Regeneration Programmes Improve Public Health and Reduce Health Inequalities?},
  author = {Thomson, H.},
  year = {2006},
  month = feb,
  journal = {Journal of Epidemiology \& Community Health},
  volume = {60},
  number = {2},
  pages = {108--115},
  issn = {0143-005X},
  doi = {10/cgdmtn},
  abstract = {Objectives: To synthesise data on the impact on health and key socioeconomic determinants of health and health inequalities reported in evaluations of national UK regeneration programmes. Data Sources: Eight electronic databases were searched from 1980 to 2004 (IBSS, COPAC, HMIC, IDOX, INSIDE, Medline, Urbadisc/Accompline, Web of Knowledge). Bibliographies of located documents and relevant web sites were searched. Experts and government departmental libraries were also contacted. Review methods: Evaluations that reported achievements drawing on data from at least two target areas of a national urban regeneration programme in the UK were included. Process evaluations and evaluations reporting only business outcomes were excluded. All methods of evaluation were included. Impact data on direct health outcomes and direct measures of socioeconomic determinants of health were narratively synthesised. Results: 19 evaluations reported impacts on health or socioeconomic determinants of health; data from 10 evaluations were synthesised. Three evaluations reported health impacts; in one evaluation three of four measures of self reported health deteriorated, typically by around 4\%. Two other evaluations reported overall reductions in mortality rates. Most socioeconomic outcomes assessed showed an overall improvement after regeneration investment; however, the effect size was often similar to national trends. In addition, some evaluations reported adverse impacts. Conclusion: There is little evidence of the impact of national urban regeneration investment on socioeconomic or health outcomes. Where impacts have been assessed, these are often small and positive but adverse impacts have also occurred. Impact data from future evaluations are required to inform healthy public policy; in the meantime work to exploit and synthesise ``best available'' data is required.},
  langid = {english},
  annotation = {ZSCC: 0000179},
  file = {/home/cjber/drive/pdf/ENVS416/Thomson_2006_.pdf}
}

@article{thomson2008,
  title = {A Dose of Realism for Healthy Urban Policy: Lessons from Area-Based Initiatives in the {{UK}}},
  shorttitle = {A Dose of Realism for Healthy Urban Policy},
  author = {Thomson, H},
  year = {2008},
  month = oct,
  journal = {Journal of Epidemiology \& Community Health},
  volume = {62},
  number = {10},
  pages = {932--936},
  issn = {0143-005X},
  doi = {10/dx6698},
  abstract = {Many urban policies aim to improve areas and address socioeconomic deprivation. The resulting investment is often delivered through area-based programmes which incorporate initiatives to improve the physical, social and economic environment. Hypotheses that these investments can contribute to wider public health strategies are based on epidemiological data and used to support the concept of healthy urban policy. However, there is little evidence on their ability to generate positive impacts on socioeconomic or health outcomes. The lack of validating evidence on actual impacts raises two important questions: (1) Is areabased investment an effective strategy to tackle socioeconomic deprivation? (2) What is the prospect for new and improved evaluations to provide stronger evidence? Both the programmes of area investment and their accompanying evaluations have been criticised for being overly ambitious in what can be achieved by the investment and what can be measured by an evaluation. Area-based approaches to tackling deprivation have their advantages but a mix of area and individual-level targeting is likely to be needed. While there is scope to improve the utility of evaluation data there are also inevitable constraints on assessing and attributing impacts from urban investment. The inherent limitations to an area-based approach and the ongoing constraints on impact evaluation will inevitably temper expectations of what healthy urban policy can achieve. However, lack of evidence is not grounds to abandon the concept of healthy urban policy; adoption of more realistic expectations together with improved evaluation data may help to increase its credibility.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Thomson_2008_.pdf}
}

@article{tibshirani,
  title = {Valerie and {{Patrick Hastie}}},
  author = {Tibshirani, Sami and Friedman, Harry},
  pages = {764},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Tibshirani_Friedman_Valerie and Patrick Hastie.pdf}
}

@inproceedings{tjongkimsang2003,
  title = {Introduction to the {{CoNLL-2003 Shared Task}}: {{Language-Independent Named Entity Recognition}}},
  shorttitle = {Introduction to the {{CoNLL-2003 Shared Task}}},
  booktitle = {Proceedings of the {{Seventh Conference}} on {{Natural Language Learning}} at {{HLT-NAACL}} 2003},
  author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
  year = {2003},
  pages = {142--147},
  doi = {10/d8qpkd},
  file = {/home/cjber/drive/pdf/Tjong Kim Sang_De Meulder_2003_Introduction to the CoNLL-2003 Shared Task.pdf}
}

@inproceedings{tobin2010,
  title = {Evaluation of Georeferencing},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Geographic Information Retrieval}} - {{GIR}} '10},
  author = {Tobin, Richard and Grover, Claire and Byrne, Kate and Reid, James and Walsh, Jo},
  year = {2010},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Zurich, Switzerland}},
  doi = {10/b3s3pg},
  abstract = {In this paper we describe a georeferencing system which first uses Information Extraction techniques to identify place names in textual documents and which then resolves the place names against a choice of gazetteers. We have used the system to georeference three digitised historical collections and have evaluated its performance against human annotated gold standard samples from the three collections. We have also evaluated its performance on the SpatialML corpus which is a geo-annotated corpus of newspaper text. The main focus of this paper is the evaluation of georesolution and we discuss evaluation methods and issues arising from the evaluation.},
  isbn = {978-1-60558-826-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Tobin et al_2010_Evaluation of georeferencing.pdf}
}

@book{townsend1979,
  title = {Poverty in the {{United Kingdom}}: A Survey of Household Resources and Standards of Living},
  author = {Townsend, Peter},
  year = {1979},
  publisher = {{Univ of California Press}},
  isbn = {0-520-03976-9},
  file = {/home/cjber/drive/pdf/ENVS416/townsend1979.pdf}
}

@book{townsend1988,
  title = {Health and Deprivation: Inequality and the {{North}}},
  author = {Townsend, Peter and Phillimore, Peter and Beattie, Alastair},
  year = {1988},
  publisher = {{Routledge}},
  isbn = {0-7099-4351-2},
  keywords = {\#nosource}
}

@misc{tracc2019,
  title = {{{TRACC}}},
  author = {{TRACC}},
  year = {2019},
  journal = {Basemap},
  abstract = {TRACC is the leading multi-modal transport accessibility tool. Quickly and accurately calculate travel time. Create contour maps and reports.},
  howpublished = {https://www.basemap.co.uk/tracc/},
  langid = {british},
  file = {/home/cjber/drive/zotero/storage/J87F8WPC/tracc.html}
}

@book{transportresearchcentre2008,
  title = {Towards Zero: Ambitious Road Safety Targets and the Safe System Approach},
  shorttitle = {Towards Zero},
  editor = {{Transport Research Centre}},
  year = {2008},
  publisher = {{OECD}},
  address = {{Paris}},
  isbn = {978-92-821-0195-7},
  langid = {english},
  lccn = {HE5614 .T683 2008},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: ocn276890093},
  file = {/home/cjber/drive/pdf/ENVS492/Transport Research Centre_2008_.pdf}
}

@article{treveil,
  title = {Introducing {{MLOps}}},
  author = {Treveil, Mark},
  pages = {185},
  langid = {english},
  file = {/home/cjber/drive/pdf/Treveil_Introducing MLOps.pdf}
}

@book{tuan1977,
  title = {Space and Place: {{The}} Perspective of Experience},
  author = {Tuan, Yi-Fu},
  year = {1977},
  publisher = {{U of Minnesota Press}},
  isbn = {1-4529-0553-3}
}

@article{turok1992,
  title = {Property-{{Led Urban Regeneration}}: {{Panacea}} or {{Placebo}}?},
  shorttitle = {Property-{{Led Urban Regeneration}}},
  author = {Turok, I},
  year = {1992},
  month = mar,
  journal = {Environment and Planning A: Economy and Space},
  volume = {24},
  number = {3},
  pages = {361--379},
  issn = {0308-518X, 1472-3409},
  doi = {10/ctd674},
  abstract = {In recent years urban policy has come to rely increasingly on private-sector property development to provide the driving force. Popular opinion is sharply divided about the value of this approach. In this paper, an examination is made of five ways in which property could contribute to urban economic regeneration: through the direct employment effects of construction-related activity; by accommodating the expansion of indigenous firms; by attracting inward investment; by revitalising run-down neighbourhoods; and by initiating areawide economic restructuring. Appropriate property development can have positive economic effects but it has to be part of a more holistic approach that embodies concerns for people living in deprived areas and for the underlying condition of the local economy. Unrestrained market-led development may have detrimental consequences for the economic fabric of cities and for the quality of life of their residents.},
  langid = {english},
  annotation = {ZSCC: 0000335},
  file = {/home/cjber/drive/pdf/ENVS416/turok1992.pdf}
}

@inproceedings{twaroch2008,
  title = {Acquisition of a Vernacular Gazetteer from Web Sources},
  booktitle = {Proceedings of the First International Workshop on {{Location}} and the Web  - {{LOCWEB}} '08},
  author = {Twaroch, Florian A. and Jones, Christopher B. and Abdelmoty, Alia I.},
  year = {2008},
  pages = {61--64},
  publisher = {{ACM Press}},
  address = {{Beijing, China}},
  doi = {10.1145/1367798.1367808},
  abstract = {Vernacular place names are names that are commonly in use to refer to geographical places. For purposes of effective information retrieval, the spatial extent associated with these names should be able to reflect people's perception of the place, even though this may differ sometimes from the administrative definition of the same place name. Due to their informal nature, vernacular place names are hard to capture, but methods to acquire and define vernacular place names are of great benefit to search engines and all kind of information services that deal with geographic data. This paper discusses the acquisition of vernacular use of place names from web sources and their representation as surface models derived by kernel density estimators.},
  isbn = {978-1-60558-160-6},
  langid = {english},
  keywords = {Vernacular},
  file = {/home/cjber/drive/pdf/Twaroch et al_2008_Acquisition of a vernacular gazetteer from web sources.pdf}
}

@inproceedings{twaroch2010,
  title = {A Web Platform for the Evaluation of Vernacular Place Names in Automatically Constructed Gazetteers},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{Geographic Information Retrieval}} - {{GIR}} '10},
  author = {Twaroch, Florian A. and Jones, Christopher B.},
  year = {2010},
  pages = {1},
  publisher = {{ACM Press}},
  address = {{Zurich, Switzerland}},
  doi = {10/fstrvs},
  abstract = {Vernacular place names pose a research challenge in geographic information retrieval. There is a long standing demand from investigators for a reference collection to train their methods and evaluate their models and data. However no large collection of informal place names associated with type and footprint data is currently available to the GIR community. The present contribution discusses the implementation of a web platform to collect such an evaluation data set. Design considerations of the user interface are addressed and we present first results of a nationwide attempt to collect the vernacular place names of Great Britain. Our result will aid further research in automatic gazetteer construction, considering vernacular place names.},
  isbn = {978-1-60558-826-1},
  langid = {english},
  file = {/home/cjber/drive/pdf/Twaroch_Jones_2010_A web platform for the evaluation of vernacular place names in automatically.pdf;/home/cjber/drive/pdf/Twaroch_Jones_2010_A web platform for the evaluation of vernacular place names in automatically2.pdf}
}

@article{twaroch2019,
  title = {Investigating Behavioural and Computational Approaches for Defining Imprecise Regions},
  author = {Twaroch, Florian A. and Brindley, Paul and Clough, Paul D. and Jones, Christopher B. and Pasley, Robert C. and Mansbridge, Sue},
  year = {2019},
  month = apr,
  journal = {Spatial Cognition \& Computation},
  volume = {19},
  number = {2},
  pages = {146--171},
  issn = {1387-5868, 1542-7633},
  doi = {10/gg2cm4},
  abstract = {Centre from a street survey (with 61 participants) to spatial extents derived from various web-based sources. Such automated approaches have advantages of speed, cost and repeatability. Our results show that footprints derived from web sources are often in concordance with models derived from more labourintensive methods. There were, however, differences between some of the data sources, with those advertising/selling residential property diverging the most from the street survey data. Agreement between sources was measured by aggregating the web sources to identify locations of consensus.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Twaroch et al_2019_Investigating behavioural and computational approaches for defining imprecise.pdf}
}

@misc{ukgovernment1981,
  title = {Wildlife and {{Countryside Act}} 1981},
  author = {{UK Government}},
  year = {1981},
  keywords = {\#nosource}
}

@misc{ukgovernment1997,
  title = {The {{Hedgerow Regulations}} 1997},
  author = {{UK Government}},
  year = {1997},
  keywords = {\#nosource}
}

@misc{ukgovernment2011,
  title = {Rural {{Urban Classification}}},
  author = {{UK Government}},
  year = {2011},
  journal = {GOV.UK},
  abstract = {Rural areas as defined by the Rural Urban Classification.},
  howpublished = {https://www.gov.uk/government/collections/rural-urban-classification},
  langid = {english},
  annotation = {ZSCC: 0000347[s0]},
  file = {/home/cjber/drive/zotero/storage/59DBGB2T/rural-urban-classification.html}
}

@misc{ukgovernment2019,
  title = {Speed Limits},
  author = {{UK Government}},
  year = {2019},
  journal = {GOV.UK},
  howpublished = {https://www.gov.uk/speed-limits},
  langid = {english},
  annotation = {ZSCC: 0000773[s0]},
  file = {/home/cjber/drive/zotero/storage/99JD6HXW/speed-limits.html;/home/cjber/drive/zotero/storage/K2JL8YMP/speed-limits.html}
}

@misc{ukgovernment2019a,
  title = {Find Open Data - Data.Gov.Uk},
  author = {{UK Government}},
  year = {2019},
  howpublished = {https://data.gov.uk/},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/FMIFCB92/data.gov.uk.html}
}

@misc{ukgovernment2021,
  title = {{{UK COVID-19}} Vaccine Uptake Plan},
  author = {{UK Government}},
  year = {2021},
  journal = {GOV.UK},
  howpublished = {https://www.gov.uk/government/publications/covid-19-vaccination-uptake-plan/uk-covid-19-vaccine-uptake-plan},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/2YMYXPV2/uk-covid-19-vaccine-uptake-plan.html}
}

@article{umihara2013,
  title = {Emergent {{Use}} of {{Twitter}} in the 2011 {{Tohoku Earthquake}}},
  author = {Umihara, Junko and Nishikitani, Mariko},
  year = {2013},
  month = oct,
  journal = {Prehospital and Disaster Medicine},
  volume = {28},
  number = {5},
  pages = {434--440},
  issn = {1049-023X, 1945-1938},
  doi = {10/f5jv86},
  abstract = {Introduction: Social networks play an important role in disaster situations as they have become a new form of social convergence that provides collective information. The effect of social media on people who experienced disaster should be assessed. Hypothesis: In this study, Twitter communication during the Great East Japan Earthquake of March 11, 2011 was assessed. The hypothesis of this study was that usage of Twitter had psychological effects on victims of the disaster. Methods: A cross-sectional questionnaire survey was carried out in cooperation with a major Japanese newspaper three months after the disaster, and 1,144 volunteer participants responded. They were asked about their health, area of residence, property damage they had experienced, information sources they used at the time of the disaster, and their usage of Twitter. Further, the Twitter users were divided into two groups\textemdash with and without disaster experience. Their psychological effects relating to feelings of relief, stress or anxiety that they experienced in using Twitter were compared between two groups, and Twitter's psychological risk of disaster experience was estimated as an odds ratio. Results: Twitter users in this study tended to reside in disaster-affected areas and thought Twitter was a credible information source during the time of the disaster. The psychological effect of Twitter differed based on participants' disaster experience and gender. Females with disaster experience reported more feelings of relief and stress as a result of using Twitter compared to females who did not experience the disaster. On the other hand, males with disaster experience only reported more stress experiences as a result of using Twitter compared to those without disaster experience. Conclusion: Twitter users with disaster experience had a higher usage of Twitter than those without disaster experience. Social media might have had a material psychological influence on people who experienced disaster, and the effect differed by gender. Regardless of gender, negative feelings were transmitted easily among people who experienced the disaster. It was anticipated that the application of Twitter in a disaster situation will be expanded further by taking these findings into consideration.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Umihara_Nishikitani_2013_Emergent Use of Twitter in the 2011 Tohoku Earthquake.pdf}
}

@article{unankard2015,
  title = {Emerging Event Detection in Social Networks with Location Sensitivity},
  author = {Unankard, Sayan and Li, Xue and Sharaf, Mohamed A.},
  year = {2015},
  month = sep,
  journal = {World Wide Web},
  volume = {18},
  number = {5},
  pages = {1393--1417},
  issn = {1386-145X, 1573-1413},
  doi = {10/ggwjs2},
  abstract = {With the increasing number of real-world events that are originated and discussed over social networks, event detection is becoming a compelling research issue. However, the traditional approaches to event detection on large text streams are not designed to deal with a large number of short and noisy messages. This paper proposes an approach for the early detection of emerging hotspot events in social networks with location sensitivity. We consider the message-mentioned locations for identifying the locations of events. In our approach, we identify strong correlations between user locations and event locations in detecting the emerging events. We evaluate our approach based on a real-world Twitter dataset. Our experiments show that the proposed approach can effectively detect emerging events with respect to user locations that have different granularities.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Unankard et al_2015_Emerging event detection in social networks with location sensitivity.pdf}
}

@article{unitednations2016,
  title = {Dying from Lack of Medicines | {{Africa Renewal}}},
  author = {{United Nations}},
  year = {2016},
  mendeley-groups = {ENVS418},
  keywords = {\#nosource}
}

@article{uschold1996,
  title = {Ontologies: Principles, Methods and Applications},
  shorttitle = {Ontologies},
  author = {Uschold, Mike and Gruninger, Michael},
  year = {1996},
  month = jun,
  journal = {The Knowledge Engineering Review},
  volume = {11},
  number = {2},
  pages = {93--136},
  issn = {0269-8889, 1469-8005},
  doi = {10/bvcsft},
  abstract = {Abstract             This paper is intended to serve as a comprehensive introduction to the emerging field concerned with the design and use of ontologies. We observe that disparate backgrounds, languages, tools and techniques are a major barrier to effective communication among people, organisations and/or software understanding (i.e. an ``ontology'') in a given subject area, can improve such communication, which in turn, can give rise to greater reuse and sharing, inter-operability, and more reliable software. After motivating their need, we clarify just what ontologies are and what purpose they serve. We outline a methodology for developing and evaluating ontologies, first discussing informal techniques, concerning such issues as scoping, handling ambiguity, reaching agreement and producing definitions. We then consider the benefits and describe, a more formal approach. We re-visit the scoping phase, and discuss the role of formal languages and techniques in the specification, implementation and evalution of ontologies. Finally, we review the state of the art and practice in this emerging field, considering various case studies, software tools for ontology development, key research issues and future prospects.},
  langid = {english},
  keywords = {Ontology},
  file = {/home/cjber/drive/pdf/Uschold_Gruninger_1996_Ontologies.pdf}
}

@manual{ushey2020,
  type = {Manual},
  title = {Reticulate: {{Interface}} to 'Python'},
  author = {Ushey, Kevin and Allaire, JJ and Tang, Yuan},
  year = {2020}
}

@book{vanderbei2014,
  title = {Linear {{Programming}}: {{Foundations}} and {{Extensions}}},
  shorttitle = {Linear {{Programming}}},
  author = {Vanderbei, Robert J.},
  year = {2014},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  volume = {196},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4614-7630-6},
  isbn = {978-1-4614-7629-0 978-1-4614-7630-6},
  langid = {english},
  file = {/home/cjber/drive/pdf/Vanderbei_2014_Linear Programming.pdf}
}

@article{vanrijsbergen1979,
  title = {Information Retrieval/{{CJ}} van {{Rijsbergen}}},
  author = {Van Rijsbergen, CJ},
  year = {1979},
  keywords = {\#nosource,⛔ No DOI found}
}

@book{vanrossum1995,
  title = {Python Tutorial},
  author = {Van Rossum, Guido and Drake Jr, Fred L},
  year = {1995},
  volume = {620},
  publisher = {{Centrum voor Wiskunde en Informatica Amsterdam}},
  file = {/home/cjber/drive/pdf/Van Rossum_Drake Jr_1995_Python tutorial.pdf}
}

@book{vansambeek2018,
  title = {Advanced {{Mobile Location}} ({{AML}})},
  author = {{van Sambeek}, MJM and {van den Brink}, Mark and {en Veiligheid}, Opdrachtgever Ministerie van Justitie and Melden, Projectnaam Het Nieuwe},
  year = {2018},
  publisher = {{Den Haag: TNO}},
  file = {/home/cjber/drive/pdf/van Sambeek et al_2018_Advanced Mobile Location (AML).pdf}
}

@misc{various2012,
  title = {Stan {{Openshaw Festschrift}}},
  author = {{various}},
  year = {2012},
  journal = {Google Docs},
  abstract = {Please help to develop this document.  If you are not an editor of the document you can still comment on it using the comments functionality. If you want to edit the document directly, let us know so we can share it. (We have thought it best not to let anyone edit the document.) Stan Openshaw Fes...},
  howpublished = {https://docs.google.com/document/d/1\_BaSKdGR-5auf7JxXp8MaogvFOVp1pDCM\_47jB3cilc/edit?usp=embed\_facebook},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/2DTCVJFS/edit.html}
}

@article{vasardani2013,
  title = {Locating Place Names from Place Descriptions},
  author = {Vasardani, Maria and Winter, Stephan and Richter, Kai-Florian},
  year = {2013},
  month = dec,
  journal = {International Journal of Geographical Information Science},
  volume = {27},
  number = {12},
  pages = {2509--2532},
  issn = {1365-8816, 1362-3087},
  doi = {10/ggfmsf},
  abstract = {In this paper we review current literature on geographic information retrieval based on place names. We focus on the positional uncertainties and extent vagueness frequently associated with place names in linguistic place descriptions and on the differences between common users' perception and the way that geographic information services interpret place names. We argue that despite some notable efforts from the scientific community, geographic information services still cannot unambiguously recognize and sufficiently reason spatially with place names from linguistic expressions. We focus on three interrelated research areas: (a) the use of place names in gazetteers, (b) the use of formal models to reason with spatial relations and with the spatial extent of place names in linguistic place descriptions, and (c) Web harvesting and crowd sourcing techniques for identifying place names and their spatial extension from public and volunteer sources such as social networks and photo-sharing sites. We identify some opportunities for synthesizing existing approaches that would expedite the process of intelligent communication about place names between services and users. We discuss the shortcomings of the current state of affairs in locating place names from place descriptions, and identify new areas of importance for future research.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Vasardani et al_2013_Locating place names from place descriptions.pdf}
}

@inproceedings{vasardani2013a,
  title = {From Descriptions to Depictions: {{A}} Conceptual Framework},
  booktitle = {International {{Conference}} on {{Spatial Information Theory}}},
  author = {Vasardani, Maria and Timpf, Sabine and Winter, Stephan and Tomko, Martin},
  year = {2013},
  pages = {299--319},
  publisher = {{Springer}},
  keywords = {\#nosource}
}

@article{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/Vaswani et al_2017_Attention Is All You Need.pdf;/home/cjber/drive/zotero/storage/37NNQGYQ/1706.html}
}

@article{velaga2012,
  title = {Transport Poverty Meets the Digital Divide: Accessibility and Connectivity in Rural Communities},
  shorttitle = {Transport Poverty Meets the Digital Divide},
  author = {Velaga, Nagendra R. and Beecroft, Mark and Nelson, John D. and Corsar, David and Edwards, Peter},
  year = {2012},
  month = mar,
  journal = {Journal of Transport Geography},
  volume = {21},
  pages = {102--112},
  issn = {09666923},
  doi = {10/fx4rhh},
  abstract = {Rural communities face a range of challenges associated with accessibility and connectivity which apply in both the physical and virtual sphere. Constraints in rural transport infrastructure and services are often compounded by limitations in the development and resilience of technological infrastructures. In this context there is significant disparity between urban and rural communities.},
  langid = {english},
  annotation = {ZSCC: 0000141},
  file = {/home/cjber/drive/pdf/ENVS492/Velaga et al_2012_.pdf;/home/cjber/drive/pdf/ENVS492/Velaga et al_2012_2.pdf;/home/cjber/drive/pdf/ENVS492/Velaga et al_2012_3.pdf;/home/cjber/drive/pdf/ENVS492/Velaga et al_2012_4.pdf}
}

@inproceedings{verma2006,
  title = {{{3D Building Detection}} and {{Modeling}} from {{Aerial LIDAR Data}}},
  booktitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} - {{Volume}} 2 ({{CVPR}}'06)},
  author = {Verma, V. and Kumar, R. and Hsu, S.},
  year = {2006},
  volume = {2},
  pages = {2213--2220},
  publisher = {{IEEE}},
  address = {{New York, NY, USA}},
  doi = {10/b4btjs},
  abstract = {This paper presents a method to detect and construct a 3D geometric model of an urban area with complex buildings using aerial LIDAR (Light Detection and Ranging) data. The LIDAR data collected from a nadir direction is a point cloud containing surface samples of not only the building roofs and terrain but also undesirable clutter from trees, cars, etc. The main contribution of this work is the automatic recognition and estimation of simple parametric shapes that can be combined to model very complex buildings from aerial LIDAR data. The main components of the detection and modeling algorithms are (i) Segmentation of roof and terrain points. (ii) Roof topology Inference. We introduce the concept of a roof-topology graph to represent the relationships between the various planar patches of a complex roof structure. (iii) Parametric roof composition. Simple parametric roof shapes that can be combined to create a complex roof structure of a building are recognized by searching for sub-graphs in its roof-topology graph. (iv) Terrain Modeling. The terrain is identified and modeled as a triangulated mesh. Finally, we provide experimental results that demonstrate the validity of our approach for rapid and automatic building detection and geometric modeling with real LIDAR data. We are able to model cities and other urban areas at the rate of about 10 minutes per sq. mile on a low-end PC.},
  isbn = {978-0-7695-2597-6},
  langid = {english},
  annotation = {ZSCC: 0000291},
  file = {/home/cjber/drive/pdf/ENVS492/Verma et al_2006_.pdf}
}

@article{viner2004,
  title = {Review of {{UK}} Skid Resistance Policy},
  author = {Viner, Helen and Sinhal, Ramesh and Parry, Tony},
  year = {2004},
  journal = {Preprint SURF},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000023}
}

@article{virtanen2020,
  title = {{{SciPy}} 1.0--{{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and Contributors, SciPy 1 0},
  year = {2020},
  month = mar,
  journal = {Nature Methods},
  volume = {17},
  number = {3},
  eprint = {1907.10121},
  eprinttype = {arxiv},
  pages = {261--272},
  issn = {1548-7091, 1548-7105},
  doi = {10/ggj45f},
  abstract = {SciPy is an open source scientific computing library for the Python programming language. SciPy 1.0 was released in late 2017, about 16 years after the original version 0.1 release. SciPy has become a de facto standard for leveraging scientific algorithms in the Python programming language, with more than 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories, and millions of downloads per year. This includes usage of SciPy in almost half of all machine learning projects on GitHub, and usage by high profile projects including LIGO gravitational wave analysis and creation of the first-ever image of a black hole (M87). The library includes functionality spanning clustering, Fourier transforms, integration, interpolation, file I/O, linear algebra, image processing, orthogonal distance regression, minimization algorithms, signal processing, sparse matrix handling, computational geometry, and statistics. In this work, we provide an overview of the capabilities and development practices of the SciPy library and highlight some recent technical developments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Mathematical Software,Computer Science - Software Engineering,Physics - Computational Physics},
  file = {/home/cjber/drive/pdf/Virtanen et al_2020_SciPy 1.pdf;/home/cjber/drive/zotero/storage/7KFFEI4B/1907.html}
}

@article{voas2000,
  title = {The {{Scale}} of {{Dissimilarity}}: {{Concepts}}, {{Measurement}} and an {{Application}} to {{Socio-Economic Variation Across England}} and {{Wales}}},
  shorttitle = {The {{Scale}} of {{Dissimilarity}}},
  author = {Voas, David and Williamson, Paul},
  year = {2000},
  month = dec,
  journal = {Transactions of the Institute of British Geographers},
  volume = {25},
  number = {4},
  pages = {465--481},
  issn = {0020-2754, 1475-5661},
  doi = {10/d8d338},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/voas2000.pdf}
}

@article{vosselman2000,
  title = {Slope {{Based Filtering}} of {{Laser Altimetry Data}}},
  author = {Vosselman, George},
  year = {2000},
  pages = {9},
  abstract = {Laser altimetry is becoming the prime method for large scale acquisition of height data. Although laser altimetry is full integrated into processes for the production of digital elevation models in different countries, the derivation of DEM's from the raw laser altimetry measurements still causes problems. In particular the laser pulses reflected on the ground surface need to be distinguished from those reflecting on buildings and vegetation. In this paper a new method is proposed for filtering laser data. This method is closely related to the erosion operator used for mathematical grey scale morphology. Based on height differences in a representative training dataset, filter functions are derived that either preserve important terrain characteristics or minimise the number of classification errors. In experiments it is shown that the latter filter causes smaller errors in the resulting digital elevation models. In general the performance of the filters deteriorates with a decreasing point density.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000833},
  file = {/home/cjber/drive/pdf/ENVS492/Vosselman_2000_.pdf}
}

@article{vosselman2004,
  title = {Recognising Structure in Laser Scanner Point Clouds},
  author = {Vosselman, George and Gorte, Ben GH and Sithole, George and Rabbani, Tahir},
  year = {2004},
  journal = {International archives of photogrammetry, remote sensing and spatial information sciences},
  volume = {46},
  number = {8},
  pages = {33--38},
  keywords = {\#nosource,⛔ No DOI found},
  annotation = {ZSCC: 0000541}
}

@article{vosselman2009,
  title = {Detection of Curbstones in Airborne Laser Scanning Data},
  author = {Vosselman, George and Zhou, Liang},
  year = {2009},
  journal = {International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {38},
  number = {Part 3/W8},
  pages = {111--116},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000030},
  file = {/home/cjber/drive/pdf/ENVS492/Vosselman_Zhou_2009_.pdf;/home/cjber/drive/pdf/ENVS492/Vosselman_Zhou_2009_2.pdf}
}

@article{vosselman2009a,
  title = {Advanced {{Point Cloud Processing}}},
  author = {Vosselman, George},
  year = {2009},
  pages = {10},
  abstract = {The high pulse frequencies of today's airborne, mobile and terrestrial laser scanners enable the acquisition of point clouds with densities from some 20-50 points/m2 for airborne scanners to several thousands points/m2 for mobile and terrestrial scanners. For the (semi-)automated extraction of geo-information from point clouds these high point densities are very beneficial. The large number of points on the surfaces of objects to be extracted describe the surface geometry with a high redundancy. This allows the reliable detection of such surfaces in a point cloud. In this paper various examples are presented on how point cloud segmentations can be used to automatically extract geo-information. The paper focusses on the extraction of man-made objects in the urban environment. The examples include the processing of point clouds acquired by airborne, mobile as well as terrestrial laser scanners. The usage of generic knowledge on the objects to be mapped is shown to play a key role in the automation of the point cloud interpretation.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000074},
  file = {/home/cjber/drive/pdf/ENVS492/Vosselman_2009_.pdf}
}

@inproceedings{wacholder1997,
  title = {Disambiguation of Proper Names in Text},
  booktitle = {Proceedings of the Fifth Conference on {{Applied}} Natural Language Processing  -},
  author = {Wacholder, Nina and Ravin, Yael and Choi, Misook},
  year = {1997},
  pages = {202--208},
  publisher = {{Association for Computational Linguistics}},
  address = {{Washington, DC}},
  doi = {10/df5qr4},
  abstract = {Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the manyto-many mapping between names and their referents. We analyze the types of ambiguity -- structural and semantic -- that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T.J. Watson Research Center.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wacholder et al_1997_Disambiguation of proper names in text.pdf}
}

@article{waddell1993,
  title = {Residential Property Values in a Multinodal Urban Area: {{New}} Evidence on the Implicit Price of Location},
  shorttitle = {Residential Property Values in a Multinodal Urban Area},
  author = {Waddell, Paul and Berry, Brian J. L. and Hoch, Irving},
  year = {1993},
  month = sep,
  journal = {The Journal of Real Estate Finance and Economics},
  volume = {7},
  number = {2},
  pages = {117--141},
  issn = {0895-5638, 1573-045X},
  doi = {10/d22v6p},
  abstract = {The monocentricmodel predicts a housingprice gradient from the central business district, and it followsthat the extensionof this model to account for modem multinodalmetropolitanareas wouldpredict housing price gradients frommultipleemploymentcenters.Empiricalanalysisusinghedonicregressiontechniquesforthe estimation of price gradients in a multinodal contextis limited. This study extends prior work by exploringnonlinear housing price gradients in a multinodal urban area with an unusually robust database of housing sales transactions, and using a geographicinformationsystemfor spatial analysis. The results confirm the importanceof non-CBDemploymentcenters, a strong if asymmetricCBD price gradient, and significantnonlineargradients from such other urban amenities as major retail sites and highways.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS453/waddell1993.pdf}
}

@article{wadsworth2010,
  title = {{{THE UK LABOUR MARKET AND IMMIGRATION}}},
  author = {Wadsworth, Jonathan},
  year = {2010},
  month = jul,
  journal = {National Institute Economic Review},
  volume = {213},
  number = {1},
  pages = {R35-R42},
  issn = {0027-9501, 1741-3036},
  doi = {10/br4529},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/wadsworth2010.pdf}
}

@article{wagner2020,
  title = {Place in the {{GIScience Community}} \textendash{} an {{Indicative}} and {{Preliminary Systematic Literature Review}}},
  author = {Wagner, Daniel and Zipf, Alexander and Westerholt, Rene},
  year = {2020},
  month = jan,
  publisher = {{Zenodo}},
  doi = {10/gg89hn},
  abstract = {The concept of place has recently gained importance in geographical information science (GIScience). One reason for this is the emergence of user-generated geographic information, which partially represents subjective everyday geographical encounters. No consensus, however, on how to deal with place in GIScience has yet been reached. This paper presents a systematic literature review providing an overview of how parts of the GIScience community currently use the concept of place as it is understood in human geography. The results suggest that most place related GIScience scholars refer to the humanistic tradition of geography focusing on the essence of experiences of place. Further, it is found that geotagged data published online are a major driver of place-based research, whereas scientific data (e.g., surveys) are less commonly found in respective papers. Many researchers make use of exploratory approaches, which may reflect the early stage at which place-based GIScience research still sits. We also identify a difference between the approach core members of GIScience take and those working on the edge of the field. Thereby, the former often work more conceptually than the latter. The results of this preliminary review inform the current GIScience discourse on place by important evidence about the intellectual standpoints of GIScience scholars, thus fostering future research into place.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  langid = {english},
  keywords = {place},
  file = {/home/cjber/drive/pdf/Wagner et al_2020_Place in the GIScience Community – an Indicative and Preliminary Systematic.pdf}
}

@inproceedings{wallgrun2014,
  title = {Building a Corpus of Spatial Relational Expressions Extracted from Web Documents},
  booktitle = {Proceedings of the 8th {{Workshop}} on {{Geographic Information Retrieval}} - {{GIR}} '14},
  author = {Wallgr{\"u}n, Jan Oliver and Klippel, Alexander and Baldwin, Timothy},
  year = {2014},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{Dallas, Texas}},
  doi = {10/ggwjss},
  abstract = {Spatial language, despite decades of research, still poses substantial challenges for automated systems, for instance in geographic information retrieval or human-robot interaction. We describe an approach to building a corpus of natural language expressions extracted from web documents for analyzing and modeling spatial relational expressions (SRE). The unique characteristic of this corpus is that it is built around georeferenced triplets, with each triplet containing two entities (including their latitude/longitude coordinates) related by a spatial expression such as near. While the approach is still experimental, our first results are promising, in that we believe they will form the foundation for a comprehensive contextualized model for interpreting spatial natural language expressions. For the time being, we are focusing on a single domain, hotel reviews. This domain restriction allowed us to implement a proof-of-concept that this approach, with advances in natural language technologies, will indeed deliver a comprehensive corpus. The potential to collect larger corpora, and associated challenges, is discussed.},
  isbn = {978-1-4503-3135-7},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wallgrün et al_2014_Building a corpus of spatial relational expressions extracted from web documents.pdf}
}

@article{wallgrun2018,
  ids = {wallgrun2017},
  title = {{{GeoCorpora}}: Building a Corpus to Test and Train Microblog Geoparsers},
  shorttitle = {{{GeoCorpora}}},
  author = {Wallgr{\"u}n, Jan Oliver and Karimzadeh, Morteza and MacEachren, Alan M. and Pezanowski, Scott},
  year = {2018},
  month = jan,
  journal = {International Journal of Geographical Information Science},
  volume = {32},
  number = {1},
  pages = {1--29},
  issn = {1365-8816, 1362-3087},
  doi = {10/ggwjtb},
  abstract = {In this article, we present the GeoCorpora corpus building framework and software tools as well as a geo-annotated Twitter corpus built with these tools to foster research and development in the areas of microblog/Twitter geoparsing and geographic information retrieval. The developed framework employs crowdsourcing and geovisual analytics to support the construction of large corpora of text in which the mentioned location entities are identified and geolocated to toponyms in existing geographical gazetteers. We describe how the approach has been applied to build a corpus of geo-annotated tweets that will be made freely available to the research community alongside this article to support the evaluation, comparison and training of geoparsers. Additionally, we report lessons learned related to corpus construction for geoparsing as well as insights about the notions of place and natural spatial language that we derive from application of the framework to building this corpus.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wallgrün et al_2018_GeoCorpora.pdf;/home/cjber/drive/zotero/storage/6EI3MZ8K/Wallgrün et al_2018_GeoCorpora2.pdf;/home/cjber/drive/zotero/storage/QBFJXSS2/Wallgrün et al_2018_GeoCorpora3.pdf}
}

@article{wallgrun2018a,
  title = {{{GeoCorpora}}: Building a Corpus to Test and Train Microblog Geoparsers},
  shorttitle = {{{GeoCorpora}}},
  author = {Wallgr{\"u}n, Jan Oliver and Karimzadeh, Morteza and MacEachren, Alan M. and Pezanowski, Scott},
  year = {2018},
  month = jan,
  journal = {International Journal of Geographical Information Science},
  volume = {32},
  number = {1},
  pages = {1--29},
  issn = {1365-8816, 1362-3087},
  doi = {10.1080/13658816.2017.1368523},
  abstract = {In this article, we present the GeoCorpora corpus building framework and software tools as well as a geo-annotated Twitter corpus built with these tools to foster research and development in the areas of microblog/Twitter geoparsing and geographic information retrieval. The developed framework employs crowdsourcing and geovisual analytics to support the construction of large corpora of text in which the mentioned location entities are identified and geolocated to toponyms in existing geographical gazetteers. We describe how the approach has been applied to build a corpus of geo-annotated tweets that will be made freely available to the research community alongside this article to support the evaluation, comparison and training of geoparsers. Additionally, we report lessons learned related to corpus construction for geoparsing as well as insights about the notions of place and natural spatial language that we derive from application of the framework to building this corpus.},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/3RW25IEY/Wallgrün et al. - 2018 - GeoCorpora building a corpus to test and train mi.pdf}
}

@inproceedings{wan2007,
  title = {A {{Road Extraction Approach Based}} on {{Fuzzy Logic}} for {{High-Resolution Multispectral Data}}},
  booktitle = {Fourth {{International Conference}} on {{Fuzzy Systems}} and {{Knowledge Discovery}} ({{FSKD}} 2007)},
  author = {Wan, Y. and Shen, S. and Song, Y. and Liu, S.},
  year = {2007},
  month = aug,
  volume = {2},
  pages = {203--207},
  doi = {10/cpf8x8},
  abstract = {In this paper, we present an approach based on information fusion for road networks extraction from high-resolution multi-spectral satellite image data that builds upon pixel-based fuzzy logic segmentation approach. This road networks extraction system includes three different modules: geometrical features processing based on local context; segmentation based on fuzzy classification; angular texture signature to refine road clusters. Experimental results show that this method is efficient and distinct in the discrimination of roads and other spectral similar land cover classes.},
  annotation = {ZSCC: 0000014},
  file = {/home/cjber/drive/pdf/ENVS492/Wan et al_2007_.pdf;/home/cjber/drive/zotero/storage/A6P758J8/4406073.html}
}

@article{wang2009,
  title = {Segmentation of {{LiDAR Point Clouds}} for {{Building Extraction}}},
  author = {Wang, Jun and Shan, Jie},
  year = {2009},
  pages = {11},
  abstract = {The objective of segmentation on point clouds is to spatially group points with similar properties into homogeneous regions. Segmentation is a fundamental issue in processing point clouds data acquired by LiDAR and the quality of segmentation largely determines the success of information retrieval. Unlike the image or TIN model, the point clouds do not explicitly represent topology information. As a result, most existing segmentation methods for image and TIN have encountered two difficulties. First, converting data from irregular 3-D point clouds to other models usually leads to information loss; this is particularly a serious drawback for range image based algorithms. Second, the high computation cost of converting a large volume of point data is a considerable problem for any large scale LiDAR application. In this paper, we investigate the strategy to develop LiDAR segmentation methods directly based on point clouds data model. We first discuss several potential local similarity measures based on discrete computation geometry and machine learning. A prototype algorithm supported by fast nearest neighborhood search and based on advanced similarity measures is proposed and implemented to segment point clouds directly. Our experiments show that the proposed method is efficient and robust comparing with algorithms based on image and TIN. The paper will review popular segmentation methods in related disciplines and present the segmentation results of diverse buildings with different levels of difficulty.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {ZSCC: 0000076},
  file = {/home/cjber/drive/pdf/ENVS492/Wang_Shan_2009_.pdf}
}

@inproceedings{wang2010,
  title = {Don't Follow Me: {{Spam}} Detection in {{Twitter}}},
  shorttitle = {Don't Follow Me},
  booktitle = {2010 {{International Conference}} on {{Security}} and {{Cryptography}} ({{SECRYPT}})},
  author = {Wang, Alex Hai},
  year = {2010},
  month = jul,
  pages = {1--10},
  abstract = {The rapidly growing social network Twitter has been infiltrated by large amount of spam. In this paper, a spam detection prototype system is proposed to identify suspicious users on Twitter. A directed social graph model is proposed to explore the ``follower'' and ``friend'' relationships among Twitter. Based on Twitter's spam policy, novel content-based features and graph-based features are also proposed to facilitate spam detection. A Web crawler is developed relying on API methods provided by Twitter. Around 25K users, 500K tweets, and 49M follower/friend relationships in total are collected from public available data on Twitter. Bayesian classification algorithm is applied to distinguish the suspicious behaviors from normal ones. I analyze the data set and evaluate the performance of the detection system. Classic evaluation metrics are used to compare the performance of various traditional classification methods. Experiment results show that the Bayesian classifier has the best overall performance in term of F-measure. The trained classifier is also applied to the entire data set. The result shows that the spam detection system can achieve 89\% precision.},
  keywords = {Bayesian methods,Classification,Crawlers,Feature extraction,Machine learning,Social network security,Spam detection,Twitter,Unsolicited electronic mail},
  file = {/home/cjber/drive/pdf/Wang_2010_Don't follow me.pdf;/home/cjber/drive/zotero/storage/DMIFCVYV/5741690.html}
}

@article{wang2015,
  title = {Road Network Extraction: {{A}} Neural-Dynamic Framework Based on Deep Learning and a Finite State Machine},
  author = {Wang, Jun and Song, Jingwei and Chen, Mingquan and Yang, Zhi},
  year = {2015},
  journal = {International Journal of Remote Sensing},
  volume = {36},
  number = {12},
  pages = {3144--3169},
  issn = {0143-1161},
  doi = {10/gf3tdp},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000062}
}

@article{wang2018,
  title = {Social Media Analytics for Natural Disaster Management},
  author = {Wang, Zheye and Ye, Xinyue},
  year = {2018},
  journal = {International Journal of Geographical Information Science},
  volume = {32},
  number = {1},
  pages = {49--72},
  publisher = {{Taylor \& Francis}},
  issn = {1365-8816},
  doi = {10/gg2rj5},
  file = {/home/cjber/drive/pdf/Wang_Ye_2018_Social media analytics for natural disaster management.pdf}
}

@inproceedings{wang2018a,
  title = {Label-{{Free Distant Supervision}} for {{Relation Extraction}} via {{Knowledge Graph Embedding}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wang, Guanying and Zhang, Wen and Wang, Ruoxu and Zhou, Yalin and Chen, Xi and Zhang, Wei and Zhu, Hai and Chen, Huajun},
  year = {2018},
  pages = {2246--2255},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10/ggnxd9},
  abstract = {Distant supervision is an effective method to generate large scale labeled data for relation extraction, which assumes that if a pair of entities appears in some relation of a Knowledge Graph (KG), all sentences containing those entities in a large unlabeled corpus are then labeled with that relation to train a relation classifier. However, when the pair of entities has multiple relationships in the KG, this assumption may produce noisy relation labels. This paper proposes a label-free distant supervision method, which makes no use of the relation labels under this inadequate assumption, but only uses the prior knowledge derived from the KG to supervise the learning of the classifier directly and softly. Specifically, we make use of the type information and the translation law derived from typical KG embedding model to learn embeddings for certain sentence patterns. As the supervision signal is only determined by the two aligned entities, neither hard relation labels nor extra noise-reduction model for the bag of sentences is needed in this way. The experiments show that the approach performs well in current distant supervision dataset.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wang et al_2018_Label-Free Distant Supervision for Relation Extraction via Knowledge Graph.pdf}
}

@inproceedings{wang2019,
  title = {Are We There yet?: Evaluating State-of-the-Art Neural Network Based Geoparsers Using {{EUPEG}} as a Benchmarking Platform},
  shorttitle = {Are We There Yet?},
  booktitle = {Proceedings of the 3rd {{ACM SIGSPATIAL International Workshop}} on {{Geospatial Humanities}}  - {{GeoHumanities}} '19},
  author = {Wang, Jimin and Hu, Yingjie},
  year = {2019},
  pages = {1--6},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois}},
  doi = {10/ggwsbj},
  abstract = {Geoparsing is an important task in geographic information retrieval. A geoparsing system, known as a geoparser, takes some texts as the input and outputs the recognized place mentions and their location coordinates. In June 2019, a geoparsing competition, Toponym Resolution in Scientific Papers, was held as one of the SemEval 2019 tasks. The winning teams developed neural network based geoparsers that achieved outstanding performances (over 90\% precision, recall, and F1 score for toponym recognition). This exciting result brings the question ``are we there yet?'', namely have we achieved high enough performances to possibly consider the problem of geoparsing as solved? One limitation of this competition is that the developed geoparsers were tested on only one dataset which has 45 research articles collected from the particular domain of Bio-medicine. It is known that the same geoparser can have very different performances on different datasets. Thus, this work performs a systematic evaluation of these state-of-the-art geoparsers using our recently developed benchmarking platform EUPEG that has eight annotated datasets, nine baseline geoparsers, and eight performance metrics. The evaluation result suggests that these new geoparsers indeed improve the performances of geoparsing on multiple datasets although some challenges remain.},
  isbn = {978-1-4503-6960-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wang_Hu_2019_Are we there yet.pdf}
}

@inproceedings{wang2019a,
  title = {{{DM}}\_{{NLP}} at {{SemEval-2018 Task}} 12: {{A Pipeline System}} for {{Toponym Resolution}}},
  shorttitle = {{{DM}}\_{{NLP}} at {{SemEval-2018 Task}} 12},
  booktitle = {Proceedings of the 13th {{International Workshop}} on {{Semantic Evaluation}}},
  author = {Wang, Xiaobin and Ma, Chunping and Zheng, Huafei and Liu, Chu and Xie, Pengjun and Li, Linlin and Si, Luo},
  year = {2019},
  month = jun,
  pages = {917--923},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota, USA}},
  doi = {10/gg68pj},
  abstract = {This paper describes DM-NLP's system for toponym resolution task at Semeval 2019. Our system was developed for toponym detection, disambiguation and end-to-end resolution which is a pipeline of the former two. For toponym detection, we utilized the state-of-the-art sequence labeling model, namely, BiLSTM-CRF model as backbone. A lot of strategies are adopted for further improvement, such as pre-training, model ensemble, model averaging and data augment. For toponym disambiguation, we adopted the widely used searching and ranking framework. For ranking, we proposed several effective features for measuring the consistency between the detected toponym and toponyms in GeoNames. Eventually, our system achieved the best performance among all the submitted results in each sub task.},
  file = {/home/cjber/drive/pdf/Wang et al_2019_DM_NLP at SemEval-2018 Task 12.pdf}
}

@article{wang2019b,
  title = {Geographic {{Knowledge Graph}} ({{GeoKG}}): {{A Formalized Geographic Knowledge Representation}}},
  shorttitle = {Geographic {{Knowledge Graph}} ({{GeoKG}})},
  author = {Wang, Shu and Zhang, Xueying and Ye, Peng and Du, Mi and Lu, Yanxu and Xue, Haonan},
  year = {2019},
  month = apr,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {8},
  number = {4},
  pages = {184},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10/ghwq43},
  abstract = {Formalized knowledge representation is the foundation of Big Data computing, mining and visualization. Current knowledge representations regard information as items linked to relevant objects or concepts by tree or graph structures. However, geographic knowledge differs from general knowledge, which is more focused on temporal, spatial, and changing knowledge. Thus, discrete knowledge items are difficult to represent geographic states, evolutions, and mechanisms, e.g., the processes of a storm \&ldquo;\{9:30-60 mm-precipitation\}-\{12:00-80 mm-precipitation\}-\&hellip;\&rdquo;. The underlying problem is the constructors of the logic foundation (ALC description language) of current geographic knowledge representations, which cannot provide these descriptions. To address this issue, this study designed a formalized geographic knowledge representation called GeoKG and supplemented the constructors of the ALC description language. Then, an evolution case of administrative divisions of Nanjing was represented with the GeoKG. In order to evaluate the capabilities of our formalized model, two knowledge graphs were constructed by using the GeoKG and the YAGO by using the administrative division case. Then, a set of geographic questions were defined and translated into queries. The query results have shown that GeoKG results are more accurate and complete than the YAGO\&rsquo;s with the enhancing state information. Additionally, the user evaluation verified these improvements, which indicates it is a promising powerful model for geographic knowledge representation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {formalization,geographic knowledge graph,geographic knowledge representation,GeoKG},
  file = {/home/cjber/drive/pdf/Wang et al_2019_Geographic Knowledge Graph (GeoKG).pdf;/home/cjber/drive/zotero/storage/N4FAGPMV/184.html}
}

@article{wang2020,
  title = {{{NeuroTPR}}: {{A}} Neuro-net Toponym Recognition Model for Extracting Locations from Social Media Messages},
  shorttitle = {{{NeuroTPR}}},
  author = {Wang, Jimin and Hu, Yingjie and Joseph, Kenneth},
  year = {2020},
  month = may,
  journal = {Transactions in GIS},
  pages = {tgis.12627},
  issn = {1361-1682, 1467-9671},
  doi = {10/ggwpdt},
  abstract = {Social media messages, such as tweets, are frequently used by people during natural disasters to share real-time information and to report incidents. Within these messages, geographic locations are often described. Accurate recognition and geolocation of these locations are critical for reaching those in need. This article focuses on the first part of this process, namely recognizing locations from social media messages. While general named entity recognition tools are often used to recognize locations, their performance is limited due to the various language irregularities associated with social media text, such as informal sentence structures, inconsistent letter cases, name abbreviations, and misspellings. We present NeuroTPR, which is a Neuro-net ToPonym Recognition model designed specifically with these linguistic irregularities in mind. Our approach extends a general bidirectional recurrent neural network model with a number of features designed to address the task of location recognition in social media messages. We also propose an automatic workflow for generating annotated data sets from Wikipedia articles for training toponym recognition models. We demonstrate NeuroTPR by applying it to three test data sets, including a Twitter data set from Hurricane Harvey, and comparing its performance with those of six baseline models.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wang et al_2020_NeuroTPR.pdf;/home/cjber/drive/pdf/Wang et al_2020_NeuroTPR2.pdf}
}

@article{wang2020a,
  title = {Language {{Models}} Are {{Open Knowledge Graphs}}},
  author = {Wang, Chenguang and Liu, Xiao and Song, Dawn},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.11967 [cs]},
  eprint = {2010.11967},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/cjber/drive/pdf/Wang et al_2020_Language Models are Open Knowledge Graphs.pdf;/home/cjber/drive/zotero/storage/GUHI78CG/2010.html}
}

@article{wankhade2011,
  title = {Performance Measurement and the {{UK}} Emergency Ambulance Service: {{Unintended}} Consequences of the Ambulance Response Time Targets},
  shorttitle = {Performance Measurement and the {{UK}} Emergency Ambulance Service},
  author = {Wankhade, Paresh},
  year = {2011},
  month = jul,
  journal = {International Journal of Public Sector Management},
  volume = {24},
  number = {5},
  pages = {384--402},
  issn = {0951-3558},
  doi = {10/bb6r9x},
  abstract = {Purpose \textendash{} The purpose of this paper is to assess the performance measurement in the UK NHS ambulance service documenting various unintended consequences of the current performance framework and to suggest a future research agenda.},
  langid = {english},
  keywords = {ES},
  file = {/home/cjber/drive/pdf/Wankhade_2011_Performance measurement and the UK emergency ambulance service.pdf}
}

@article{warf2010,
  title = {From {{GIS}} to Neogeography: Ontological Implications and Theories of Truth},
  shorttitle = {From {{GIS}} to Neogeography},
  author = {Warf, Barney and Sui, Daniel},
  year = {2010},
  month = dec,
  journal = {Annals of GIS},
  volume = {16},
  number = {4},
  pages = {197--209},
  issn = {1947-5683, 1947-5691},
  doi = {10/cvkf44},
  langid = {english},
  keywords = {Ontology},
  file = {/home/cjber/drive/pdf/Warf_Sui_2010_From GIS to neogeography.pdf}
}

@article{waters2003,
  title = {Tools for Web-Based {{GIS}} Mapping of a "Fuzzy" Vernacular Geography},
  author = {Waters, T and Evans, A J},
  year = {2003},
  pages = {10},
  abstract = {The vast majority of people don't use a scientific geographical vocabulary, nevertheless most use a wide variety of geographical terms on a day to day basis. Identifiers like ``Downtown'' or ``The grim area around the docks'' are part of a vernacular geographical terminology which is vastly more used than the coordinate systems and scientifically defined variables so beloved of professional geographers. These terms not only identify areas, but give members of our sociolinguistic group information about them, building up a jointly-defined cultural world-view within which we all act on a daily basis. Despite its importance for policy making and quality of life, attention is rarely paid to this vernacular geography because it is so hard to capture and use. This paper presents a new set of tools for capturing these ``fuzzy'' psychogeographical areas and their associated attributes, through a web based mapping system. The system contains a spraycan tool that allows users to tag information onto diffuse areas of varying density. An example of their use to define areas people consider are ``high crime'' within a UK city is also presented, along with users' responses to the system. Such a system aims to pull together professional and popular geographical understanding, to the advantage of both.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Waters_Evans_2003_Tools for web-based GIS mapping of a fuzzy vernacular geography.pdf}
}

@article{watt1994,
  title = {Health and Health Care of Rural Populations in the {{UK}}: Is It Better or Worse?},
  shorttitle = {Health and Health Care of Rural Populations in the {{UK}}},
  author = {Watt, I S and Franks, A J and Sheldon, T A},
  year = {1994},
  month = feb,
  journal = {Journal of Epidemiology \& Community Health},
  volume = {48},
  number = {1},
  pages = {16--21},
  issn = {0143-005X},
  doi = {10/fkjgg8},
  abstract = {Objective - To review available evidence on the problems facing rural health care in the UK. In particular, to determine whether the health of rural populations is worse than that of town dwellers and how the quality of health care is influenced by rurality.},
  langid = {english},
  file = {/home/cjber/drive/bib/pdfs/files/health and health;/home/cjber/drive/pdf/ENVS492/Watt et al_1994_.pdf}
}

@article{weissenbacher2015,
  title = {Knowledge-Driven Geospatial Location Resolution for Phylogeographic Models of Virus Migration},
  author = {Weissenbacher, Davy and Tahsin, Tasnia and Beard, Rachel and Figaro, Mari and Rivera, Robert and Scotch, Matthew and Gonzalez, Graciela},
  year = {2015},
  month = jun,
  journal = {Bioinformatics},
  volume = {31},
  number = {12},
  pages = {i348-i356},
  issn = {1367-4803, 1460-2059},
  doi = {10/f7g3ft},
  abstract = {Summary: Diseases caused by zoonotic viruses (viruses transmittable between humans and animals) are a major threat to public health throughout the world. By studying virus migration and mutation patterns, the field of phylogeography provides a valuable tool for improving their surveillance. A key component in phylogeographic analysis of zoonotic viruses involves identifying the specific locations of relevant viral sequences. This is usually accomplished by querying public databases such as GenBank and examining the geospatial metadata in the record. When sufficient detail is not available, a logical next step is for the researcher to conduct a manual survey of the corresponding published articles.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Weissenbacher et al_2015_Knowledge-driven geospatial location resolution for phylogeographic models of.pdf}
}

@inproceedings{weissenbacher2019,
  title = {{{SemEval-2019 Task}} 12: {{Toponym Resolution}} in {{Scientific Papers}}},
  shorttitle = {{{SemEval-2019 Task}} 12},
  booktitle = {Proceedings of the 13th {{International Workshop}} on {{Semantic Evaluation}}},
  author = {Weissenbacher, Davy and Magge, Arjun and O'Connor, Karen and Scotch, Matthew and {Gonzalez-Hernandez}, Graciela},
  year = {2019},
  pages = {907--916},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota, USA}},
  doi = {10/ggwjtv},
  abstract = {We present the SemEval-2019 Task 12 which focuses on toponym resolution in scientific articles. Given an article from PubMed, the task consists of detecting mentions of names of places, or toponyms, and mapping the mentions to their corresponding entries in GeoNames.org, a database of geospatial locations. We proposed three subtasks. In Subtask 1, we asked participants to detect all toponyms in an article. In Subtask 2, given toponym mentions as input, we asked participants to disambiguate them by linking them to entries in GeoNames. In Subtask 3, we asked participants to perform both the detection and the disambiguation steps for all toponyms. A total of 29 teams registered, and 8 teams submitted a system run. We summarize the corpus and the tools created for the challenge. They are freely available at https://competitions.codalab. org/competitions/19948. We also analyze the methods, the results and the errors made by the competing systems with a focus on toponym disambiguation.},
  langid = {english},
  keywords = {TD},
  file = {/home/cjber/drive/pdf/Weissenbacher et al_2019_SemEval-2019 Task 12.pdf;/home/cjber/drive/pdf/Weissenbacher et al_2019_SemEval-2019 Task 2.pdf}
}

@article{wells2009,
  title = {Individualism and {{Identity}}: {{Resistance}} to {{Speed Cameras}} in the {{UK}}},
  shorttitle = {Individualism and {{Identity}}},
  author = {Wells, Helen and Wills, David},
  year = {2009},
  month = apr,
  journal = {Surveillance \& Society},
  volume = {6},
  number = {3},
  pages = {259--274},
  issn = {1477-7487},
  doi = {10/gf5m5x},
  abstract = {As a surveillance technology, speed cameras have produced significant levels of resistance from the general (driving) public. This resistance has not, however, drawn on the kinds of civil liberties or 'Big Brother' narratives that might be expected. Using this context as a case study, this paper suggests that significant resistance to surveillance practices may emerge when surveillance technologies produce data doubles that are antagonistically incompatible with those identities which have emerged 'organically' from the resisting individuals and communities.},
  langid = {english},
  annotation = {ZSCC: 0000028[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Wells_Wills_2009_.pdf}
}

@article{wennemo2008,
  title = {Infant Mortality, Public Policy and Inequality - a Comparison of 18 Industrialised Countries 1950-85},
  author = {Wennemo, Irene},
  year = {2008},
  month = jun,
  journal = {Sociology of Health \& Illness},
  volume = {15},
  number = {4},
  pages = {429--446},
  issn = {01419889},
  doi = {10/c7nppv},
  abstract = {Using new comparative data bases this paper examines whether infant mortality rates in industrialised nations are affected by public policies and income inequality. Particular attention is given to the role of the level of economic development, public policy and the distribution of economic resources. The study shows that the level of economic development has a strong, but decreasing impact on the infant mortality rate. Income inequality and relative poverty rates appear to be of greater importance for the variation in infant mortality rates than the level of economic development between rich countries. Levels of unemployment and of social security benefits seems to affect the infant mortality rate; the combination of high unemployment and low unemployment benefits seems to be associated with particularly high mortality rates. A high level of family benefits is also associated with low infant mortality rates.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS418/Wennemo_2008_.pdf}
}

@article{westerholt2018,
  title = {Introduction to the {{PLATIAL}}'18 {{Workshop}} on {{Platial Analysis}}},
  author = {Westerholt, Rene and Mocnik, Franz-Benjamin and Zipf, Alexander},
  year = {2018},
  month = oct,
  publisher = {{Zenodo}},
  doi = {10/gg89fd},
  abstract = {The concept of ``place'' is about to become one of the major research themes in the discipline of geographical information science (GIScience), as well as in adjoining fields. Briefly put, while locations provide objective references (e.g., point coordinates), places are the units utilized by humans to approach the geographic world. The PLATIAL'18 workshop makes a significant contribution towards establishing a notion of place and is meant to be the starting point for a series of future events. What sets this workshop apart from others dealing with the concept of place is that the focus is decisively on its quantitative investigation and conceptual formalization. The workshop accommodates a wide range of aspects all of which in one or another way are related to the two outlined core foci. This is well reflected by the various topical sessions into which the workshop has been organized. These include ``Conceptual Anatomy of Place'', ``Disclosing Places from Human Discourse'', ``Bridging Space and Place'', and ``Exploratory and Visual Analytics of Place''. The content sessions were concomitantly inspired by two keynote talks by Alexis Comber (University of Leeds) and Clare Davies (University of Winchester).},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  langid = {english},
  file = {/home/cjber/drive/pdf/Westerholt et al_2018_Introduction to the PLATIAL'18 Workshop on Platial Analysis.pdf}
}

@book{wickham2014,
  title = {Advanced r},
  author = {Wickham, Hadley},
  year = {2014},
  publisher = {{Chapman and Hall/CRC}},
  isbn = {1-4665-8697-4},
  keywords = {\#nosource},
  annotation = {ZSCC: 0000183}
}

@book{wickham2016,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  isbn = {978-3-319-24277-4}
}

@article{wickham2019,
  title = {Welcome to the {{tidyverse}}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and Fran{\textexclamdown}U+00E7{\textquestiondown}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and M{\textexclamdown}U+00FC{\textquestiondown}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10/ggddkj}
}

@article{wijnands2018,
  title = {Identifying Behavioural Change among Drivers Using {{Long Short-Term Memory}} Recurrent Neural Networks},
  author = {Wijnands, Jasper S. and Thompson, Jason and Aschwanden, Gideon D.P.A. and Stevenson, Mark},
  year = {2018},
  month = feb,
  journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
  volume = {53},
  pages = {34--49},
  issn = {13698478},
  doi = {10/gc49pv},
  abstract = {Globally, motor vehicle crashes account for over 1.2 million fatalities per year and are the leading cause of death for people aged 15\textendash 29 years. The majority of road crashes are caused by human error, with risk heightened among young and novice drivers learning to negotiate the complexities of the road environment. Direct feedback has been shown to have a positive impact on driving behaviour. Methods that could detect behavioural changes and therefore, positively reinforce safer driving during the early stages of driver licensing could have considerable road safety benefit. A new methodology is presented combining in-vehicle telematics technology, providing measurements forming a personalised driver profile, with neural networks to identify changes in driving behaviour. Using Long Short-Term Memory (LSTM) recurrent neural networks, individual drivers are identified based on their pattern of acceleration, deceleration and exceeding the speed limit. After model calibration, new, real-time data of the driver is supplied to the LSTM and, by monitoring prediction performance, one can assess whether a (positive or negative) change in driving behaviour is occurring over time. The paper highlights that the approach is robust to different neural network structures, data selections, calibration settings, and methodologies to select benchmarks for safe and unsafe driving. Presented case studies show additional model applications for investigating changes in driving behaviour among individuals following or during specific events (e.g., receipt of insurance renewal letters) and time periods (e.g., driving during holiday periods). The application of the presented methodology shows potential to form the basis of timely provision of direct feedback to drivers by telematics-based insurers. Such feedback may prevent internalisation of new, risky driving habits contributing to crash risk, potentially reducing deaths and injuries among young drivers as a result.},
  langid = {english},
  annotation = {ZSCC: 0000008},
  file = {/home/cjber/drive/pdf/ENVS492/Wijnands et al_2018_.pdf}
}

@article{willems2012,
  title = {Rainfall in the Urban Context: {{Forecasting}}, Risk and Climate Change},
  shorttitle = {Rainfall in the Urban Context},
  author = {Willems, Patrick and Molnar, Peter and Einfalt, Thomas and {Arnbjerg-Nielsen}, Karsten and Onof, Christian and Nguyen, Van-Thanh-Van and Burlando, Paolo},
  year = {2012},
  month = jan,
  journal = {Atmospheric Research},
  volume = {103},
  pages = {1--3},
  issn = {01698095},
  doi = {10/bv2qqn},
  abstract = {Cities became since last decades more vulnerable to flooding because urban drainage infrastructure has been built at large scale, but also due to climate change. The number of quantitative assessment studies of the impact of climate change on urban drainage remains, however, rather limited. This is partly because of the particular difficulties when dealing with this type of impact. First problem is that the results of climate models need be downscaled to very small space and time resolutions (e.g. catchments of few kilometers, time scales as low as 5 minutes). The impact results consequently are highly uncertain, a problem that becomes more challenging since the properties of extremes do not automatically reflect those of average precipitation.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Willems et al_2012_Rainfall in the urban context.pdf}
}

@article{williams2008,
  title = {The {{Educational Priofity Area Project}} Forth Years on. '{{If}} at First You Don't Succeed, You [Still] Don't Succeed'},
  author = {Williams, Keith},
  year = {2008},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/williams2008.pdf}
}

@article{williams2013,
  title = {What Do People Study When They Study {{Twitter}}? {{Classifying Twitter}} Related Academic Papers},
  shorttitle = {What Do People Study When They Study {{Twitter}}?},
  author = {Williams, S.A. and Terras, M.M. and Warwick, C.},
  year = {2013},
  journal = {Journal of Documentation},
  volume = {69},
  number = {3},
  pages = {384--410},
  issn = {0022-0418},
  doi = {10/f42xw4},
  abstract = {Purpose: Since its introduction in 2006, messages posted to the microblogging system Twitter have provided a rich dataset for researchers, leading to the publication of over a thousand academic papers. This paper aims to identify this published work and to classify it in order to understand Twitter based research. Design/methodology/approach: Firstly the papers on Twitter were identified. Secondly, following a review of the literature, a classification of the dimensions of microblogging research was established. Thirdly, papers were qualitatively classified using open coded content analysis, based on the paper's title and abstract, in order to analyze method, subject, and approach. Findings: The majority of published work relating to Twitter concentrates on aspects of the messages sent and details of the users. A variety of methodological approaches is used across a range of identified domains. Research limitations/implications: This work reviewed the abstracts of all papers available via database search on the term "Twitter" and this has two major implications: the full papers are not considered and so works may be misclassified if their abstract is not clear; publications not indexed by the databases, such as book chapters, are not included. The study is focussed on microblogging, the applicability of the approach to other media is not considered. Originality/value: To date there has not been an overarching study to look at the methods and purpose of those using Twitter as a research subject. The paper's major contribution is to scope out papers published on Twitter until the close of 2011. The classification derived here will provide a framework within which researchers studying Twitter related topics will be able to position and ground their work. \textcopyright{} Emerald Group Publishing Limited.},
  langid = {english},
  keywords = {Abstracts,Blogs,Classification,Microblogging,Papers,Social network systems,Social networking sites,Twitter},
  file = {/home/cjber/drive/pdf/Williams et al_2013_What do people study when they study Twitter.pdf;/home/cjber/drive/zotero/storage/EN4Y9KAE/display.html}
}

@book{willis2017,
  title = {Learning to Labour: {{How}} Working Class Kids Get Working Class Jobs},
  author = {Willis, Paul},
  year = {2017},
  publisher = {{Routledge}},
  isbn = {1-351-21877-8},
  keywords = {\#nosource}
}

@article{wilson,
  title = {Recognizing {{Contextual Polarity}} in {{Phrase-Level Sentiment Analysis}}},
  author = {Wilson, Theresa and Wiebe, Janyce and Hoffmann, Paul},
  pages = {8},
  doi = {10/dxmdgn},
  abstract = {This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wilson et al_Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.pdf}
}

@inproceedings{wing2014,
  title = {Hierarchical {{Discriminative Classification}} for {{Text-Based Geolocation}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Wing, Benjamin and Baldridge, Jason},
  year = {2014},
  pages = {336--348},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10/ggwjtf},
  abstract = {Text-based document geolocation is commonly rooted in language-based information retrieval techniques over geodesic grids. These methods ignore the natural hierarchy of cells in such grids and fall afoul of independence assumptions. We demonstrate the effectiveness of using logistic regression models on a hierarchy of nodes in the grid, which improves upon the state of the art accuracy by several percent and reduces mean error distances by hundreds of kilometers on data from Twitter, Wikipedia, and Flickr. We also show that logistic regression performs feature selection effectively, assigning high weights to geocentric terms.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wing_Baldridge_2014_Hierarchical Discriminative Classification for Text-Based Geolocation.pdf}
}

@misc{winograd1970,
  title = {{{SHRDLU}}},
  author = {Winograd, Terry},
  year = {1970},
  howpublished = {http://hci.stanford.edu/\textasciitilde winograd/shrdlu/},
  file = {/home/cjber/drive/zotero/storage/2A9ZVIYK/shrdlu.html}
}

@article{winter2008,
  ids = {wintera,winterb,winterc},
  title = {Held in Conjunction with {{GIScience}}'08 {{Park City}}, {{Utah}}, {{USA}} 19 {{September}} 2008},
  author = {Winter, Stephan and Kuhn, Werner and Kruger, Antonio},
  year = {2008},
  pages = {83},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Winter et al_2008_Held in conjunction with GIScience’08 Park City, Utah, USA 19 September 2008.pdf;/home/cjber/drive/pdf/Winter et al_2008_Held in conjunction with GIScience’08 Park City, Utah, USA 19 September 22.pdf;/home/cjber/drive/pdf/Winter et al_2008_Held in conjunction with GIScience’08 Park City, Utah, USA 19 September 3.pdf;/home/cjber/drive/pdf/Winter et al_2008_Held in conjunction with GIScience’08 Park City, Utah, USA 19 September 4.pdf;/home/cjber/drive/pdf/Winter et al_2008_Held in conjunction with GIScience’08 Park City, Utah, USA 19 September 5.pdf}
}

@article{winter2010,
  title = {Spatially {{Enabling}} `{{Place}}' {{Information}}},
  author = {Winter, Stephan and Bennett, Rohan and Truelove, Marie and Rajabifard, Abbas and Duckham, Matt and Kealy, Allison and Leach, Joe},
  year = {2010},
  pages = {15},
  abstract = {Many attempts to embed the concept of `place' into current location-based technologies and spatial data infrastructures have failed spectacularly. Resolving places to single georeferenced points has been identified as a major factor. The prevailing hypothesis is that more complex georeferenced descriptions of places will resolve the problem. This paper refutes this hypothesis. It challenges it from five perspectives including issues related to: determining positional accuracy, defining vague and dynamic places, administering natural places with uncertain boundaries, supporting vanity and vernacular places, and representing the salience of places. These five perspectives demonstrate that while more complex modeling of place is necessary, it is only part of the solution. Successful spatial enablement of place will require other essential issues to be addressed including: combining absolute, relative, and visual accuracy definitions of place, using emerging sources of data (e.g. crowd-sourced) to develop dynamic descriptions of places, determining how to capture `places' from remotely sensed data, incorporating vanity and vernacular places into spatial data infrastructures, and embedding measures of the salience of place into spatial data infrastructures. These findings are synthesized into a future research agenda in spatial data infrastructures, or spatial information science in general.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Winter et al_2010_Spatially Enabling ‘Place’ Information.pdf}
}

@article{winter2012,
  ids = {winter2012a},
  title = {Approaching the Notion of Place by Contrast},
  author = {Winter, Stephan and Freksa, Christian},
  year = {2012},
  month = dec,
  journal = {Journal of Spatial Information Science},
  number = {5},
  pages = {31--50},
  issn = {1948-660X},
  doi = {10/ggwjsq},
  abstract = {Place is an elusive notion in geographic information science. This paper presents an approach to capture the notion of place by contrast. This approach is developed from cognitive concepts and the language that is used to describe places. It is complementary to those of coordinate-based systems that dominate contemporary geographic information systems. Accordingly, the approach is aimed at explaining structures in verbal place descriptions and at localizing objects without committing to geometrically specified positions in space. We will demonstrate how locations can be identified by place names that are not crisply defined in terms of geometric regions. Capturing the human cognitive notion of place is considered crucial for smooth communication between human users and computer-based geographic assistance systems.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Winter_Freksa_2012_Approaching the notion of place by contrast.pdf;/home/cjber/drive/zotero/storage/RMEYRNQD/Winter_Freksa_2012_Approaching the notion of place by contrast2.pdf}
}

@book{witelski2015,
  title = {Methods of {{Mathematical Modelling}}},
  author = {Witelski, Thomas and Bowen, Mark},
  year = {2015},
  series = {Springer {{Undergraduate Mathematics Series}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23042-9},
  isbn = {978-3-319-23041-2 978-3-319-23042-9},
  langid = {english},
  file = {/home/cjber/drive/pdf/Witelski_Bowen_2015_Methods of Mathematical Modelling.pdf;/home/cjber/drive/pdf/Witelski_Bowen_2015_Methods of Mathematical Modelling2.pdf}
}

@article{wolf2020,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State-of-the-art Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = jul,
  journal = {arXiv:1910.03771 [cs]},
  eprint = {1910.03771},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/ huggingface/transformers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Wolf et al_2020_HuggingFace's Transformers.pdf}
}

@incollection{wolter2018,
  title = {Context and {{Vagueness}} in {{Automated Interpretation}} of {{Place Description}}: {{A Computational Model}}},
  shorttitle = {Context and {{Vagueness}} in {{Automated Interpretation}} of {{Place Description}}},
  booktitle = {Proceedings of {{Workshops}} and {{Posters}} at the 13th {{International Conference}} on {{Spatial Information Theory}} ({{COSIT}} 2017)},
  author = {Wolter, Diedrich and Yousaf, Madiha},
  editor = {Fogliaroni, Paolo and Ballatore, Andrea and Clementini, Eliseo},
  year = {2018},
  pages = {137--142},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-63946-8_27},
  abstract = {We investigate automated interpretation of human-generated place descriptions. This paper presents work in progress towards a computational model that can represent spatial knowledge occurring in place descriptions and fosters efficient querying of a spatial database. In this paper we analyse how context information shapes the meaning of a place description and we outline a computational model to represent vague spatial knowledge and context occurring in human-generated place descriptions.},
  isbn = {978-3-319-63945-1 978-3-319-63946-8},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wolter_Yousaf_2018_Context and Vagueness in Automated Interpretation of Place Description.pdf}
}

@article{wrigley2003,
  title = {Deprivation, {{Diet}}, and {{Food-Retail Access}}: {{Findings}} from the {{Leeds}} `{{Food Deserts}}' {{Study}}},
  shorttitle = {Deprivation, {{Diet}}, and {{Food-Retail Access}}},
  author = {Wrigley, Neil and Warm, Daniel and Margetts, Barrie},
  year = {2003},
  month = jan,
  journal = {Environment and Planning A: Economy and Space},
  volume = {35},
  number = {1},
  pages = {151--188},
  issn = {0308-518X, 1472-3409},
  doi = {10/dfjp6n},
  abstract = {Within a context of public policy debate in the United Kingdom on social exclusion, health inequalities, and food poverty, the metaphor of the `food desert' caught the imagination of those involved in policy development. Drawing from a major cross-disciplinary investigation of food access and food poverty in British cities, the authors report in this paper findings from the first `before/after' study of food consumption in a highly deprived area of a British city experiencing a sudden and significant change in its food-retail access. The study has been viewed as the first opportunity in the United Kingdom to assess the impact of a non-healthcare intervention (specifically a retail-provision intervention) on food-consumption patterns, and by extension diet-related health, in such a deprived, previously poor-retail-access community. The paper offers evidence of a positive but modest impact of the retail intervention on diet, and the authors discuss the ways in which their findings are potentially significant in the context of policy debate.},
  langid = {english},
  file = {/home/cjber/drive/pdf/ENVS416/Wrigley et al_2003_.pdf}
}

@article{wu2012,
  title = {Mobile {{Phone Use}} for {{Contacting Emergency Services}} in {{Life-threatening Circumstances}}},
  author = {Wu, Olivia and Briggs, Andrew and Kemp, Tom and Gray, Alastair and MacIntyre, Kate and Rowley, Jack and Willett, Keith},
  year = {2012},
  month = mar,
  journal = {The Journal of Emergency Medicine},
  volume = {42},
  number = {3},
  pages = {291-298.e3},
  issn = {07364679},
  doi = {10/cz5wxf},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wu et al_2012_Mobile Phone Use for Contacting Emergency Services in Life-threatening.pdf}
}

@article{wu2013,
  title = {Exploring the {{Association}} between {{Traffic Safety}} and {{Geometric Design Consistency Based}} on {{Vehicle Speed Metrics}}},
  author = {Wu, Kun-Feng and Donnell, Eric T. and Himes, Scott C. and Sasidharan, Lekshmi},
  year = {2013},
  month = jul,
  journal = {Journal of Transportation Engineering},
  volume = {139},
  number = {7},
  pages = {738--748},
  issn = {0733-947X, 1943-5436},
  doi = {10/f4575w},
  abstract = {Past design consistency research has demonstrated the relationship between operating speeds and geometric design features on two-lane rural highways. However, little is known about the relationship between geometric design consistency and traffic safety. In this study, design consistency is referred to as the difference between operating speed and inferred design speed, and design consistency density is measured to account for the effect of elements upstream and downstream of the study element. To perform the design consistency\textendash safety evaluation in the present study, geometric design, roadway inventory, crash, and operating speed data were collected along two case-study highways in central Pennsylvania (U.S. 322 and PA 350). Several count regression model formulations were used to explore the statistical association between design consistency and total crash frequency. A statistically significant positive association between geometric design consistency and safety was found. Design consistency surrounding the study elements was also found to increase the expected crash frequency in the study element. The significant effects of design consistency and design consistency density measures on crash frequency for the two case-study sites indicate that there is value in further considering the relationship between design consistency and safety in future research, particularly in the context of the highway safety manual crash-prediction algorithm. DOI: 10.1061/(ASCE)TE.1943-5436 .0000553. \textcopyright{} 2013 American Society of Civil Engineers.},
  langid = {english},
  annotation = {ZSCC: 0000027},
  file = {/home/cjber/drive/pdf/ENVS492/Wu et al_2013_.pdf}
}

@inproceedings{wu2019,
  title = {Enriching {{Pre-trained Language Model}} with {{Entity Information}} for {{Relation Classification}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Wu, Shanchan and He, Yifan},
  year = {2019},
  month = nov,
  pages = {2361--2364},
  publisher = {{ACM}},
  address = {{Beijing China}},
  doi = {10/ghkfr8},
  abstract = {Relation classification is an important NLP task to extract relations between entities. The state-of-the-art methods for relation classification are primarily based on Convolutional or Recurrent Neural Networks. Recently, the pre-trained BERT model achieves very successful results in many NLP classification / sequence labeling tasks. Relation classification differs from those tasks in that it relies on information of both the sentence and the two target entities. In this paper, we propose a model that both leverages the pre-trained BERT language model and incorporates information from the target entities to tackle the relation classification task. We locate the target entities and transfer the information through the pre-trained architecture and incorporate the corresponding encoding of the two entities. We achieve significant improvement over the state-of-the-art method on the SemEval-2010 task 8 relational dataset.},
  isbn = {978-1-4503-6976-3},
  langid = {english},
  file = {/home/cjber/drive/pdf/Wu_He_2019_Enriching Pre-trained Language Model with Entity Information for Relation.pdf}
}

@article{wwg2016,
  title = {Evidence {{Review}} 10: {{Area Based Initiatives}}},
  author = {WWG},
  year = {2016},
  abstract = {Area based initiatives (ABIs) are policy initiatives aimed at tightly defined geographical areas, and provide a package of support aimed at improving},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/ENVS416/WWG_2016_.pdf;/home/cjber/drive/zotero/storage/9ZMMYNIF/why-area-based-initiatives.html}
}

@book{xie2018,
  title = {R Markdown: {{The}} Definitive Guide},
  author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@manual{xie2020,
  type = {Manual},
  title = {Tufte: {{Tufte}}'s Styles for r Markdown Documents},
  author = {Xie, Yihui and Allaire, JJ},
  year = {2020}
}

@book{xie2020a,
  title = {R Markdown Cookbook},
  author = {Xie, Yihui and Dervieux, Christophe and Riederer, Emily},
  year = {2020},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, Florida}}
}

@inproceedings{xiong2017,
  title = {Chinese {{Geographical Knowledge Entity Relation Extraction}} via {{Deep Neural Networks}}:},
  shorttitle = {Chinese {{Geographical Knowledge Entity Relation Extraction}} via {{Deep Neural Networks}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Agents}} and {{Artificial Intelligence}}},
  author = {Xiong, Shengwu and Mao, Jingjing and Duan, Pengfei and Miao, Shaohao},
  year = {2017},
  pages = {24--33},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  address = {{Porto, Portugal}},
  doi = {10/gmxwsm},
  isbn = {978-989-758-219-6 978-989-758-220-2},
  langid = {english},
  file = {/home/cjber/drive/pdf/Xiong et al_2017_Chinese Geographical Knowledge Entity Relation Extraction via Deep Neural.pdf}
}

@article{xu2018,
  title = {Road {{Extraction}} from {{High-Resolution Remote Sensing Imagery Using Deep Learning}}},
  author = {Xu, Yongyang and Xie, Zhong and Feng, Yaxing and Chen, Zhanlong},
  year = {2018},
  month = sep,
  journal = {Remote Sensing},
  volume = {10},
  number = {9},
  pages = {1461},
  issn = {2072-4292},
  doi = {10/gfnn9c},
  abstract = {The road network plays an important role in the modern traffic system; as development occurs, the road structure changes frequently. Owing to the advancements in the field of high-resolution remote sensing, and the success of semantic segmentation success using deep learning in computer version, extracting the road network from high-resolution remote sensing imagery is becoming increasingly popular, and has become a new tool to update the geospatial database. Considering that the training dataset of the deep convolutional neural network will be clipped to a fixed size, which lead to the roads run through each sample, and that different kinds of road types have different widths, this work provides a segmentation model that was designed based on densely connected convolutional networks (DenseNet) and introduces the local and global attention units. The aim of this work is to propose a novel road extraction method that can efficiently extract the road network from remote sensing imagery with local and global information. A dataset from Google Earth was used to validate the method, and experiments showed that the proposed deep convolutional neural network can extract the road network accurately and effectively. This method also achieves a harmonic mean of precision and recall higher than other machine learning and deep learning methods.},
  langid = {english},
  annotation = {ZSCC: 0000007},
  file = {/home/cjber/drive/pdf/ENVS492/Xu et al_2018_.pdf}
}

@article{yadav2018,
  title = {Road {{Surface Detection}} from {{Mobile LiDAR Data}}},
  author = {Yadav, M. and Lohani, B. and Singh, A. K.},
  year = {2018},
  month = nov,
  journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {IV-5},
  pages = {95--101},
  issn = {2194-9050},
  doi = {10/gf3tdd},
  abstract = {The accurate three-dimensional road surface information is highly useful for health assessment and maintenance of roads. It is basic information for further analysis in several applications including road surface settlement, pavement condition assessment and slope collapse. Mobile LiDAR system (MLS) is frequently used now a days to collect detail road surface and its surrounding information in terms three-dimensional (3D) point cloud. Extraction of road surface from volumetric point cloud data is still in infancy stage because of heavy data processing requirement and the complexity in the road environment. The extraction of roads especially rural road, where road-curb is not present is very tedious job especially in Indian roadway settings. Only a few studies are available, and none for Indian roads, in the literature for rural road detection. The limitations of existing studies are in terms of their lower accuracy, very slow speed of data processing and detection of other objects having similar characteristics as the road surface. A fast and accurate method is proposed for LiDAR data points of road surface detection, keeping in mind the essence of road surface extraction especially for Indian rural roads. The Mobile LiDAR data in XYZI format is used as input in the proposed method. First square gridding is performed and ground points are roughly extracted. Then planar surface detection using mathematical framework of principal component analysis (PCA) is performed and further road surface points are detected using similarity in intensity and height difference of road surface pointe in their neighbourhood.},
  langid = {english},
  annotation = {ZSCC: 0000000},
  file = {/home/cjber/drive/pdf/ENVS492/Yadav et al_2018_.pdf}
}

@article{yao2005,
  title = {How {{Far Is Too Far}}? - {{A Statistical Approach}} to {{Context-contingent Proximity Modeling}}},
  shorttitle = {How {{Far Is Too Far}}?},
  author = {Yao, Xiaobai and Thill, Jean-Claude},
  year = {2005},
  month = mar,
  journal = {Transactions in GIS},
  volume = {9},
  number = {2},
  pages = {157--178},
  issn = {1361-1682, 1467-9671},
  doi = {10/dqb7z2},
  abstract = {Proximity is a fundamental concept in any comprehensive ontology of space (Worboys 2001). The provision of a context-contingent translation mechanism between linguistic proximity measures (e.g. ``near'', ``far'') and metric distance measures is an important topic in current GIS research. After a discussion of context factors that mediate the relationship between linguistic and metric distance measures, we present a statistical approach, Ordered Logit Regression, to the context-contingent proximity modeling. The approach can predict proximity given the corresponding metric distance and context variables. An empirical case study with human subjects is carried out using this statistical approach. Interpretation and predictive accuracy of the empirical case study are discussed.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Yao_Thill_2005_How Far Is Too Far.pdf}
}

@article{ye2020,
  title = {Coreferential {{Reasoning Learning}} for {{Language Representation}}},
  author = {Ye, Deming and Lin, Yankai and Du, Jiaju and Liu, Zhenghao and Li, Peng and Sun, Maosong and Liu, Zhiyuan},
  year = {2020},
  month = oct,
  journal = {arXiv:2004.06870 [cs]},
  eprint = {2004.06870},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github. com/thunlp/CorefBERT.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {/home/cjber/drive/pdf/Ye et al_2020_Coreferential Reasoning Learning for Language Representation.pdf;/home/cjber/drive/pdf/Ye et al_2020_Coreferential Reasoning Learning for Language Representation2.pdf}
}

@article{yin,
  title = {Using {{Social Media}} to {{Enhance Emergency Situation Awareness}}: {{Extended Abstract}}},
  author = {Yin, Jie and Karimi, Sarvnaz and Lampert, Andrew and Cameron, Mark and Robinson, Bella and Power, Robert},
  pages = {5},
  abstract = {Social media platforms, such as Twitter, offer a rich source of real-time information about real-world events, particularly during mass emergencies. Sifting valuable information from social media provides useful insight into time-critical situations for emergency officers to understand the impact of hazards and act on emergency responses in a timely manner. This work focuses on analyzing Twitter messages generated during natural disasters, and shows how natural language processing and data mining techniques can be utilized to extract situation awareness information from Twitter. We present key relevant approaches that we have investigated including burst detection, tweet filtering and classification, online clustering, and geotagging.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Yin et al_Using Social Media to Enhance Emergency Situation Awareness.pdf}
}

@article{yin2017,
  title = {Comparative {{Study}} of {{CNN}} and {{RNN}} for {{Natural Language Processing}}},
  author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.01923 [cs]},
  eprint = {1702.01923},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks (DNNs) have revolutionized the field of natural language processing (NLP). Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting positioninvariant features and RNN at modeling units in sequence. The state-of-the-art on many NLP tasks often switches due to the battle of CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Yin et al_2017_Comparative Study of CNN and RNN for Natural Language Processing.pdf}
}

@inproceedings{yoon2009,
  title = {Evaluation of Terrain Using {{LADAR}} Data in Urban Environment for Autonomous Vehicles and Its Application in the {{DARPA}} Urban Challenge},
  booktitle = {2009 {{ICCAS-SICE}}},
  author = {Yoon, J. and Crane, C. D.},
  year = {2009},
  month = aug,
  pages = {641--646},
  abstract = {This paper describes the autonomous ground vehicle developed by researchers at the University of Florida that participated in the 2007 DARPA Urban Challenge. Specifically, this paper introduces LADAR based terrain evaluation algorithms for an urban environments as well as off road. The terrain evaluation algorithm is very important for safe driving at high speed. On the real road, the driver is faced with numerous road conditions such as the smoothness of the road surface, curbs, or debris. For an unmanned vehicle to be successful, the algorithm has to decide whether the surface is traversable or non-traversable. For this reason, this paper focuses on the problem of extracting the ground terrain surface from 3-D point clouds obtained from LADAR sensors. The paper outlines the approach used by the University of Florida's Team Gator Nation to address the question of classifying traversable road conditions.},
  annotation = {ZSCC: 0000011},
  file = {/home/cjber/drive/pdf/ENVS492/Yoon_Crane_2009_.pdf;/home/cjber/drive/zotero/storage/Q89ACQ2C/5333076.html}
}

@article{young2009,
  title = {Geographically Intelligent Disclosure Control for Flexible Aggregation of Census Data},
  author = {Young, Caroline and Martin, David and Skinner, Chris},
  year = {2009},
  journal = {International Journal of Geographical Information Science},
  volume = {23},
  number = {4},
  pages = {457--482},
  issn = {1365-8816},
  doi = {10/dwbhqp},
  annotation = {ZSCC: 0000019},
  file = {/home/cjber/drive/pdf/ENVS416/Young et al_2009_.pdf}
}

@article{yuan2020,
  title = {A {{Thematic Similarity Network Approach}} for {{Analysis}} of {{Places Using Volunteered Geographic Information}}},
  author = {Yuan, Xiaoyi and Crooks, Andrew and Z{\"u}fle, Andreas},
  year = {2020},
  month = jun,
  journal = {ISPRS International Journal of Geo-Information},
  volume = {9},
  number = {6},
  pages = {385},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10/ggz26w},
  abstract = {The research presented in this paper proposes a thematic network approach to explore rich relationships between places. We connect places in networks through their thematic similarities by applying topic modeling to the textual volunteered geographic information (VGI) pertaining to the places. The network approach enhances previous research involving place clustering using geo-textual information, which often simplifies relationships between places to be either in-cluster or out-of-cluster. To demonstrate our approach, we use as a case study in Manhattan (New York) that compares networks constructed from three different geo-textural data sources\&mdash;TripAdvisor attraction reviews, TripAdvisor restaurant reviews, and Twitter data. The results showcase how the thematic similarity network approach enables us to conduct clustering analysis as well as node-to-node and node-to-cluster analysis, which is fruitful for understanding how places are connected through individuals\&rsquo; experiences. Furthermore, by enriching the networks with geodemographic information as node attributes, we discovered that some low-income communities in Manhattan have distinctive restaurant cultures. Even though geolocated tweets are not always related to place they are posted from, our case study demonstrates that topic modeling is an efficient method to filter out the place-irrelevant tweets and therefore refining how of places can be studied.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {crowdsourcing,geo-textual data,similarity network analysis,topic modeling,volunteered geographic information},
  file = {/home/cjber/drive/pdf/Yuan et al_2020_A Thematic Similarity Network Approach for Analysis of Places Using Volunteered.pdf;/home/cjber/drive/zotero/storage/KGHKWZLD/htm.html}
}

@article{zade2018,
  title = {From {{Situational Awareness}} to {{Actionability}}: {{Towards Improving}} the {{Utility}} of {{Social Media Data}} for {{Crisis Response}}},
  shorttitle = {From {{Situational Awareness}} to {{Actionability}}},
  author = {Zade, Himanshu and Shah, Kushal and Rangarajan, Vaibhavi and Kshirsagar, Priyanka and Imran, Muhammad and Starbird, Kate},
  year = {2018},
  month = nov,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {2},
  number = {CSCW},
  pages = {1--18},
  issn = {2573-0142, 2573-0142},
  doi = {10/gg7q95},
  langid = {english},
  file = {/home/cjber/drive/pdf/Zade et al_2018_From Situational Awareness to Actionability.pdf}
}

@article{zhang2003,
  title = {A Progressive Morphological Filter for Removing Nonground Measurements from Airborne {{LIDAR}} Data},
  author = {{Zhang} and {Shu-Ching Chen} and Whitman, D. and {Mei-Ling Shyu} and {Jianhua Yan} and {Chengcui Zhang}},
  year = {2003},
  month = apr,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {41},
  number = {4},
  pages = {872--882},
  issn = {0196-2892},
  doi = {10/dv3889},
  abstract = {Recent advances in airborne light detection and ranging (LIDAR) technology allow rapid and inexpensive measurements of topography over large areas. This technology is becoming a primary method for generating high-resolution digital terrain models (DTMs) that are essential to numerous applications such as flood modeling and landslide prediction. Airborne LIDAR systems usually return a three-dimensional cloud of point measurements from reflective objects scanned by the laser beneath the flight path. In order to generate a DTM, measurements from nonground features such as buildings, vehicles, and vegetation have to be classified and removed. In this paper, a progressive morphological filter was developed to detect nonground LIDAR measurements. By gradually increasing the window size of the filter and using elevation difference thresholds, the measurements of vehicles, vegetation, and buildings are removed, while ground data are preserved. Datasets from mountainous and flat urbanized areas were selected to test the progressive morphological filter. The results show that the filter can remove most of the nonground points effectively.},
  langid = {english},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/cjber/drive/pdf/ENVS492/Zhang et al_2003_.pdf}
}

@article{zhang2014,
  ids = {zhang2014a},
  title = {Geocoding Location Expressions in {{Twitter}} Messages: {{A}} Preference Learning Method},
  shorttitle = {Geocoding Location Expressions in {{Twitter}} Messages},
  author = {Zhang, Wei and Gelernter, Judith},
  year = {2014},
  month = dec,
  journal = {Journal of Spatial Information Science},
  number = {9},
  pages = {37--70},
  issn = {1948-660X},
  doi = {10/ggwjs9},
  abstract = {Resolving location expressions in text to the correct physical location, also known as geocoding or grounding, is complicated by the fact that so many places around the world share the same name. Correct resolution is made even more difficult when there is little context to determine which place is intended, as in a 140-character Twitter message, or when location cues from different sources conflict, as may be the case among different metadata fields of a Twitter message. We used supervised machine learning to weigh the different fields of the Twitter message and the features of a world gazetteer to create a model that will prefer the correct gazetteer candidate to resolve the extracted expression. We evaluated our model using the F1 measure and compared it to similar algorithms. Our method achieved results higher than state-of-the-art competitors.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Zhang_Gelernter_2014_Geocoding location expressions in Twitter messages.pdf;/home/cjber/drive/zotero/storage/RA2H3FK9/Zhang_Gelernter_2014_Geocoding location expressions in Twitter messages2.pdf}
}

@article{zhang2015,
  title = {Exploring {{Metaphorical Senses}} and {{Word Representations}} for {{Identifying Metonyms}}},
  author = {Zhang, Wei and Gelernter, Judith},
  year = {2015},
  pages = {9},
  abstract = {A metonym is a word with a figurative meaning, similar to a metaphor. Because metonyms are closely related to metaphors, we apply features that are used successfully for metaphor recognition to the task of detecting metonyms. On the ACL SemEval 2007 Task 8 data with gold standard metonym annotations, our system achieved 86.45\% accuracy on the location metonyms. Our code can be found on GitHub1.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/cjber/drive/pdf/Zhang_Gelernter_2015_Exploring Metaphorical Senses and Word Representations for Identifying Metonyms.pdf;/home/cjber/drive/pdf/Zhang_Gelernter_2015_Exploring Metaphorical Senses and Word Representations for Identifying Metonyms2.pdf}
}

@article{zhang2016,
  title = {An {{Easy-to-Use Airborne LiDAR Data Filtering Method Based}} on {{Cloth Simulation}}},
  author = {Zhang, Wuming and Qi, Jianbo and Wan, Peng and Wang, Hongtao and Xie, Donghui and Wang, Xiaoyan and Yan, Guangjian},
  year = {2016},
  month = jun,
  journal = {Remote Sensing},
  volume = {8},
  number = {6},
  pages = {501},
  issn = {2072-4292},
  doi = {10/gftcvs},
  abstract = {Separating point clouds into ground and non-ground measurements is an essential step to generate digital terrain models (DTMs) from airborne LiDAR (light detection and ranging) data. However, most filtering algorithms need to carefully set up a number of complicated parameters to achieve high accuracy. In this paper, we present a new filtering method which only needs a few easy-to-set integer and Boolean parameters. Within the proposed approach, a LiDAR point cloud is inverted, and then a rigid cloth is used to cover the inverted surface. By analyzing the interactions between the cloth nodes and the corresponding LiDAR points, the locations of the cloth nodes can be determined to generate an approximation of the ground surface. Finally, the ground points can be extracted from the LiDAR point cloud by comparing the original LiDAR points and the generated surface. Benchmark datasets provided by ISPRS (International Society for Photogrammetry and Remote Sensing) working Group III/3 are used to validate the proposed filtering method, and the experimental results yield an average total error of 4.58\%, which is comparable with most of the state-of-the-art filtering algorithms. The proposed easy-to-use filtering method may help the users without much experience to use LiDAR data and related technology in their own applications more easily.},
  langid = {english},
  annotation = {ZSCC: 0000112},
  file = {/home/cjber/drive/pdf/ENVS492/Zhang et al_2016_.pdf}
}

@article{zhang2018,
  title = {Road {{Centerline Extraction}} from {{Very-High-Resolution Aerial Image}} and {{LiDAR Data Based}} on {{Road Connectivity}}},
  author = {Zhang, Zhiqiang and Zhang, Xinchang and Sun, Ying and Zhang, Pengcheng},
  year = {2018},
  month = aug,
  journal = {Remote Sensing},
  volume = {10},
  number = {8},
  pages = {1284},
  issn = {2072-4292},
  doi = {10/gd9j5f},
  abstract = {The road networks provide key information for a broad range of applications such as urban planning, urban management, and navigation. The fast-developing technology of remote sensing that acquires high-resolution observational data of the land surface offers opportunities for automatic extraction of road networks. However, the road networks extracted from remote sensing images are likely affected by shadows and trees, making the road map irregular and inaccurate. This research aims to improve the extraction of road centerlines using both very-high-resolution (VHR) aerial images and light detection and ranging (LiDAR) by accounting for road connectivity. The proposed method first applies the fractal net evolution approach (FNEA) to segment remote sensing images into image objects and then classifies image objects using the machine learning classifier, random forest. A post-processing approach based on the minimum area bounding rectangle (MABR) is proposed and a structure feature index is adopted to obtain the complete road networks. Finally, a multistep approach, that is, morphology thinning, Harris corner detection, and least square fitting (MHL) approach, is designed to accurately extract the road centerlines from the complex road networks. The proposed method is applied to three datasets, including the New York dataset obtained from the object identification dataset, the Vaihingen dataset obtained from the International Society for Photogrammetry and Remote Sensing (ISPRS) 2D semantic labelling benchmark and Guangzhou dataset. Compared with two state-of-the-art methods, the proposed method can obtain the highest completeness, correctness, and quality for the three datasets. The experiment results show that the proposed method is an efficient solution for extracting road centerlines in complex scenes from VHR aerial images and light detection and ranging (LiDAR) data.},
  langid = {english},
  annotation = {ZSCC: 0000000},
  file = {/home/cjber/drive/pdf/ENVS492/Zhang et al_2018_.pdf}
}

@article{zhang2018a,
  title = {The Representativeness and Spatial Bias of Volunteered Geographic Information: A Review},
  shorttitle = {The Representativeness and Spatial Bias of Volunteered Geographic Information},
  author = {Zhang, Guiming and Zhu, A-Xing},
  year = {2018},
  month = jul,
  journal = {Annals of GIS},
  volume = {24},
  number = {3},
  pages = {151--162},
  issn = {1947-5683, 1947-5691},
  doi = {10/gjwbf3},
  abstract = {Many applications of volunteered geographic information (VGI) involve inferring the properties of the underlying population from a sample consisting of VGI observations, i.e. VGI sample. The representativeness of VGI sample is crucial for deciding the fitness for use of VGI in such applications. Due to the volunteers' opportunistic observation efforts, spatial distribution of VGI observations is often biased (i.e. spatial bias). This degrades the representativeness of VGI and impedes the quality of inference made from VGI. Extensive research has been conducted on assessing or assuring VGI quality from the perspective of the fundamental dimensions of spatial data quality. Yet, this perspective alone provides limited insights on the representativeness of VGI. Assessing VGI representativeness and developing novel approaches to accounting for spatial bias in VGI is in need for broadening the spectrum of VGI applications. This article offers a comprehensive survey of the scientific literature from various domains (ecology, statistics, machine learning, etc.) to summarize existing endeavors related to sample representativeness assessment and sample selection bias correction for enlightening the treatment of these issues in VGI applications.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Zhang_Zhu_2018_The representativeness and spatial bias of volunteered geographic information.pdf}
}

@inproceedings{zhao2012,
  title = {Road Network Extraction from Airborne {{LiDAR}} Data Using Scene Context},
  booktitle = {2012 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Zhao, Jiaping and You, Suya},
  year = {2012},
  month = jun,
  pages = {9--16},
  publisher = {{IEEE}},
  address = {{Providence, RI, USA}},
  doi = {10/gf3tc9},
  abstract = {We presented a novel procedure to extract ground road networks from airborne LiDAR data. First point clouds were separated into ground and non-ground parts, and ground roads were to be extracted from ground planes. Then, buildings and trees were distinguished in an energy minimization framework after incorporation of two new features. The separation provided supportive information for later road extractions. After that, we designed structure templates to search for roads on ground intensity images, and road widths and orientations were determined by a subsequent voting scheme. This local searching process produced road candidates only, in order to prune false positives and infer undetected roads, a scene-dependent Markov network was constructed to help infer a global road network. Combination of local template fitting and global MRF inference made extracted ground roads more accurate and complete. Finally, we extended developed methods to elevated roads extraction from non-ground points and combined them with the ground road network to formulate a whole network.},
  isbn = {978-1-4673-1612-5 978-1-4673-1611-8 978-1-4673-1610-1},
  langid = {english},
  annotation = {ZSCC: 0000042},
  file = {/home/cjber/drive/pdf/ENVS492/Zhao_You_2012_.pdf;/home/cjber/drive/pdf/ENVS492/Zhao_You_2012_2.pdf}
}

@article{zheng2018,
  title = {A {{Survey}} of {{Location Prediction}} on {{Twitter}}},
  author = {Zheng, Xin and Han, Jialong and Sun, Aixin},
  year = {2018},
  month = sep,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {30},
  number = {9},
  pages = {1652--1671},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10/gd3kq4},
  abstract = {Locations, e.g., countries, states, cities, and point-of-interests, are central to news, emergency events, and people's daily lives. Automatic identification of locations associated with or mentioned in documents has been explored for decades. As one of the most popular online social network platforms, Twitter has attracted a large number of users who send millions of tweets on daily basis. Due to the world-wide coverage of its users and real-time freshness of tweets, location prediction on Twitter has gained significant attention in recent years. Research efforts are spent on dealing with new challenges and opportunities brought by the noisy, short, and context-rich nature of tweets. In this survey, we aim at offering an overall picture of location prediction on Twitter. Specifically, we concentrate on the prediction of user home locations, tweet locations, and mentioned locations. We first define the three tasks and review the evaluation metrics. By summarizing Twitter network, tweet content, and tweet context as potential inputs, we then structurally highlight how the problems depend on these inputs. Each dependency is illustrated by a comprehensive review of the corresponding strategies adopted in state-of-the-art approaches. In addition, we also briefly review two related problems, i.e., semantic location prediction and point-of-interest recommendation. Finally, we make a conclusion of the survey and list future research directions.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Zheng et al_2018_A Survey of Location Prediction on Twitter.pdf}
}

@manual{zhu2020,
  type = {Manual},
  title = {{{kableExtra}}: {{Construct}} Complex Table with 'kable' and Pipe Syntax},
  author = {Zhu, Hao},
  year = {2020}
}

@inproceedings{zhu2020a,
  title = {Towards {{Accurate}} and {{Consistent Evaluation}}: {{A Dataset}} for {{Distantly-Supervised Relation Extraction}}},
  shorttitle = {Towards {{Accurate}} and {{Consistent Evaluation}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Zhu, Tong and Wang, Haitao and Yu, Junjie and Zhou, Xiabing and Chen, Wenliang and Zhang, Wei and Zhang, Min},
  year = {2020},
  pages = {6436--6447},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10/gm7973},
  abstract = {In recent years, distantly-supervised relation extraction has achieved a certain success by using deep neural networks. Distant Supervision (DS) can automatically generate large-scale annotated data by aligning entity pairs from Knowledge Bases (KB) to sentences. However, these DSgenerated datasets inevitably have wrong labels that result in incorrect evaluation scores during testing, which may mislead the researchers. To solve this problem, we build a new dataset NYTH, where we use the DS-generated data as training data and hire annotators to label test data. Compared with the previous datasets, NYT-H has a much larger test set and then we can perform more accurate and consistent evaluation. Finally, we present the experimental results of several widely used systems on NYT-H. The experimental results show that the ranking lists of the comparison systems on the DS-labelled test data and human-annotated test data are different. This indicates that our human-annotated data is necessary for evaluation of distantly-supervised relation extraction.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Zhu et al_2020_Towards Accurate and Consistent Evaluation.pdf}
}

@article{zimmer2010,
  title = {``{{But}} the Data Is Already Public'': On the Ethics of Research in {{Facebook}}},
  shorttitle = {``{{But}} the Data Is Already Public''},
  author = {Zimmer, Michael},
  year = {2010},
  month = dec,
  journal = {Ethics and Information Technology},
  volume = {12},
  number = {4},
  pages = {313--325},
  issn = {1388-1957, 1572-8439},
  doi = {10/dpz72s},
  abstract = {In 2008, a group of researchers publicly released profile data collected from the Facebook accounts of an entire cohort of college students from a US university. While good-faith attempts were made to hide the identity of the institution and protect the privacy of the data subjects, the source of the data was quickly identified, placing the privacy of the students at risk. Using this incident as a case study, this paper articulates a set of ethical concerns that must be addressed before embarking on future research in social networking sites, including the nature of consent, properly identifying and respecting expectations of privacy on social network sites, strategies for data anonymization prior to public release, and the relative expertise of institutional review boards when confronted with research projects based on data gleaned from social media.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Zimmer_2010_“But the data is already public”.pdf}
}

@book{zlatanova2008,
  title = {Geospatial Information Technology for Emergency Response},
  author = {Zlatanova, Sisi and Li, Jonathan},
  year = {2008},
  volume = {6},
  publisher = {{CRC Press}},
  isbn = {0-203-92881-4},
  keywords = {\#nosource,ES}
}

@misc{zotero-126,
  title = {New Speed Limit Guidance for Councils},
  journal = {GOV.UK},
  abstract = {Guidance to help local councils set more appropriate speed limits on local roads published.},
  howpublished = {https://www.gov.uk/government/news/new-speed-limit-guidance-for-councils},
  langid = {english},
  annotation = {00000},
  file = {/home/cjber/drive/zotero/storage/LFDJCGUS/new-speed-limit-guidance-for-councils.html}
}

@article{zotero-138,
  title = {Linear {{Probability Model}}},
  pages = {7},
  langid = {english},
  annotation = {00328},
  file = {/home/cjber/drive/pdf/ENVS492/pdf}
}

@misc{zotero-1419,
  title = {Troubled {{Families Programme}} Annual Report Published},
  journal = {GOV.UK},
  abstract = {First annual report, setting out how the current Troubled Families Programme has been supporting the most disadvantaged families.},
  howpublished = {https://www.gov.uk/government/news/troubled-families-programme-annual-report-published},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/CUVX9UEQ/troubled-families-programme-annual-report-published.html}
}

@misc{zotero-1439,
  title = {[{{News}}] {{OpenAI Model Generates Python Code}} - {{YouTube}}},
  howpublished = {https://www.youtube.com/watch?v=utuz7wBGjKM},
  file = {/home/cjber/drive/zotero/storage/5CUKBMWM/watch.html}
}

@misc{zotero-1443,
  title = {{{ISO}} - {{International Organization}} for {{Standardization}}},
  journal = {ISO},
  abstract = {We're ISO, the International Organization for Standardization. We develop and publish International Standards.},
  howpublished = {https://www.iso.org/home.html},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/Y8TL94V6/home.html}
}

@misc{zotero-1456,
  title = {Doi:10.1016/j.Ijforecast.2003.09.015 | {{Elsevier Enhanced Reader}}},
  shorttitle = {Doi},
  doi = {10.1016/j.ijforecast.2003.09.015},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0169207003001134?token=28CA5BE175EAE16CDB0293BC5C639EDB7834207E127EEC0AA3F797E1DD5D4324D0EB8BADC0ACE3242C503367F12AEFC8},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/PM2UQY5E/S0169207003001134.html}
}

@misc{zotero-155,
  title = {Emergency Services | {{Business}} and Government | {{Ordnance Survey}}},
  howpublished = {https://www.ordnancesurvey.co.uk/business-and-government/public-sector/emergency-services/},
  annotation = {00000},
  file = {/home/cjber/drive/zotero/storage/Y2C7X9BR/emergency-services.html}
}

@misc{zotero-2945,
  title = {{{OS MasterMap Highways Network Roads}} | {{Road Network}} | {{Tools}} \& {{Support}}},
  abstract = {Support for OS MasterMap Highways Network Roads including frequently-asked questions, user guide, release notes and other useful documents.},
  howpublished = {https://www.ordnancesurvey.co.uk/business-government/tools-support/mastermap-highways-support},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/M2AVLBYL/mastermap-highways-support.html}
}

@misc{zotero-2946,
  title = {{{UK Data Service}}},
  howpublished = {https://borders.ukdataservice.ac.uk/easy\_download\_data.html?data=England\_msoa\_2011},
  file = {/home/cjber/drive/zotero/storage/K9HATTUA/easy_download_data.html}
}

@misc{zotero-3042,
  title = {Early Detection and Information Extraction for Weather-Induced Floods Using Social Media Streams | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.ijdrr.2018.03.002},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S2212420918302735?token=ED22062DEF1DC7CF0E042E229E40A81D3C702CEA179D8CFD85D4014FB94B27A09790594812C59C1AB81CF20C93701433\&originRegion=eu-west-1\&originCreation=20210810105730},
  langid = {english},
  file = {/home/cjber/drive/pdf/Early detection and information extraction for weather-induced floods using.pdf;/home/cjber/drive/zotero/storage/XLJXEMXZ/S2212420918302735.html}
}

@misc{zotero-3050,
  title = {Using {{AI}} and {{Social Media Multimodal Content}} for {{Disaster Response}} and {{Management}}\_ {{Opportunities}}, {{Challenges}}, and {{Future Directions}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.ipm.2020.102261},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0306457320306002?token=82705272DCBE1F0DF6896956883C52A568AE0D9A34D6D70A47210EFBF3141B0DF079699E600E98E07A9A01F87E2B0076\&originRegion=eu-west-1\&originCreation=20210810110221},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/5AG57GTW/S0306457320306002.html}
}

@misc{zotero-3260,
  title = {{{CSD Knowledge}} - {{How}} to Install and Connect to {{Cisco AnyConnect VPN}} Client on {{Linux}}},
  howpublished = {https://liverpool.service-now.com/sp?id=kb\_article\&sysparm\_article=KB0012311\&sys\_kb\_id=e4287e361b8bac10a394ec60f54bcb30\&spa=1},
  file = {/home/cjber/drive/zotero/storage/E5RQF3Q3/sp.html}
}

@misc{zotero-3277,
  title = {{{GeoNames Ontology}} - {{Geo Semantic Web}}},
  howpublished = {https://www.geonames.org/ontology/documentation.html},
  file = {/home/cjber/drive/zotero/storage/ZJPBWDLR/documentation.html}
}

@misc{zotero-3360,
  title = {{{WhatsApp Web}}},
  abstract = {Quickly send and receive WhatsApp messages right from your computer.},
  howpublished = {https://web.whatsapp.com/},
  langid = {english},
  file = {/home/cjber/drive/zotero/storage/2G49PZNP/web.whatsapp.com.html}
}

@article{zotero-86,
  title = {Interaction between {{Speed Choice}} and {{Road Environment}}},
  pages = {25},
  langid = {english},
  file = {/home/cjber/drive/pdf/undefined/pdf}
}

@book{zuur2009,
  title = {A {{Beginner}}'s {{Guide}} to {{R}}},
  author = {Zuur, Alain F. and Ieno, Elena N. and Meesters, Erik},
  year = {2009},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-93837-0},
  isbn = {978-0-387-93836-3 978-0-387-93837-0},
  langid = {english},
  file = {/home/cjber/drive/pdf/Zuur et al_2009_A Beginner's Guide to R.pdf}
}

@article{zwarts2005,
  title = {Prepositional {{Aspect}} and the {{Algebra}} of {{Paths}}},
  author = {Zwarts, Joost},
  year = {2005},
  month = dec,
  journal = {Linguistics and Philosophy},
  volume = {28},
  number = {6},
  pages = {739--779},
  issn = {0165-0157, 1573-0549},
  doi = {10/djkckb},
  abstract = {The semantics of directional prepositions is investigated from the perspective of aspect. What distinguishes telic PPs (like to the house) from atelic PPs (like towards the house), taken as denoting sets of paths, is their algebraic structure: atelic PPs are cumulative, closed under the operation of concatenation, telic PPs are not. Not only does this allow for a natural and compositional account of how PPs contribute to the aspect of a sentence, but it also guides our understanding of the lexical semantics of prepositions in important ways. Semantically, prepositions turn out to be quite similar to nouns and verbs. Nominal distinctions (like singular and plural, mass and count) and verbal classes (like semelfactives and degree achieve ments) have their prepositional counterparts.},
  langid = {english},
  file = {/home/cjber/drive/pdf/Zwarts_2005_Prepositional Aspect and the Algebra of Paths.pdf}
}


